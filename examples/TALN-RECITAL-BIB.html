<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <title>Liste des conférences</title>
      <link href="../ressources/layout.css" rel="stylesheet" type="text/css">
   </head>
   <body>
      <h1>Index des conférences</h1>
      <div id="indexConferences">
         <ul>
            <li>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique
               des Langues - 
               
               						<a href="#RECITAL'2008">2008</a></li>
            <li>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique
               des Langues - 
               
               						<a href="#RECITAL'2009">2009</a></li>
            <li>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique
               des Langues - 
               
               						<a href="#RECITAL'2010">2010</a></li>
            <li>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique
               des Langues - 
               
               						<a href="#RECITAL'2011">2011</a></li>
            <li>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique
               des Langues - 
               
               						<a href="#RECITAL'2012">2012</a></li>
            <li>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique
               des Langues - 
               
               						<a href="#RECITAL'2013">2013</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2007">2007</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2008">2008</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2009">2009</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2010">2010</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2011">2011</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2012">2012</a></li>
            <li>conférence sur le Traitement Automatique des Langues Naturelles - 
               
               						<a href="#TALN'2013">2013</a></li>
         </ul>
      </div>
      <h1>Détails des conférences</h1>
      <div id="detailConferences">
         <div class="conference">
            <h2 id="RECITAL'2008">2008 - 
               					RECITAL'2008 - Président(s) :  
               					Patrice Bellot - 
               	Marie-Laure Guénot - 
               	 à 
               					Avignon
            </h2>
            
            				Il y a 14 articles référencés. 
            
            				
            					Il n'y a aucune information sur les articles soumis/acceptés.
            		<br>
            			Il n'y a aucun meilleur article. 
            		
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="recital-2008-long-001">Méthode de réordonnancement de réponses par transformation d’arbres : présentation
                  et analyse des résultats
               </h4>
               			Auteur : Guillaume Bernard -
               			Contact : gbernard@limsi.fr<br><p>Dans cet article nous présentons une évaluation et une analyse des résultats d’une
                  méthode de réordonnancement de réponses pour un système de questions-réponses. Cette
                  méthode propose une sélection des réponses candidates à une question en calculant
                  un coût par transformation d’arbres. Nous présentons une analyse des résultats obtenus
                  sur le corpus Clef 2004-2005 et nos conclusions sur les voies d’amélioration possibles
                  pour notre système.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes an evaluation and an analysis of the results of an answers reranking
                  method for a question-answering system. Candidate answers to a question are reordered
                  by computing a tree transform cost. We discuss the results of our evaluation on the
                  Clef 2004-2005 corpus and describe possible improvements to the system.
               </p>
               <hr>
               <h4 id="recital-2008-long-002">Annotation des informations temporelles dans des textes en français</h4>
               			Auteur : André Bittar -
               			Contact : andre.bittar@linguist.jussieu.fr<br><p>Le traitement des informations temporelles est crucial pour la compréhension de textes
                  en langue naturelle. Le langage de spécification TimeML a été conçu afin de permettre
                  le repérage et la normalisation des expressions temporelles et des événements dans
                  des textes écrits en anglais. L’objectif des divers projets TimeML a été de formuler
                  un schéma d’annotation pouvant s’appliquer à du texte libre, comme ce que l’on trouve
                  sur le Web, par exemple. Des efforts ont été faits pour l’application de TimeML à
                  d’autres langues que l’anglais, notamment le chinois, le coréen, l’italien, l’espagnol
                  et l’allemand. Pour le français, il y a eu des efforts allant dans ce sens, mais ils
                  sont encore un peu éparpillés. Dans cet article, nous détaillons nos travaux actuels
                  qui visent à élaborer des ressources complètes pour l’annotation de textes en français
                  selon TimeML - notamment un guide d’annotation, un corpus de référence (Gold Standard)
                  et des modules d’annotation automatique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The processing of temporal information is crucial for the understanding of natural
                  language texts. The specification language TimeML was developed to facilitate the
                  identification and normalization of temporal expressions and events in texts written
                  in English. The aim of the various TimeML projects was to formulate an annotation
                  scheme able to be applied to free text, such as that which is found on the Web, for
                  example. Recently, efforts have been made to apply TimeML to languages other than
                  English, namely Chinese, Korean, Italian, Spanish and German. Some efforts have been
                  made in this direction with respect to French, but they remain somewhat scattered.
                  In this paper, we detail our ongoing work, which aims to establish comprehensive resources
                  for the annotation of French texts according to TimeML - an annotation guide, a Gold
                  Standard corpus and modules for automatic annotation.
               </p>
               <hr>
               <h4 id="recital-2008-long-003">Morphosyntaxe de l'interrogation pour le système de question-réponse RITEL</h4>
               			Auteur : Anne Garcia-Fernandez -
               			Contact : annegf@limsi.fr<br>
               			Auteur : Carole Lailler -
               			Contact : carole.lailler@lium.univ-lemans.fr<br><p>Nous proposons d'étudier le cas de l'interrogation en Dialogue Homme-Machine au sein
                  d'un système de Question-Réponse à travers le prisme de la Grammaire Interactive.
                  Celle-ci établit un rapport direct entre question et réponse et présuppose que la
                  morphosyntaxe d'une interrogation dépend d'une « réponse escomptée »; l'interlocuteur
                  humain ou machine ayant la possibilité de produire une réponse effective divergente.
                  Nous proposons d’observer la présence des différentes formes de questions dans un
                  corpus issu de l’utilisation du système RITEL. Et nous présentons une expérience menée
                  sur des locuteurs natifs qui nous a permis de mettre en valeur la différence entre
                  réponses effectives produites par nos sujets et réponses présupposées par le contenu
                  intentionnel des questions. Les formalismes ainsi dégagés ont pour but de donner aux
                  systèmes de DHM des fonctionnalités nouvelles comme la capacité à interpréter et à
                  générer de la variabilité dans les énoncés produits.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we study the interrogation in Human-Computer Dialogue in order to integrate
                  interpretative and dynamic functionnalities in a Question-Answering system using Interactive
                  Grammar. It constitues a direct link between question and answer and assumes that
                  the morphosyntax of interrogation depends on an anticipated answer. We propose an
                  observation of question-answering corpora coming from the RITEL system showing the
                  distribution of different morphosyntatic kinds of questions. Then we present an experimentation
                  based on this observation in which French native speakers answer to those different
                  kinds of questions. Our objective is to observe the difference between the anticipated
                  answer and the effective one.
               </p>
               <hr>
               <h4 id="recital-2008-long-004">Un système d'annotation des entités nommées du type personne pour la résolution de
                  la référence
               </h4>
               			Auteur : Elzbieta Gryglicka -
               			Contact : elzbieta.gryglicka@thalesgroup.com<br><p>Dans cet article nous présentons notre démarche pour l’annotation des expressions
                  référentielles désignant les personnes et son utilisation pour la résolution partielle
                  de la référence. Les choix effectués dans notre implémentation s'inspirent des travaux
                  récents dans le domaine de l'extraction d'information et plus particulièrement de
                  la reconnaissance des entités nommées. Nous utilisons les grammaires locales dans
                  le but d'annoter les entités nommées du type Personne et pour construire, à partir
                  des annotations produites, une base de connaissances extra-linguistiques. Les informations
                  acquises par ce procédé sont ensuite utilisées pour implémenter une méthode de la
                  résolution de la référence pour les syntagmes nominaux coréférentiels.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The aim of this paper is to describe our approach for annotating of the referential
                  mentions that refer to the entities which are the instances of Person. Our method
                  is inspired by the recent work in information extraction and particularly the named
                  entities recognition and classification task. Local grammars are used to identify
                  this category of named entities and to generate an extra-linguistic knowledge base
                  which is further used for the process of reference resolution.
               </p>
               <hr>
               <h4 id="recital-2008-long-005">Description de la structure de la phrase japonaise en vue d'une analyse syntaxique</h4>
               			Auteur : Alexis Kauffmann -
               			Contact : alexis.kauffmann@lettres.unige.ch<br><p>Nous décrivons la façon dont est formée la phrase japonaise, avec son contenu minimal,
                  la structure des composants d'une phrase simple et l'ordre des mots dans ses composants,
                  les différentes phrases complexes et les possibilités de changements modaux. Le but
                  de cette description est de permettre l'analyse de la phrase japonaise selon des principes
                  universels tout en restant fidèles aux particularités de la langue. L'analyseur syntaxique
                  multilingue FIPS est en cours d'adaptation pour le japonais selon les règles de grammaire
                  qui ont été définies. Bien qu'il fonctionnait alors uniquement pour des langues occidentales,
                  les premiers résultats sont très positifs pour l'analyse des phrases simples, ce qui
                  montre la capacité de Fips à s'adapter à des langues très différentes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We describe here the way the Japanese sentence is structured, with its minimal content,
                  the structure of its components, the different kinds of complex sentences and the
                  possible modal effects. The aim of this description is to analyse the Japanese sentence
                  in a universal way while respecting the language properties. The syntactic parser
                  FIPS is now being developed to analyse Japanese sentences following grammar rules
                  coming from this syntactic description. Even though the parser used to be used for
                  Western languages only, the first results of simple Japanese sentence analysis are
                  very positive. This shows how FIPS can adapt easily to very different languages.
               </p>
               <hr>
               <h4 id="recital-2008-long-006">Adaptation d’un système de compréhension pour un robot compagnon</h4>
               			Auteur : Marc Le Tallec -
               			Contact : marc.letallec@etu.univ-tours.fr<br><p>Le projet EmotiRob, financé par l’ANR, a pour but de réaliser un robot compagnon pour
                  des enfants fragilisés. Le projet se décompose en deux sous parties que sont le module
                  de compréhension pour comprendre ce que dit l’enfant et un module d’interaction émotionnelle
                  pour apporter une réponse en simulant des émotions par les mouvements du corps, les
                  traits du visage et par l'émission de petits sons simples. Le module de compréhension
                  dont il est question ici réutilise les travaux du système Logus. La principale difficulté
                  est de faire évoluer le système existant d’un dialogue homme-machine finalisé vers
                  un domaine plus large et de détecter l’état émotionnel de l’enfant. Dans un premier
                  temps, nous présentons le projet EmotiRob et ses spécificités. Ensuite, le système
                  de compréhension de la parole Logus, sur lequel se base ce travail, est présenté en
                  détail. Enfin, nous présentons les adaptations du système à la nouvelle tâche EmotiRob.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The EmotiRob project, financed by ANR, aims at realizing a robot companion for weakened
                  children. The project decomposes into two under parts that which are the module of
                  understanding to include what says the child and a module of emotional interaction
                  to bring an answer by feigning feelings by the movements of the body, lines of the
                  face and by emission of small simple sounds. The module of understanding reuses the
                  works of the system Logus. The main difficulty is to develop the existing system of
                  a human-machine dialogue finalized towards a wider domain and to detect the emotional
                  state of the child. At first, we present the EmotiRob project and its specificities.
                  Then, the system of understanding Logus, on which bases itself this work is presented.
                  Finally, we present the adaptations of the system to his new task EmotiRob.
               </p>
               <hr>
               <h4 id="recital-2008-long-007">Identification automatique de marques d'opinion dans des textes</h4>
               			Auteur : Aiala Rosá -
               			Contact : aialar@fing.edu.uy<br><p>Nous présentons un modèle conceptuel pour la représentation d'opinions, en analysant
                  les éléments qui les composent et quelques propriétés. Ce modèle conceptuel est implémenté
                  et nous en décrivons le jeu d’annotations. Le processus automatique d’annotation de
                  textes en espagnol est effectué par application de règles contextuelles. Un premier
                  sous-ensemble de règles a été écrit pour l'identification de quelques éléments du
                  modèle. Nous analysons les premiers résultats de leur application.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We  present a model for the representation of opinions, by analyzing the elements
                  which compose them and some properties. The model has an operating counterpart, implemented
                  in the form of a set of tags. For the automatic application of these tags on Spanish
                  texts, we work on the writing of contextual rules. A primary subset of rules was written
                  for the identification of some elements of the model. We analyze the first results
                  of their application.
               </p>
               <hr>
               <h4 id="recital-2008-long-008">Transducteurs à fenêtre glissante pour l’induction lexicale</h4>
               			Auteur : Yves Scherrer -
               			Contact : yves.scherrer@lettres.unige.ch<br><p>Nous appliquons différents modèles de similarité graphique à la tâche de l’induction
                  de lexiques bilingues entre un dialecte de Suisse allemande et l’allemand standard.
                  Nous comparons des transducteurs stochastiques utilisant des fenêtres glissantes de
                  1 à 3 caractères, entraînés à l’aide de l’algorithme de maximisation de l’espérance
                  avec des corpus d’entraînement de tailles différentes. Si les transducteurs à unigrammes
                  donnent des résultats satisfaisants avec des corpus très petits, nous montrons que
                  les transducteurs à bigrammes les dépassent à partir de 750 paires de mots d’entraînement.
                  En général, les modèles entraînés nous ont permis d’améliorer la F-mesure de 7% à
                  15% par rapport à la distance de Levenshtein.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We apply different models of graphemic similarity to the task of bilingual lexicon
                  induction between a Swiss German dialect and Standard German. We compare stochastic
                  transducers using sliding windows from 1 to 3 letters, trained with the Expectation-Maximisation
                  algorithm on training corpora of different sizes. While the unigram transducers provide
                  good results with very small corpora, we show that bigram transducers outperform them
                  with corpora of 750 word pairs or more. Overall, the trained models show between 7%
                  and 15% F-measure improvement over Levenshtein distance.
               </p>
               <hr>
               <h4 id="recital-2008-long-009">Génération intégrée localisée pour la production de documents</h4>
               			Auteur : Pierre Hankach -
               			Contact : pierre.hankach@orange-ftgroup.com<br><p>Dans cet article, nous proposons une approche intégrée localisée pour la génération.
                  Dans cette approche, le traitement intégré des décisions linguistiques est limité
                  à la production des propositions dont les décisions qui concernent leurs générations
                  sont dépendantes. La génération se fait par groupes de propositions de tailles limitées
                  avec traitement intégré des décisions linguistiques qui concernent la production des
                  propositions qui appartiennent au même groupe. Notre approche apporte une solution
                  pour le problème de complexité computationnelle de la génération intégrée classique.
                  Elle fournit ainsi une alternative à la génération séparée (séquentielle ou interactive)
                  qui présente plusieurs défauts mais qui est implémentée de manière répandue dans les
                  systèmes de générations existants.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we propose a localized integrated approach for generation. In this
                  approach, the integrated handling of linguistic decisions is limited to the production
                  of propositions whose decisions concerning their generation are dependant. The generation
                  is performed by groups of propositions of limited size with an integrated handling
                  of linguistic decisions that concern the production of the propositions that belong
                  to the same group. Our approach provides a solution for the computational complexity
                  problem of classical integrated generation. Therefore, it provides an alternative
                  to separated generation (sequential and interactive) that has many drawbacks but is
                  widely implemented in today’s generation systems.
               </p>
               <hr>
               <h4 id="recital-2008-long-010">Un système de génération et étiquetage automatique de dictionnaires linguistiques
                  de l'arabe
               </h4>
               			Auteur : Mourad Mars<br>
               			Auteur : Mounir Zrigui<br>
               			Auteur : Georges Antoniadis<br>
               			Auteur : Mohamed Belgacem<br><p>L'objectif de cet article est la présentation d'un système de génération automatique
                  de dictionnaires électroniques de la langue arabe classique, développé au sein de
                  laboratoire UTIC (unité de Monastir). Dans cet article, nous présenterons, les différentes
                  étapes de réalisation, et notamment la génération automatique de ces dictionnaires
                  se basant sur une théorie originale : les Conditions de Structures Morphomatiques
                  (CSM), et les matrices lexicales. Ce système rentre dans le cadre des deux projets
                  MIRTO et OREILLODULE réalisés dans les deux laboratoires LIDILEM de Grenoble et UTIC
                  Monastir de Tunisie
               </p>
               <hr>
               <h4 id="recital-2008-long-011">Analyse quantitative et qualitative de citations extraites d’un corpus journalistique</h4>
               			Auteur : Fabien Poulard -
               			Contact : Fabien.Poulard@univ-nantes.fr<br><p>Dans le contexte de la détection de plagiats, le repérage de citations et de ses constituants
                  est primordial puisqu’il peut aider à évaluer le caractère licite ou illicite d’une
                  reprise (source citée ou non). Nous proposons ici une étude quantitative et qualitative
                  des citations extraites d’un corpus que nous avons auparavant construit. Cette étude
                  a pour but de tracer des axes de recherche vers une méthode de repérage automatique
                  des citations.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In the plagiarism detection context, finding citations and their components is essential
                  as it may help estimating legal value of a copy (with or without original source specified).
                  We propose here a quantitative and qualitative study of citations we extracted from
                  a corpus we previously built. This study aims at orienting our research towards an
                  efficient automatic citations extraction method.
               </p>
               <hr>
               <h4 id="recital-2008-long-012">Une structure pour les questions enchainées</h4>
               			Auteur : Kévin Séjourné -
               			Contact : kevin.sejourne@limsi.fr<br><p>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses
                  (SQR) utilisant des questions enchainées. La recherche des documents dans un SQR est
                  perturbée par l’absence d’informations sur la valeur à accorder aux éléments de texte
                  éventuellement utiles à la recherche d’informations qui figurent dans les questions
                  liées. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé,
                  et n’a pas fait l’oeuvre de technique dédiée. Afin d’améliorer la recherche des documents
                  dans un SQR nous étudions une nouvelle méthode pour organiser les informations liées
                  aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure
                  de données adaptée à la transmission des informations des questions liées jusqu’au
                  moteur d’interrogation.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present works realized in the field of questions answering systems(SQR) using chained
                  questions. The search for documents in a SQR is disrupted by the absence of information
                  on the value to be granted to the possibly useful elements of text in search of information
                  which appear in bound questions. The recent campaigns of evaluation show that this
                  problem is under estimated, and did not make the work of dedicated technique. To improve
                  the search for documents in a SQR we study a new method to organize the information
                  bound to the interactions between questions. This one is based on the operation of
                  a structure of data adapted to the transmission of the information of bound questions
                  up to the search engine.
               </p>
               <hr>
               <h4 id="recital-2008-long-013">Vers une nouvelle approche de la correction grammaticale automatique</h4>
               			Auteur : Agnès Souque -
               			Contact : asouque@gmail.com<br><p>La correction grammaticale automatique du français est une fonctionnalité qui fait
                  cruellement défaut à la communauté des utilisateurs de logiciels libres. Dans le but
                  de combler cette lacune, nous avons travaillé à l’adaptation au français d’un outil
                  initialement développé pour une langue étrangère. Ce travail nous a permis de montrer
                  que les approches classiques du traitement automatique des langues utilisées dans
                  le domaine ne sont pas appropriées. Pour y remédier, nous proposons de faire évoluer
                  les formalismes des correcteurs en intégrant les principes linguistiques de la segmentation
                  en chunks et de l’unification. Bien qu’efficace, cette évolution n’est pas suffisante
                  pour obtenir un bon correcteur grammatical du français. Nous envisageons alors une
                  nouvelle approche de la problématique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Free software users community is sorely lacking French grammar checking. With the
                  aim of filling this gap, we have worked on the adaptation to French of a tool originally
                  developped for a foreign language. Thanks to this work, we could show that classic
                  natural language processing approaches used in grammar checking are not suitable.
                  To remedy it, we suggest an evolution of grammar checkers that includes linguistic
                  principles such as chunking and unification. Despite its efficiency, this evolution
                  is not sufficient to get a good French grammr checker. We are then thinking of a new
                  approach of the problem.
               </p>
               <hr>
               <h4 id="recital-2008-long-014">Informations spatio-temporelles et objets touristiques dans des pages Web : repérage
                  et annotation
               </h4>
               			Auteur : Stéphanie Weiser<br><p>Cet article présente un projet de repérage, d'extraction et d'annotation d'informations
                  temporelles, d'informations spatiales et d'objets touristiques dans des pages Web
                  afin d'alimenter la base de connaissance d'un portail touristique. Nous portons une
                  attention particulière aux différences qui distinguent le repérage d'information dans
                  des pages Web du repérage d’informations dans des documents structurés. Après avoir
                  introduit et classifié les différentes informations à extraire, nous nous intéressons
                  à la façon de lier ces informations entre elles (par exemple apparier une information
                  d’ouverture et un restaurant) et de les annoter. Nous présentons également le logiciel
                  que nous avons réalisé afin d'effectuer cette opération d'annotation ainsi que les
                  premiers résultats obtenus. Enfin, nous nous intéressons aux autres types de marques
                  que l'on trouve dans les pages Web, les marques sémiotiques en particulier, dont l'analyse
                  peut être utile à l’interprétation des pages.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a project for the detection, extraction and annotation of temporal
                  and spatial information and of tourism objects in order to fill the knowledge base
                  of a tourism Web portal. We focus on the differences that exist between extraction
                  from structured documents and extraction from Web pages. First, the different types
                  of information to extract are presented. We then discuss methods for linking these
                  pieces of information together – for example relating the name of a restaurant to
                  its opening hours – and how to annotate the extracted data. The program we have developed
                  to perform the extraction and annotation, as well as an evaluation of this program,
                  are presented here. Finally, we focus on the semiotic marks which appear on the Web
                  and show they also prove useful in interpreting Web pages.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="RECITAL'2009">2009 - 
               					RECITAL'2009 - Président(s) :  
               					Thibault Mondary - 
               	Aurélien Bossard - 
               	Thierry Hamon - 
               	 à 
               					Senlis
            </h2>
            
            				Il y a 12 articles référencés. 
            
            				<br>
            			Statistiques :
            			
            <ul>
               <li>15 articles long soumis dont 12 acceptés. </li>
            </ul><br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#recital-2009-long-003">recital-2009-long-003</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="recital-2009-long-001">Apprentissage automatique et Co-training</h4>
               			Auteur : Pierre Gotab -
               			Contact : pierre.gotab@univ-avignon.fr<br><p>Dans le domaine de la classification supervisée et semi-supervisée, cet article présente
                  un contexte favorable à l’application de méthodes statistiques de classification.
                  Il montre l’application d’une stratégie alternative dans le cas où les données d’apprentissage
                  sont insuffisantes, mais où de nombreuses données non étiquetées sont à notre disposition
                  : le cotraining multi-classifieurs. Les deux vues indépendantes habituelles du co-training
                  sont remplacées par deux classifieurs basés sur des techniques de classification différentes
                  : icsiboost sur le boosting et LIBLINEAR sur de la régression logistique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In the domain of supervised and semi-supervised classification, this paper describes
                  an experimental context suitable with statistical classification. It shows an alternative
                  method usable when learning data is unsufficient but when many unlabeled data is avaliable
                  : the multi-classifier co-training. Two classifiers based on different classification
                  methods replace the two independent views of the original co-training algorithm :
                  icsiboost based on boosting and LIBLINEAR which is a logistic regression classifier.
               </p>
               <hr>
               <h4 id="recital-2009-long-002"></h4>
               			Auteur : Marianne Santaholma -
               			Contact : Marianne.Santaholma@unige.ch<br><p>Nous présentons une comparaison de la performance de deux types différents de reconnaisseurs
                  pour le japonais et l’anglais basés sur les grammaires. L’un des systèmes est dérivé
                  à partir de règles d’une grammaire monolingue et l'autre de règles paramétrisées et
                  multilingues. Ce dernier emploie, les mêmes règles de grammaire pour la création de
                  modèles de langue nécessaires à la reconnaissance des langues typologiquement différentes.
                  Nous avons effectué des expériences sur la reconnaissance dans les applications de
                  dialogue de domaine limitée. Ces expériences montrent que les modèles de langue dérivés
                  des règles multilingues de grammaire (1) traitent aussi bien l’un que l’autre les
                  deux langues examinées, et (2) que leur performance est comparable à celle des reconnaisseurs
                  dérivés de grammaires monolingues. Ceci suggère que le partage de grammaires entre
                  langues typologiquement différentes pourrait être une solution pour rendre plus efficace
                  le développement de systèmes de reconnaissance de la parole linguistiques.
               </p><em>Version anglaise :</em><h4>Comparing Speech Recognizers Derived from Mono- and Multilingual Grammars</h4>
               <p>This paper examines the performance of multilingual parameterized grammar rules on
                  speech recognition. We present a performance comparison of two different types of
                  Japanese and English grammar-based speech recognizers. One system is derived from
                  monolingual grammar rules and the other from multilingual parameterized grammar rules.
                  The latter one uses hence the same grammar rules for creation of the language models
                  for these two different languages. We carried out experiments on speech recognition
                  of limited domain dialog application. These experiments show that the language models
                  derived from multilingual parameterized grammar rules (1) perform equally well on
                  both tested languages, on English and Japanese, and (2) that the performance is comparable
                  with the recognizers derived from monolingual grammars that were explicitly developed
                  for these languages. This suggests that the sharing grammar resources between different
                  languages could be one solution for more efficient development of rule-based speech
                  recognizers.
               </p>
               <hr>
               <h4 id="recital-2009-long-003">Détection de la cohésion lexicale par voisinage distributionnel : application à la
                  segmentation thématique
               </h4>
               			Auteur : Clémentine Adam -
               			Contact : adam@univ-tlse2.fr<br>
               			Auteur : François Morlane-Hondère -
               			Contact : morlanehondere@gmail.com<br><p>Cette étude s’insère dans le projet VOILADIS (VOIsinage Lexical pour l’Analyse du
                  DIScours), qui a pour objectif d’exploiter des marques de cohésion lexicale pour mettre
                  au jour des phénomènes discursifs. Notre propos est de montrer la pertinence d’une
                  ressource, construite par l’analyse distributionnelle automatique d’un corpus, pour
                  repérer les liens lexicaux dans les textes. Nous désignons par voisins les mots rapprochés
                  par l’analyse distributionnelle sur la base des contextes syntaxiques qu’ils partagent
                  au sein du corpus. Pour évaluer la pertinence de la ressource ainsi créée, nous abordons
                  le problème du repérage des liens lexicaux à travers une application de TAL, la segmentation
                  thématique. Nous discutons l’importance, pour cette tâche, de la ressource lexicale
                  mobilixsée ; puis nous présentons la base de voisins distributionnels que nous utilisons
                  ; enfin, nous montrons qu’elle permet, dans un système de segmentation thématique
                  inspiré de (Hearst, 1997), des performances supérieures à celles obtenues avec une
                  ressource traditionnelle.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The present work takes place within the Voiladis project (Lexical neighborhood for
                  discourse analysis), whose purpose is to exploit lexical cohesion markers in the study
                  of various discursive phenomena. We want to show the relevance of a distribution-based
                  lexical resource to locate interesting relations between lexical items in a text.We
                  call neighbors lexical items that share a significant number of syntactic contexts
                  in a given corpus. In order to evaluate the usefulness of such a resource, we address
                  the task of topical segmentation of text, which generally makes use of some kind of
                  lexical relations. We discuss here the importance of the particular resource used
                  for the task of text segmentation. Using a system inspired by (Hearst, 1997), we show
                  that lexical neighbors provide better results than a classical resource.
               </p>
               <hr>
               <h4 id="recital-2009-long-004">Extraction de lexique dans un corpus spécialisé en chinois contemporain</h4>
               			Auteur : Gaël Patin -
               			Contact : gael.patin@inalco.fr<br><p>La constitution de ressources lexicales est une tâche cruciale pour l’amélioration
                  des performances des systèmes de recherche d’information. Cet article présente une
                  méthode d’extraction d’unités lexicales en chinois contemporain dans un corpus spécialisé
                  non-annoté et non-segmenté. Cette méthode se base sur une construction incrémentale
                  de l’unité lexicale orientée par une mesure d’association. Elle se distingue des travaux
                  précédents par une approche linguistique non-supervisée assistée par les statistiques.
                  Les résultats de l’extraction, évalués sur un échantillon aléatoire du corpus de travail,
                  sont honorables avec des scores de précision et de rappel respectivement de 52,6 %
                  et 53,7 %.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Building lexical resources is a vital task in improving the efficiency of information
                  retrieval systems. This article introduces a Chinese lexical unit extraction method
                  for untagged specialized corpora. This method is based on an incremental process driven
                  by an association score. This work features an unsupervised statistically aided linguistic
                  approach. The extraction results — evaluated on a random sample of the working corpus
                  — show decent precision and recall which amount respectively to 52.6% and 53.7%.
               </p>
               <hr>
               <h4 id="recital-2009-long-005">Induction de sens de mots à partir de multiples espaces sémantiques</h4>
               			Auteur : Claire Mouton -
               			Contact : Claire.Mouton@cea.fr<br><p>Les mots sont souvent porteurs de plusieurs sens. Pour traiter l’information correctement,
                  un ordinateur doit être capable de décider quel sens d’un mot est employé à chacune
                  de ses occurrences. Ce problème non parfaitement résolu a généré beaucoup de travaux
                  sur la désambiguïsation du sens des mots (Word Sense Disambiguation) et dans la génération
                  d’espaces sémantiques dont un des buts est de distinguer ces différents sens. Nous
                  nous inspirons ici de deux méthodes existantes de détection automatique des différents
                  usages et/ou sens des mots, pour les appliquer à des espaces sémantiques issus d’une
                  analyse syntaxique effectuée sur un très grand nombre de pages web. Les adaptations
                  et résultats présentés dans cet article se distinguent par le fait d’utiliser non
                  plus une seule représentation mais une combinaison de multiples espaces de forte dimensionnalité.
                  Ces multiples représentations étant en compétition entre elles, elles participent
                  chacune par vote à l’induction des sens lors de la phase de clustering.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Words can have many senses. In order to process information correctly, a computer
                  should be able to decide which sense of a word is used in a given context. This unsolved
                  problem has generated much research in word sense disambiguation and in the generation
                  of semantic spaces in order to separate possible meanings. Here, we adapt two existing
                  methods to automatically distinguish words uses and senses.We apply them to multiple
                  semantic spaces produced by a syntactic analysis of a very large number of web pages.
                  These adaptations and the results presented in this article differ from the original
                  methods in that they use a combination of several high dimensional spaces instead
                  of one single representation. Each of these competing semantic spaces takes part in
                  a clustering phase in which they vote on sense induction.
               </p>
               <hr>
               <h4 id="recital-2009-long-006">Méta-moteur de traduction automatique : proposition d’une métrique pour le classement
                  de traductions
               </h4>
               			Auteur : Marion Potet -
               			Contact : Marion.Potet@imag.fr<br><p>Compte tenu de l’essor du Web et du développement des documents multilingues, le besoin
                  de traductions "à la volée" est devenu une évidence. Cet article présente un système
                  qui propose, pour une phrase donnée, non pas une unique traduction, mais une liste
                  de N hypothèses de traductions en faisant appel à plusieurs moteurs de traduction
                  pré-existants. Neufs moteurs de traduction automatique gratuits et disponibles sur
                  leWeb ont été sélectionnés pour soumettre un texte à traduire et réceptionner sa traduction.
                  Les traductions obtenues sont classées selon une métrique reposant sur l’utilisation
                  d’un modèle de langage. Les expériences conduites ont montré que ce méta-moteur de
                  traduction se révèle plus pertinent que l’utilisation d’un seul système de traduction.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Considering the Web and multilingual documents development expansion, the need of
                  fast translation has become an evidence. This paper presents a system that proposes,
                  for a given sentence, a list of N translation hypotheses instead of a single translation,
                  using several machine translation systems already existing. Nine free and available
                  (on the Internet) automatic translation engines have been chosen to submit a text
                  to be translated and to receive its translation. The translations obtained are evaluated
                  individually with a language model adapted and a metric elaborated by us, and in this
                  way classified by relevance order. The experiment have pointed out that this meta-translation
                  engine is more useful than the use of one system for translation.
               </p>
               <hr>
               <h4 id="recital-2009-long-007">Modèles statistiques pour l’estimation automatique de la difficulté de textes de FLE</h4>
               			Auteur : Thomas François -
               			Contact : thomas.francois@uclouvain.be<br><p>La lecture constitue l’une des tâches essentielles dans l’apprentissage d’une langue
                  étrangère. Toutefois, la découverte d’un texte portant sur un sujet précis et qui
                  soit adapté au niveau de chaque apprenant est consommatrice de temps et pourrait être
                  automatisée. Des expériences montrent que, pour l’anglais, l’utilisation de classifieurs
                  statistiques permet d’estimer automatiquement la difficulté d’un texte. Dans cet article,
                  nous proposons une méthodologie originale comparant, pour le français langue étrangère
                  (FLE), diverses techniques de classification (la régression logistique, le bagging
                  et le boosting) sur deux corpus d’entraînement. Il ressort de cette analyse comparative
                  une légère supériorité de la régression logistique multinomiale.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Reading is known to be an essential task in language learning, but finding the appropriate
                  text for every learner is far from easy. In this context, automatic procedures can
                  support the teacher’s work. Some works on English reveal that it is possible to assess
                  the readability of texts using statistical classifiers. In this paper, we present
                  an original approach comparing various classification techniques, namely logistic
                  regression, bagging and boosting on two training corpora. The results show a slight
                  superiority for multinomial logistic regression over bagging or boosting.
               </p>
               <hr>
               <h4 id="recital-2009-long-008">Modélisation des mouvements explicites dans les ACG avec le produit dépendant</h4>
               			Auteur : Florent Pompigne -
               			Contact : florent.pompigne@loria.fr<br><p></p><em>Version anglaise :</em><h4></h4>
               <p>Abstract Categorial Grammars (ACG) is a grammatical framework based on linear lambda-calculus.
                  As in Muskens’ Lambda Grammars, an abstract term in this kind of categorial grammar
                  can be realized in different directions, such as syntactic and semantic ones. This
                  structure provides autonomy for these different processings. ACG’s architecture is
                  independent from the logic used and so the type system is easily extensible in order
                  to deal better with some linguistic phenomena. We will first introduce ACGs and the
                  dependent product construction. This paper will then be concerned with the issue of
                  overt grammatical movements, in particular extraction constraints in relative propositions,
                  and how several close frameworks deal with it. Last we will show how to capture this
                  phenomenon in extended ACG.
               </p>
               <hr>
               <h4 id="recital-2009-long-009">Normalisation des entités nommées : pour une approche mixte et orientée utilisateurs</h4>
               			Auteur : Vanessa Andréani -
               			Contact : va@tkm.fr<br><p>La normalisation intervient dans de nombreux champs du traitement de l'information.
                  Elle permet d'optimiser les performances des applications, telles que la recherche
                  ou l'extraction d'information, et de rendre plus fiable la constitution de ressources
                  langagières. La normalisation consiste à ramener toutes les variantes d'un même terme
                  ou d'une entité nommée à une forme standard, et permet de limiter l'impact de la variation
                  linguistique. Notre travail porte sur la normalisation des entités nommées, pour laquelle
                  nous avons mis en place un système complexe mêlant plusieurs approches. Nous en présentons
                  ici une des composantes : une méthode endogène de délimitation et de validation de
                  l’entité nommée normée, adaptée à des données multilingues. De plus, nous plaçons
                  l'utilisateur au centre du processus de normalisation, dans l'objectif d'obtenir des
                  données parfaitement fiables et adaptées à ses besoins.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Normalization is involved in many fields of information processing. It improves performances
                  for several applications, such as information retrieval or information extraction,
                  and makes linguistic resources constitution more reliable. Normalization consists
                  in standardizing each variant of a term or named entity into a unique form, and this
                  way restricts the impact of term variation. Our work applies to named entity normalization,
                  for which we implemented a complex system that mixes several approaches. We present
                  here one of its components: an endogenous method to mark out and validate the normalized
                  named entities. Moreover, we place the user in the center of our normalization process,
                  in order to obtain fully reliable data that fit his needs.
               </p>
               <hr>
               <h4 id="recital-2009-long-010">Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités
                  nommées en contexte
               </h4>
               			Auteur : Eric Charton -
               			Contact : eric.charton@univ-avignon.fr<br><p>Dans cet article, nous présentons une méthode de transformation de Wikipédia en ressource
                  d’information externe pour détecter et désambiguïser des entités nommées, en milieu
                  ouvert et sans apprentissage spécifique. Nous expliquons comment nous construisons
                  notre système, puis nous utilisons cinq éditions linguistiques de Wikipédia afin d’enrichir
                  son lexique. Pour finir nous réalisons une évaluation et comparons les performances
                  du système avec et sans compléments lexicaux issus des informations inter-linguistiques,
                  sur une tâche d’extraction d’entités nommées appliquée à un corpus d’articles journalistiques.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present a way to use of Wikipedia as an external resource to disambiguate
                  and detect named entities, without learning step. We explain how we build our system
                  and why we used five linguistic editions of the Wikipedia corpus to increase the volume
                  of potentially matching candidates. We finally experiment our system on a news corpus.
               </p>
               <hr>
               <h4 id="recital-2009-long-011">La distance intertextuelle pour la classification de textes en langue arabe</h4>
               			Auteur : Rami Ayadi -
               			Contact : ayadi.rami@planet.tn<br>
               			Auteur : Walid Jaoudi -
               			Contact : walidjaouadi@yahoo.fr<br><p>Nos travaux de recherche s’intéressent à l’application de la théorie de la distance
                  intertextuelle sur la langue arabe en tant qu’outil pour la classification de textes.
                  Cette théorie traite de la classification de textes selon des critères de statistique
                  lexicale, se basant sur la notion de connexion lexicale. Notre objectif est d’intégrer
                  cette théorie en tant qu’outil de classification de textes en langue arabe. Ceci nécessite
                  l’intégration d’une métrique pour la classification de textes au niveau d’une base
                  de corpus lemmatisés étiquetés et identifiés comme étant des références d’époques,
                  de genre, de thèmes littéraires et d’auteurs et ceci afin de permettre la classification
                  de textes anonymes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Our researche works are interested in the application of the intertextual distance
                  theory on the Arabic language as a tool for the classification of texts. This theory
                  handles the classification of texts according to criteria of lexical statistics, and
                  it is based on the lexical connection approach. Our objective is to integrate this
                  theory as a tool of classification of texts in Arabic language. It requires the integration
                  of a metrics for the classification of texts using a database of lemmatized and identified
                  corpus which can be considered as a literature reference for times, genres, literary
                  themes and authors and this in order to permit the classification of anonymous texts.
               </p>
               <hr>
               <h4 id="recital-2009-long-012">Techniques argumentatives pour aider à générer des descriptions orientées d’un événement</h4>
               			Auteur : Sara Boutouhami -
               			Contact : boutouhami@lipn.univ-paris13.fr<br><p>Les moyens et les formes stratégiques permettant la génération de descriptions textuelles
                  argumentées d’une même réalité effective sont nombreux. La plupart des définitions
                  proposées de l’argumentation partagent l’idée qu’argumenter c’est fournir les éléments
                  en faveur d’une conclusion donnée. Or dans notre tâche qui consiste à générer des
                  descriptions argumentées pour des accidents de la route, nous ne disposons pas uniquement
                  d’éléments en faveur de la conclusion souhaitée mais aussi d’éléments qui vont à l’encontre
                  de cette dernière et dont la présence est parfois obligatoire pour la compréhension
                  de ces descriptions. Afin de remédier à ce problème, nous proposons des techniques
                  de génération de descriptions argumentées qui présentent au mieux les éléments indésirables
                  à l’aide de stratégies argumentatives.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Strategic means and forms for the generation of textual descriptions of a same reality
                  are numerous. Most definitions of the argumentation given in the literature share
                  the idea that to argument we must provide elements in favor of a given conclusion.
                  Unfortunately, in our task consisting in generating biased descriptions of a road
                  crash, we do not have only elements in favor of the desired conclusion, but also elements
                  that are against it and must nevertheless be present in the description unless we
                  cannot be able to understand how the accident happened. To remedy this problem, we
                  proposed a module in our system for generating biased descriptions that handles the
                  task of presenting better the undesirable elements using argumentative techniques.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="RECITAL'2010">2010 - 
               					RECITAL'2010 - Président(s) :  
               					Alexandre Patry - 
               	Philippe Langlais - 
               	Aurélien Max - 
               	 à 
               					Montréal
            </h2>
            
            				Il y a 11 articles référencés. 
            
            				<br>
            			Statistiques :
            			
            <ul>
               <li>16 articles long soumis dont 11 acceptés. </li>
            </ul><br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#recital-2010-long-001">recital-2010-long-001</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="recital-2010-long-001">Attribution d’auteur au moyen de modèles de langue et de modèles stylométriques</h4>
               			Auteur : Audrey Laroche -
               			Contact : audrey.laroche@umontreal.ca<br><p>Dans une tâche consistant à trouver l’auteur (parmi 53) de chacun de 114 textes, nous
                  analysons la performance de modèles de langue et de modèles stylométriques sous les
                  angles du rappel et du nombre de paramètres. Le modèle de mots bigramme à lissage
                  de Kneser-Ney modifié interpolé est le plus performant (75 % de bonnes réponses au
                  premier rang). Parmi les modèles stylométriques, une combinaison de 7 paramètres liés
                  aux parties du discours produit les meilleurs résultats (rappel de 25 % au premier
                  rang). Dans les deux catégories de modèles, le rappel maximal n’est pas atteint lorsque
                  le nombre de paramètres est le plus élevé.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In a task consisting of attributing the proper author (among 53) of each of 114 texts,
                  we analyze the performance of language models and stylometric models from the point
                  of view of recall and the number of parameters. The best performance is obtained with
                  a bigram word model using interpolated modified Kneser-Ney smoothing (first-rank recall
                  of 75 %). The best of the stylometric models, which combines 7 parameters characterizing
                  the proportion of the different parts of speech in a text, has a firstrank recall
                  of 25 % only. In both types of models, the maximal recall is not reached when the
                  number of parameters is highest.
               </p>
               <hr>
               <h4 id="recital-2010-long-002">Densidées : calcul automatique de la densité des idées dans un corpus oral</h4>
               			Auteur : Hyeran Lee -
               			Contact : hlee1@univ-montp3.fr<br>
               			Auteur : Philippe Gambette<br>
               			Auteur : Elsa Maillé<br>
               			Auteur : Constance Thuillier<br><p>La densité des idées, qui correspond au ratio entre le nombre de propositions sémantiques
                  et le nombre de mots dans un texte reflète la qualité informative des propositions
                  langagières d’un texte. L'apparition de la maladie d'Alzheimer a été reliée à une
                  dégradation de la densité des idées, ce qui explique l'intérêt pour un calcul automatique
                  de cette mesure. Nous proposons une méthode basée sur un étiquetage morphosyntaxique
                  et des règles d'ajustement, inspirée du logiciel CPIDR. Cette méthode a été validée
                  sur un corpus de quarante entretiens oraux transcrits et obtient de meilleurs résultats
                  pour le français que CPIDR pour l’anglais. Elle est implémentée dans le logiciel libre
                  Densidées disponible sur http://code.google.com/p/densidees.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Idea density, which is the ratio of semantic propositions divided by the number of
                  words in a text, reflects the informative quality of the sentences of a text. A decreasing
                  idea density has been identified as one of the symptoms of Alzheimer’s disease, which
                  explains the interest in an automatic calculation of idea density. We propose a method
                  based on part-of-speech tagging followed by adjustment rules inspired from the CPIDR
                  software. This method was validated on a corpus of 40 transcribed conversations in
                  French and obtains better results in French than CPIDR in English. It is implemented
                  in the free software Densidées available at http://code.google.com/p/densidees.
               </p>
               <hr>
               <h4 id="recital-2010-long-003">Outils de segmentation du chinois et textométrie</h4>
               			Auteur : Li-Chi Wu -
               			Contact : lucielichi@gmail.com<br><p>La segmentation en mots est une première étape possible dans le traitement automatique
                  de la langue chinoise. Les systèmes de segmentation se sont beaucoup développés depuis
                  le premier apparu dans les années 1980. Il n’existe cependant aucun outil standard
                  aujourd’hui. L’objectif de ce travail est de faire une comparaison des différents
                  outils de segmentation en s’appuyant sur une analyse statistique. Le but est de définir
                  pour quel type de texte chacun d’eux est le plus performant. Quatre outils de segmentation
                  et deux corpus avec des thèmes distincts ont été choisis pour cette étude. À l’aide
                  des outils textométriques Lexico3 et mkAlign, nous avons centré notre analyse sur
                  le nombre de syllabes du chinois. Les données quantitatives ont permis d’objectiver
                  des différences entre les outils. Le système Hylanda s’avère performant dans la segmentation
                  des termes spécialisés et le système Stanford est plus indiqué pour les textes généraux.
                  L’étude de la comparaison des outils de segmentation montre le statut incontournable
                  de l’analyse textométrique aujourd’hui, celle-ci permettant d’avoir accès rapidement
                  à la recherche d’information.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Chinese word segmentation is the first step in Chinese natural language processing.
                  The system of segmentation has considerably developed since the first automatic system
                  of segmentation of the 1980’s. However, till today there are no standard tools. The
                  aim of this paper is to compare various tools of segmentation by through statistical
                  analysis. Our goal is to identify the kind of texts for which these segmentation tools
                  are the most effective. This study chose four segmentation tools and two corpora,
                  marked by distinct themes. Using two textometric toolboxes, Lexico3 and mkAlign, we
                  focused on the number of syllables in Chinese. The quantitative data allowed us to
                  objectify disparities between tools. The Hylanda system turns out to be effective
                  in the segmentation of specialized terms and the Stanford system is more appropriate
                  for general texts. The comparative study of segmenters shows the undeniable status
                  of textometrical analysis which is able to quickly access information retrieval.
               </p>
               <hr>
               <h4 id="recital-2010-long-004">Acquisition de grammaires locales pour l’extraction de relations entre entités nommées</h4>
               			Auteur : Mani Ezzat -
               			Contact : mani.ezzat@arisem.com<br><p>La constitution de ressources linguistiques est une tâche cruciale pour les systèmes
                  d’extraction d’information fondés sur une approche symbolique. Ces systèmes reposent
                  en effet sur des grammaires utilisant des informations issues de dictionnaires électroniques
                  ou de réseaux sémantiques afin de décrire un phénomène linguistique précis à rechercher
                  dans les textes. La création et la révision manuelle de telles ressources sont des
                  tâches longues et coûteuses en milieu industriel. Nous présentons ici un nouvel algorithme
                  produisant une grammaire d’extraction de relations entre entités nommées, de manière
                  semi-automatique à partir d’un petit ensemble de phrases représentatives. Dans un
                  premier temps, le linguiste repère un jeu de phrases pertinentes à partir d’une analyse
                  des cooccurrences d’entités repérées automatiquement. Cet échantillon n’a pas forcément
                  une taille importante. Puis, un algorithme permet de produire une grammaire en généralisant
                  progressivement les éléments lexicaux exprimant la relation entre entités. L’originalité
                  de l’approche repose sur trois aspects : une représentation riche du document initial
                  permettant des généralisations pertinentes, la collaboration étroite entre les aspects
                  automatiques et l’apport du linguiste et sur la volonté de contrôler le processus
                  en ayant toujours affaire à des données lisibles par un humain.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Building linguistics resources is a vital task for information extraction systems
                  based on a symbolic approach : cascaded patterns use information from digital dictionaries
                  or semantic networks to describe a precise linguistic phenomenon in texts. The manual
                  elaboration and revision of such patterns is a long and costly process in an industrial
                  environment. This work presents a semi-automatic method for creating patterns that
                  detect relations between named entities in corpora. The process is made of two different
                  phases. The result of the first phase is a collection of sentences containing the
                  relevant relation. This collection isn’t necessairly big. During the second phase,
                  an algorithm automatically produces the recognition grammar by generalizing the actual
                  content of the different relevant sentences. This method is original from three different
                  points of view : it uses a rich description of the linguistic content to allow accurate
                  generalizations, it is based on a close collaboration between an automatic process
                  and a linguist and, lastly, the output of the acquisition process is always readable
                  and modifiable by the end user.
               </p>
               <hr>
               <h4 id="recital-2010-long-005">Construction d’un corpus de paraphrases d’énoncés par traduction multiple multilingue</h4>
               			Auteur : Houda Bouamor -
               			Contact : houda.bouamor@limsi.fr<br><p>Les corpus de paraphrases à large échelle sont importants dans de nombreuses applications
                  de TAL. Dans cet article nous présentons une méthode visant à obtenir un corpus parallèle
                  de paraphrases d’énoncés en français. Elle vise à collecter des traductions multiples
                  proposées par des contributeurs volontaires francophones à partir de plusieurs langues
                  européennes. Nous formulons l’hypothèse que deux traductions soumises indépendamment
                  par deux participants conservent généralement le sens de la phrase d’origine, quelle
                  que soit la langue à partir de laquelle la traduction est effectuée. L’analyse des
                  résultats nous permet de discuter cette hypothèse.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Large scale paraphrase corpora are important for a variety of natural language processing
                  applications. In this paper, we present an approach which collects multiple translations
                  from several languages proposed by volunteers in order to obtain a parallel corpus
                  of paraphrases in French. We hypothesize that two translations proposed independently
                  by two volunteers usually retain the meaning of the original sentence, regardless
                  of the language from which the translation is done. The analysis of results allows
                  us to discuss this hypothesis.
               </p>
               <hr>
               <h4 id="recital-2010-long-006">Ces noms qui cachent des événements : un premier repérage</h4>
               			Auteur : Adila Amaria Bouabdallah -
               			Contact : amaria@info.univ-angers.fr<br><p>La détection des informations temporelles est cruciale pour le traitement automatique
                  des textes, qu’il s’agisse de modélisation linguistique, d’applications en compréhension
                  du langage ou encore de tâches de recherche documentaire ou d’extraction d’informations.
                  De nombreux travaux ont été dédiés à l’analyse temporelle des textes, et plus précisément
                  l’annotation des expressions temporelles ou des événements sous leurs différentes
                  formes : verbales, adjectivales ou nominales. Dans cet article, nous décrivons une
                  méthode pour la détection des syntagmes nominaux dénotant des événements. Notre approche
                  est basée sur l’implémentation d’un test linguistique simple proposé par les linguistes
                  pour cette tâche. Nous avons expérimenté notre méthode sur deux corpus différents
                  ; le premier est composé d’articles de presse et le second est beaucoup plus grand,
                  utilisant une interface pour interroger automatiquement le moteur de recherche Yahoo.
                  Les résultats obtenus ont montré que cette méthode se révèle plus pertinente pour
                  un plus large corpus.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The detection of temporal information is a crucial task for automatic text processing.
                  It is not only used in linguistics for the modelization of phenomenon and reasoning
                  implying time entities but also in numerous applications in language comprehension,
                  information retrieval, question-answering and information extraction. Many studies
                  have been devoted to the temporal analysis of texts, and more precisely to the tagging
                  of temporal entities and relations occurring in texts. Among these lasts, the various
                  avatars of events in their multiples occurring forms has been tackled by numerous
                  works. In this article we describe a method for the detection of noun phrases denoting
                  events. Our approach is based on the implementation of a simple linguistic test proposed
                  by linguists for this task. Our method is applied on two different corpora ; the first
                  is composed of newspaper articles and the second, a much larger one, rests on an interface
                  for automatically querying the Yahoo search engine. Primary results are encouraging
                  and increasing the size of the learning corpora should allow for a real statistical
                  validation of the results.
               </p>
               <hr>
               <h4 id="recital-2010-long-007">Catégorisation automatique d'adjectifs d'opinion à partir d'une ressource linguistique
                  générique
               </h4>
               			Auteur : Baptiste Chardon -
               			Contact : chardon@irit.fr<br><p>Cet article décrit un processus d’annotation manuelle de textes d’opinion, basé sur
                  un schéma fin d'annotation indépendant de la langue et du corpus. Ensuite, à partir
                  d'une partie de ce schéma, une méthode de construction automatique d'un lexique d'opinion
                  à partir d'un analyseur syntaxique et d'une ressource linguistique est décrite. Cette
                  méthode consiste à construire un arbre de décision basé sur les classes de concepts
                  de la ressource utilisée. Dans un premier temps, nous avons étudié la couverture du
                  lexique d'opinion obtenu par comparaison avec l’annotation manuelle effectuée sur
                  un premier corpus de critiques de restaurants. La généricité de ce lexique a été mesurée
                  en le comparant avec un second lexique, généré à partir d'un corpus de commentaires
                  de films. Dans un second temps, nous avons évalué l'utilisabilité du lexique au travers
                  d'une tâche extrinsèque, la reconnaissance de la polarité de commentaires d'internautes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper introduces a manual annotation process of opinion texts, based on a fine-featured
                  annotation scheme, independent of language and corpus. Then, from a part of this scheme,
                  a method to build automatically an opinion lexicon from a syntactic analyser and a
                  linguistic resource is described. This method consists in building a decision tree
                  from the classes of the resource. The coverage of the lexicon has been determined
                  by comparing it to the gold annotation of a restaurants review corpus. Its genericity
                  was determined by comparing it to another lexicon generated from a different domain
                  corpus (movie reviews). Eventually, the usefulness of the lexicon has been measured
                  with an extrinsic task, the recognition of the polarity of reviews.
               </p>
               <hr>
               <h4 id="recital-2010-long-008">Résumé automatique de documents arabes basé sur la technique RST</h4>
               			Auteur : Mohamed Hédi Maâloul -
               			Contact : mohamed.maaloul@lpl-aix.fr<br>
               			Auteur : Iskandar keskes -
               			Contact : iskandarkeskes@gmail.com<br><p>Dans cet article, nous nous intéressons au résumé automatique de textes arabes. Nous
                  commençons par présenter une étude analytique réalisée sur un corpus de travail qui
                  nous a permis de déduire, suite à des observations empiriques, un ensemble de relations
                  et de frames (règles ou patrons) rhétoriques; ensuite nous présentons notre méthode
                  de production de résumés pour les textes arabes. La méthode que nous proposons se
                  base sur la Théorie de la Structure Rhétorique (RST) (Mann et al., 1988) et utilise
                  des connaissances purement linguistiques. Le principe de notre proposition s’appuie
                  sur trois piliers. Le premier pilier est le repérage des relations rhétoriques entres
                  les différentes unités minimales du texte dont l’une possède le statut de noyau –
                  segment de texte primordial pour la cohérence – et l’autre a le statut noyau ou satellite
                  – segment optionnel. Le deuxième pilier est le dressage et la simplification de l’arbre
                  RST. Le troisième pilier est la sélection des phrases noyaux formant le résumé final,
                  qui tiennent en compte le type de relation rhétoriques choisi pour l’extrait.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we focus on automatic summarization of Arabic texts. We start by presenting
                  an analytical study carried out on a study corpus which enabled us to deduce, following
                  empirical observations, a set of relations and rhetorical frames; then we present
                  our proposed method to produce summaries for Arabic texts. This method is based bases
                  on the Rhetorical Structure Theory (RST) technique (Mann and Al., 1988) and uses purely
                  linguistic knowledge. The principle of the proposed method is based on three pillars.
                  The first pillar is the location of the rhetorical relations between the various minimal
                  units of the text of which one has the status of nucleus - text segment necessary
                  to maintain coherence - and the other has the status of nucleus or satellite - optional
                  segment. The second pillar is the representation and the simplification of RST-tree
                  that is considered most descriptive. The third pillar is the selection of the nucleus
                  sentences forming the final summary, which hold in account the type of rhetorical
                  relations chosen.
               </p>
               <hr>
               <h4 id="recital-2010-long-009">Inférences aspecto-temporelles analysées avec la Logique Combinatoire</h4>
               			Auteur : Hee-Jin Ro -
               			Contact : heejinro@yahoo.fr<br><p>Ce travail s’inscrit dans une recherche centrée sur une approche de l’Intelligence
                  Artificielle (IA) et de la linguistique computationnelle. Il permet d’intégrer différentes
                  techniques formelles de la Logique Combinatoire avec des types (Curry) et sa programmation
                  fonctionnelle (Haskell) avec une théorie énonciative du temps et de l’aspect. Nous
                  proposons des calculs formels de valeurs aspectotemporelles (processus inaccompli
                  présent, processus inaccompli passé, événement passé et étatrésultant présent) associées
                  à des représentations de significations verbales sous forme de schèmes applicatifs.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This work is in line with the research centered on an approach of the Artificial Intelligence
                  and the Computational linguistic. It allows integrating different formal technologies
                  of the Combinatory Logic with types (Curry) and their functional programme (Haskell)
                  with an enunciative theory of the tense and the aspect. We propose formal calculus
                  of aspecto-temporal values (present unaccomplished process, past unaccomplished process,
                  past event and present résultative-state) associated with representations of verbal
                  meanings in the form of applicative schemes.
               </p>
               <hr>
               <h4 id="recital-2010-long-010">Automatiser la rédaction de définitions terminographiques : questions et traitements</h4>
               			Auteur : Selja Seppälä -
               			Contact : seppala2@etu.unige.ch<br><p>Dans cet article, nous présentons une analyse manuelle de corpus de contextes conceptuels
                  afin (i) de voir dans quelle mesure les méthodes de TALN existantes sont en principe
                  adéquates pour automatiser la rédaction de définitions terminographiques, et (ii)
                  de dégager des question précises dont la résolution permettrait d’automatiser davantage
                  la production de définitions. Le but est de contribuer à la réflexion sur les enjeux
                  de l’automatisation de cette tâche, en procédant à une série d’analyses qui nous mènent,
                  étape par étape, à examiner l’adéquation des méthodes d’extraction de définitions
                  et de contextes plus larges au travail terminographique de rédaction des définitions.
                  De ces analyses émergent des questions précises relatives à la pertinence des informations
                  extraites et à leur sélection. Des propositions de solutions et leurs implications
                  pour le TALN sont examinées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>A manual corpus analysis of conceptual contexts is presented in order (i) to indicatively
                  evaluate to what extent NLP methods can in principle be used to automate the production
                  of terminographic definitions, and (ii) to identify central questions to be answered
                  if one wants to further automate the task. The objective is to contribute to reflection
                  on the challenges faced by the automation of this task. Through a series of analyses,
                  the adequacy of extraction methods for defining or knowledge-rich contexts is examined
                  in the light of the terminographic activity of definition writing. Precise questions
                  emerge from these analyses relating to the relevance and the selection of the extracted
                  information. Some solutions are proposed and their implications to NLP reviewed.
               </p>
               <hr>
               <h4 id="recital-2010-long-011">Représentation vectorielle de textes courts d’opinions, Analyse de traitements sémantiques
                  pour la fouille d’opinions par clustering
               </h4>
               			Auteur : Benoît Trouvilliez -
               			Contact : btrouvilliez@onyme.com<br><p>Avec le développement d’internet et des sites d’échanges (forums, blogs, sondages
                  en ligne, ...), l’exploitation de nouvelles sources d’informations dans le but d’en
                  extraire des opinions sur des sujets précis (film, commerce,...) devient possible.
                  Dans ce papier, nous présentons une approche de fouille d’opinions à partir de textes
                  courts. Nous expliquons notamment en quoi notre choix d’utilisation de regroupements
                  autour des idées exprimées nous a conduit à opter pour une représentation implicite
                  telle que la représentation vectorielle. Nous voyons également les différents traitements
                  sémantiques intégrés à notre chaîne de traitement (traitement de la négation, lemmatisation,
                  stemmatisation, synonymie ou même polysémie des mots) et discutons leur impact sur
                  la qualité des regroupements obtenus.
               </p><em>Version anglaise :</em><h4></h4>
               <p>With the internet and sharing web sites developement (forums, blogs, online surveys,
                  ...), new data source exploitation in order to extract opinions about various subjects
                  (film, business, ...) becomes possible. In this paper, we show an opinion mining approach
                  from short texts. We explain how our choice of using opinions clustering have conducted
                  us to use an implicit representation like vectorial representation. We present different
                  semantic process that we have incorporated into our process chain (negation process,
                  lemmatisation, stemmatisation, synonymy or polysemy) and we discut their impact on
                  the cluster quality.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="RECITAL'2011">2011 - 
               					RECITAL'2011 - Président(s) :  
               					Cédric Lopez - 
               	 à 
               					Montpellier
            </h2>
            
            				Il y a 9 articles référencés. 
            
            				<br>
            			Statistiques :
            			
            <ul>
               <li>8 articles long soumis dont 5 acceptés. </li>
               <li>6 articles court soumis dont 4 acceptés. </li>
            </ul><br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#recital-2011-long-004">recital-2011-long-004</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="recital-2011-long-001">Analyse de l’ambiguïté des requêtes utilisateurs par catégorisation thématique</h4>
               			Auteur : Fanny Lalleman -
               			Contact : fanny.lalleman@univ-tlse2.fr<br><p>Dans cet article, nous cherchons à identifier la nature de l’ambiguïté des requêtes
                  utilisateurs issues d’un moteur de recherche dédié à l’actualité, 2424actu.fr, en
                  utilisant une tâche de catégorisation. Dans un premier temps, nous verrons les différentes
                  formes de l’ambiguïté des requêtes déjà décrites dans les travaux de TAL. Nous confrontons
                  la vision lexicographique de l’ambiguïté à celle décrite par les techniques de classification
                  appliquées à la recherche d’information. Dans un deuxième temps, nous appliquons une
                  méthode de catégorisation thématique afin d’explorer l’ambiguïté des requêtes, celle-ci
                  nous permet de conduire une analyse sémantique de ces requêtes, en intégrant la dimension
                  temporelle propre au contexte des news. Nous proposons une typologie des phénomènes
                  d’ambiguïté basée sur notre analyse sémantique. Enfin, nous comparons l’exploration
                  par catégorisation à une ressource comme Wikipédia, montrant concrètement les divergences
                  des deux approches.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we try to identify the nature of ambiguity of user queries from a search
                  engine dedicated to news, 2424actu.fr, using a categorization task. At first, we see
                  different forms of ambiguity queries already described in the works of NLP. We confront
                  lexicographical vision of the ambiguity to that described by classification techniques
                  applied to information retrieval. In a second step, we apply a method of categorizing
                  themes to explore the ambiguity of queries, it allow us to conduct a semantic analysis
                  of these applications by integrating temporal context-specific news. We propose a
                  typology of phenomena of ambiguity based on our semantic analysis. Finally, we compare
                  the exploration by categorization with a resource as Wikipedia, showing concretely
                  the differences between these two approaches.
               </p>
               <hr>
               <h4 id="recital-2011-long-002">Extraction Automatique d'Informations Pédagogiques Pertinentes à partir de Documents
                  Textuels
               </h4>
               			Auteur : Boutheina Smine -
               			Contact : Boutheina.Smine@etudiants.univ-paris4.fr<br>
               			Auteur : Rim Faiz -
               			Contact : Rim.Faiz@ihec.rnu.tn<br>
               			Auteur : Jean-Pierre Desclés -
               			Contact : Jean-pierre.Descles@paris4.sorbonne.fr<br><p>Plusieurs utilisateurs ont souvent besoin d'informations pédagogiques pour les intégrer
                  dans leurs ressources pédagogiques, ou pour les utiliser dans un processus d'apprentissage.
                  Une indexation de ces informations s'avère donc utile en vue d'une extraction des
                  informations pédagogiques pertinentes en réponse à une requête utilisateur. La plupart
                  des systèmes d'extraction d'informations pédagogiques existants proposent une indexation
                  basée sur une annotation manuelle ou semi-automatique des informations pédagogiques,
                  tâche qui n'est pas préférée par les utilisateurs. Dans cet article, nous proposons
                  une approche d'indexation d'objets pédagogiques (Définition, Exemple, Exercice, etc.)
                  basée sur une annotation sémantique par Exploration Contextuelle des documents. L'index
                  généré servira à une extraction des objets pertinents répondant à une requête utilisateur
                  sémantique. Nous procédons, ensuite, à un classement des objets extraits selon leur
                  pertinence en utilisant l'algorithme Rocchio. Notre objectif est de mettre en valeur
                  une indexation à partir de contextes sémantiques et non pas à partir de seuls termes
                  linguistiques.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Different users need pedagogical information in order to use them in their resources
                  or in a learning process. Indexing this information is therefore useful for extracting
                  relevant pedagogical information in response to a user request. Several searching
                  systems of pedagogical information propose manual or semi-automatic annotations to
                  index documents, which is a complex task for users. In this article, we propose an
                  approach to index pedagogical objects (Definition, Exercise, Example, etc.) based
                  on automatic annotation of documents using Contextual Exploration. Then, we use the
                  index to extract relevant pedagogical objects as response to the user's requests.
                  We proceed to sort the extracted objects according to their relevance. Our objective
                  is to reach the relevant objects using a contextual semantic analysis of the text.
               </p>
               <hr>
               <h4 id="recital-2011-long-003">Des outils de TAL en support aux experts de sûreté industrielle pour l’exploitation
                  de bases de données de retour d’expérience
               </h4>
               			Auteur : Nikola Tulechki -
               			Contact : nikola.tulechki@univ-tlse2.fr<br><p>Cet article présente des applications d’outils et méthodes du traitement automatique
                  des langues (TAL) à la maîtrise du risque industriel grâce à l’analyse de données
                  textuelles issues de volumineuses bases de retour d’expérience (REX). Il explicite
                  d’abord le domaine de la gestion de la sûreté, ses aspects politiques et sociaux ainsi
                  que l’activité des experts en sûreté et les besoins qu’ils expriment. Dans un deuxième
                  temps il présente une série de techniques, comme la classification automatique de
                  documents, le repérage de subjectivité, et le clustering, adaptées aux données REX
                  visant à répondre à ces besoins présents et à venir, sous forme d’outils, en support
                  à l’activité des experts.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article presents a series of natural language processing (NLP) techniques, applied
                  to the domain of industrial risk management and the analysis of large collections
                  of textual feedback data. First we describe the socio-political aspects of the risk
                  mangement domain, the activity of the investigators working with this data. We then
                  present present applications of NLP techniques like automatic text classification,
                  clustering and opinion extraction, responding to different needs stated by the investigators.
               </p>
               <hr>
               <h4 id="recital-2011-long-004">Vers une algèbre des relations de discours pour la comparaison de structures discursives</h4>
               			Auteur : Charlotte Roze -
               			Contact : charlotteroze@linguist.jussieu.fr<br><p>Nous proposons une méthodologie pour la construction de règles de déduction de relations
                  de discours, destinées à être intégrées dans une algèbre de ces relations. La construction
                  de ces règles a comme principal objectif de pouvoir calculer la fermeture discursive
                  d’une structure de discours, c’est-à-dire de déduire toutes les relations que la structure
                  contient implicitement. Calculer la fermeture des structures discursives peut permettre
                  d’améliorer leur comparaison, notamment dans le cadre de l’évaluation de systèmes
                  d’analyse automatique du discours. Nous présentons la méthodologie adoptée, que nous
                  illustrons par l’étude d’une règle de déduction.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a methodology for the construction of discourse relations inference rules,
                  to be integrated into an algebra of these relations. The construction of these rules
                  has as main objective to allow for the calculation of the discourse closure of a structure,
                  i.e. deduce all the relations implicitly contained in the structure. Calculating the
                  closure of discourse structures improves their comparison, in particular within the
                  evaluation of discourse parsing systems. We present the adopted methodology, which
                  we illustrate by the study of a rule.
               </p>
               <hr>
               <h4 id="recital-2011-long-005"></h4>
               			Auteur : Prajol Shrestha -
               			Contact : Prajol.Shrestha@etu.univ-nantes.fr<br><p>Les corpus comparables monolingues, alignés non pas au niveau des documents mais au
                  niveau d’unités textuelles plus fines (paragraphe, phrases, etc.), sont utilisés dans
                  diverses applications de traitement automatique des langues comme par exemple en détection
                  de plagiat. Mais ces types de corpus ne sont pratiquement pas disponibles et les chercheurs
                  sont donc obligés de les construire et de les annoter manuellement, ce qui est un
                  travail très fastidieux et coûteux en temps. Dans cet article, nous présentons une
                  méthode, composée de deux étapes, qui permet de réduire ce travail d’annotation de
                  segments de texte. Cette méthode est évaluée lors de l’alignement de paragraphes provenant
                  de dépêches en langue anglaise issues de diverses sources. Les résultats obtenus montrent
                  un apport considérable de la méthode en terme de réduction de temps d’annotation.
                  Nous présentons aussi des premiers résultats obtenus à l’aide de simples traitements
                  automatiques (recouvrement de mots, de racines, mesure cosinus) pour tenter de diminuer
                  encore la charge de travail humaine.
               </p><em>Version anglaise :</em><h4>Alignment of Monolingual Corpus by Reduction of the Search Space</h4>
               <p>Monolingual comparable corpora annotated with alignments between text segments (paragraphs,
                  sentences, etc.) based on similarity are used in a wide range of natural language
                  processing applications like plagiarism detection, information retrieval, summarization
                  and so on. The drawback wanting to use them is that there aren’t many standard corpora
                  which are aligned. Due to this drawback, the corpus is manually created, which is
                  a time consuming and costly task. In this paper, we propose a method to significantly
                  reduce the search space for manual alignment of the monolingual comparable corpus
                  which in turn makes the alignment process faster and easier. This method can be used
                  in making alignments on different levels of text segments. Using this method we create
                  our own gold corpus aligned on the level of paragraph, which will be used for testing
                  and building our algorithms for automatic alignment. We also present some experiments
                  for the reduction of search space on the basis of stem overlap, word overlap, and
                  cosine similarity measure which help us automatize the process to some extent and
                  reduce human effort for alignment.
               </p>
               <hr>
               <h4 id="recital-2011-court-001"></h4>
               			Auteur : Prajol Shrestha -
               			Contact : prajol.shrestha@etu.univ-nantes.fr<br><p>Cet article concerne la détermination de la similarité entre des textes courts (phrases,
                  paragraphes, ...). Ce problème est souvent abordé dans la littérature à l’aide de
                  méthodes supervisées ou de ressources externes comme le thesaurus Wordnet ou le British
                  National Corpus. Les méthodes que nous proposons sont non supervisées et n’utilisent
                  pas de connaissances à priori. La première méthode que nous présentons est basée sur
                  le modèle vectoriel de Salton auquel nous avons apporté des modifications pour prendre
                  en compte le contexte, le sens et la relation entre les mots des textes. Dans un deuxième
                  temps, nous testons les mesures de Dice et de ressemblance pour résoudre ce problème
                  ainsi que l’utilisation de la racinisation. Enfin, ces différentes méthodes sont évaluées
                  et comparées aux résultats obtenus dans la littérature.
               </p><em>Version anglaise :</em><h4>Corpus-Based methods for Short Text Similarity</h4>
               <p>This paper presents corpus-based methods to find similarity between short text (sentences,
                  paragraphs, ...) which has many applications in the field of NLP. Previous works on
                  this problem have been based on supervised methods or have used external resources
                  such as WordNet, British National Corpus etc. Our methods are focused on unsupervised
                  corpus-based methods. We present a new method, based on Vector Space Model, to capture
                  the contextual behavior, senses and correlation, of terms and show that this method
                  performs better than the baseline method that uses vector based cosine similarity
                  measure. The performance of existing document similarity measures, Dice and Resemblance,
                  are also evaluated which in our knowledge have not been used for short text similarity.
                  We also show that the performance of the vector-based baseline method is improved
                  when using stems instead of words and using the candidate sentences for computing
                  the parameters rather than some external resource.
               </p>
               <hr>
               <h4 id="recital-2011-court-002">Ressources lexicales au service de recherche et d’indexation des images</h4>
               			Auteur : Inga Gheorghita -
               			Contact : inga.gheorghita@atilf.fr<br><p>Cet article présente une méthodologie d’utilisation du Trésor de la Langue Française
                  informatisée (TLFi) pour l’indexation et la recherche des images fondée sur l’annotation
                  textuelle. Nous utilisons les définitions du TLFi pour la création automatique et
                  l’enrichissement d’un thésaurus à partir des mots-clés de la requête de recherche
                  et des mots-clés attribués à l’image lors de l’indexation. Plus précisement il s’agit
                  d’associer, de façon automatisé, à chaque mot-clé de l’image une liste des mots extraits
                  de ses définitions TLFi pour un domaine donné, en construisant ainsi un arbre hiérarchique.
                  L’approche proposée permet une catégorisation très précise des images, selon les domaines,
                  une indexation de grandes quantités d’images et une recherche rapide.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article presents a methodology for using the “Trésor de la Langue Française informatisée”
                  (TLFi) for indexing and searching images based on textual annotation. We use the definitions
                  of TLFi for automatic creation and enrichment of a thesaurus based on keywords from
                  the search query and the keywords assigned to the image during indexing. More specifically
                  it is automatically to associate, to each keyword of the image a list of words from
                  their TLFi’s definitions for a given area, thus building a hierarchical tree. The
                  proposed approach allows a very accurate categorization of images, depending on the
                  fields, a indexing of large amounts of images and a quick search.
               </p>
               <hr>
               <h4 id="recital-2011-court-003">Repérer les phrases évaluatives dans les articles de presse à partir d’indices et
                  de stéréotypes d’écriture
               </h4>
               			Auteur : Mathias Lambert -
               			Contact : Mathias.Lambert@paris-sorbonne.fr<br><p>Ce papier présente une méthode de recherche des phrases évaluatives dans les articles
                  de presse économique et financière à partir de marques et d’indices stéréotypés, propres
                  au style journalistique, apparaissant de manière concomitante à l’expression d’évaluation(s)
                  dans les phrases. Ces marques et indices ont été dégagés par le biais d’une annotation
                  manuelle. Ils ont ensuite été implémentés, en vue d’une phase-test d’annotation automatique,
                  sous forme de grammaires DCG/GULP permettant, par filtrage, de matcher les phrases
                  les contenant. Les résultats de notre première tentative d’annotation automatique
                  sont présentés dans cet article. Enfin les perspectives offertes par cette méthode
                  relativement peu coûteuse en ressources (à base d’indices non intrinsèquement évaluatifs)
                  font l’objet d’une discussion.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a method to locate evaluative sentences in financial and economic
                  newspapers, relying on marks and stereotyped signs. Peculiar to journalese, these
                  are present concomitantly with the expression of evaluation(s) in sentences. These
                  marks or signs have been found by means of a manual annotation. Then, in preparation
                  for an automatic annotation phase, they have been implemented in the form of DCG/GULP
                  grammars which, by filtering, allows to locate the sentences containing them. The
                  results of our first automatic annotation attempt are shown in this article. Furthermore,
                  the prospects offered by this method, which relies on nonintrinsically evaluative
                  marks and therefore does not require long lists of lexical resources, are discussed.
               </p>
               <hr>
               <h4 id="recital-2011-court-004">La complexité linguistique Méthode d’analyse</h4>
               			Auteur : Adrien Barbaresi<br><p>La complexité linguistique regroupe différents phénomènes dont il s’agit de modéliser
                  le rapport. Le travail en cours que je décris ici propose une réflexion sur les approches
                  linguistiques et techniques de cette notion et la mise en application d’un balayage
                  des textes qui s’efforce de contribuer à leur enrichissement. Ce traitement en surface
                  effectué suivant une liste de critères qui représentent parfois des approximations
                  de logiques plus élaborées tente de fournir une image ``raisonnable'' de la complexité.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Linguistic complexity includes various linguistic phenomena which interaction is to
                  be modeled. The ongoing work described here tackles linguistic and technical approaches
                  of this idea as well as an implementation of a parsing method which is part of text
                  enrichment techniques. This chunk parsing is performed according to a list of criteria
                  that may consist in logical approximations of more sophisticated processes in order
                  to provide a ``reasonable'' image of complexity.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="RECITAL'2012">2012 - 
               					RECITAL'2012 - Président(s) :  
               					Didier Schwab - 
               	Jorge-Mauricio Molina-Mejia - 
               	 à 
               					Grenoble
            </h2>
            
            				Il y a 26 articles référencés. 
            
            				<br>
            			Statistiques :
            			
            <ul>
               <li>42 articles long soumis dont 28 acceptés. </li>
            </ul><br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#recital-2012-long-001">recital-2012-long-001</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="recital-2012-long-001">Segmentation non supervisée : le cas du mandarin</h4>
               			Auteur : Pierre Magistry<br><p>Dans cet article, nous présentons un système de segmentation non supervisée que nous
                  évaluons sur des données en mandarin. Notre travail s’inspire de l’hypothèse de Harris
                  (1955) et suit Kempe (1999) et Tanaka-Ishii (2005) en se basant sur la reformulation
                  de l’hypothèse en termes de variation de l’entropie de branchement. Celle-ci se révèle
                  être un bon indicateur des frontières des unités linguistiques. Nous améliorons le
                  système de (Jin et Tanaka-Ishii, 2006) en ajoutant une étape de normalisation qui
                  nous permet de reformuler la façon dont sont prises les décisions de segmentation
                  en ayant recours à la programmation dynamique. Ceci nous permet de supprimer la plupart
                  des seuils de leur modèle tout en obtenant de meilleurs résultats, qui se placent
                  au niveau de l’état de l’art (Wang et al., 2011) avec un système plus simple que ces
                  derniers. Nous présentons une évaluation des résultats sur plusieurs corpus diffusés
                  pour le Chinese Word Segmentation bake-off II (Emerson, 2005) et détaillons la borne
                  supérieure que l’on peut espérer atteindre avec une méthode non-supervisée. Pour cela
                  nous utilisons ZPAR en apprentissage croisé (Zhang et Clark, 2010) comme suggéré dans
                  (Huang et Zhao, 2007; Zhao et Kit, 2008)
               </p><em>Version anglaise :</em><h4>Unsupervized Word Segmentation</h4>
               <p>In this paper, we present an unsupervised segmentation system tested on Mandarine
                  Chinese. Following Harris’s Hypothesis in Kempe (1999) and Tanaka-Ishii (2005) reformulation,
                  we base our work on the Variation of Branching Entropy. We improve on (Jin et Tanaka-Ishii,
                  2006) by adding normalization and Viterbi-decoding. This enables us to remove most
                  of the thresholds and parameters from their model and to reach near state-of-the-art
                  results (Wang et al., 2011) with a simpler system. We provide evaluation on different
                  corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more
                  precise topline for the task using cross-trained supervised system available off-the-shelf
                  (Zhang et Clark, 2010; Zhao et Kit, 2008; Huang et Zhao, 2007)
               </p>
               <hr>
               <h4 id="recital-2012-long-002">Incrémentation lexicale dans les textes : une auto-organisation</h4>
               			Auteur : Matthias Tauveron -
               			Contact : matthias.tauveron@etu.unistra.fr<br><p>Nous proposons une étude dynamique du lexique, en décrivant la manière dont il s’organise
                  progressivement du début à la fin d’un texte. Pour ce faire, nous nous focalisons
                  sur la co-occurrence généralisée, en formant un graphe qui représente tous les lemmes
                  du texte et synthétise leurs relations mutuelles de co-occurrence. L’étude d’un corpus
                  de 40 textes montre que ces relations évoluent d’une manière auto-organisée : la forme
                  - et l’identité - du graphe de co-occurrence restent stables après une phase d’organisation
                  terminée avant la 1ère moitié du texte. Ensuite, il n’évolue plus : les nouveaux mots
                  et les nouvelles relations de co-occurrence s’inscrivent peu à peu dans le réseau,
                  sans modifier la forme d’ensemble de la structure. La relation de co-occurrence généralisée
                  dans un texte apparaît donc comme la construction rapide d’un système, qui est ensuite
                  assez souple pour canaliser un flux d’information sans changer d’identité.
               </p><em>Version anglaise :</em><h4>Lexical Incrementation within Texts: a Self-Organization</h4>
               <p>We propose here a dynamic study of lexicon: we describe how it is organized progressively
                  from the beginning to the end of a given text. We focus on the “generalized co-occurrence”,
                  forming a graph that represents all the lemmas of the text and their mutual co-occurrence
                  relations. The study of a corpus of 40 texts shows that these relations have a self-organized
                  evolution: the shape and the identity of the graph of cooccurrence become stable after
                  a period of organization finished before the first half of the text. Then they no
                  longer change: new words and new co-occurrence relations gradually take place in the
                  network without changing its overall shape. We show that the evolution of the “generalized
                  co-occurrence” is the quick construction of a system, which is then flexible enough
                  to channel the flow of information without changing its identity.
               </p>
               <hr>
               <h4 id="recital-2012-long-003">Etude des mesures de similarité hétérogènes pour l’extraction de relations sémantiques</h4>
               			Auteur : Alexander Panchenko -
               			Contact : alexander.panchenko@student.uclouvain.be<br><p>L’article évalue un éventail de mesures de similarité qui ont pour but de prédire
                  les scores de similarité sémantique et les relations sémantiques qui s’établissent
                  entre deux termes, et étudie les moyens de combiner ces mesures. Nous présentons une
                  analyse comparative à grande échelle de 34 mesures basées sur des réseaux sémantiques,
                  le Web, des corpus, ainsi que des définitions. L’article met en évidence les forces
                  et les faiblesses de chaque approche en contexte de l’extraction de relations. Enfin,
                  deux techniques de combinaison de mesures sont décrites et testées. Les résultats
                  montrent que les mesures combinées sont plus performantes que toutes les mesures simples
                  et aboutissent à une corrélation de 0,887 et une Precision(20) de 0,979.
               </p><em>Version anglaise :</em><h4>A Study of Heterogeneous Similarity Measures for Semantic Relation Extraction</h4>
               <p>This paper evaluates a wide range of heterogeneous semantic similarity measures on
                  the task of predicting semantic similarity scores and the task of predicting semantic
                  relations that hold between two terms, and investigates ways to combine these measures.
                  We present a large-scale benchmarking of 34 knowledge-, web-, corpus-, and definition-based
                  similarity measures. The strengths and weaknesses of each approach regarding relation
                  extraction are discussed. Finally, we describe and test two techniques for measure
                  combination. These combined measures outperform all single measures, achieving a correlation
                  of 0.887 and Precision(20) of 0.979.
               </p>
               <hr>
               <h4 id="recital-2012-long-004">Intégration de paramètres lexicaux, syntaxiques et issus du système de traduction
                  automatique pour améliorer l’estimation des mesures de confiance au niveau des mots
               </h4>
               			Auteur : Luong Ngoc Quang -
               			Contact : Ngoc-Quang.Luong@imag.fr<br><p>L’estimation des mesures de confiance (MC) au niveau des mots consiste à prédire leur
                  exactitude dans la phrase cible générée par un système de traduction automatique.
                  Ceci permet d’estimer la fiabilité d'une sortie de traduction et de filtrer les segments
                  trop mal traduits pour une post-édition. Nous étudions l’impact sur le calcul des
                  MC de différents paramètres : lexicaux, syntaxiques et issus du système de traduction.
                  Nous présentons la méthode permettant de labelliser automatiquement nos corpus (mot
                  correct ou incorrect), puis le classifieur à base de champs aléatoires conditionnels
                  utilisé pour intégrer les différents paramètres et proposer une classification appropriée
                  des mots. Nous avons effectué des expériences préliminaires, avec l’ensemble des paramètres,
                  où nous mesurons la précision, le rappel et la F-mesure. Finalement nous comparons
                  les résultats avec notre système de référence. Nous obtenons de bons résultats pour
                  la classification des mots considérés comme corrects (F-mesure : 86.7%), et encourageants
                  pour ceux estimés comme mal traduits (F-mesure : 36,8%).
               </p><em>Version anglaise :</em><h4>Integrating Lexical, Syntactic and System-based Features to Improve Word Confidence
                  Estimation in SMT
               </h4>
               <p>Confidence Estimation at word level is the task of predicting the correct and incorrect
                  words in the target sentence generated by a MT system. It helps to conclude the reliability
                  of a given translation as well as to filter out sentences that are not good enough
                  for post-editing. This paper investigates various types of features to circumvent
                  this issue, including lexical, syntactic and system-based features. A method to set
                  training label for each word in the hypothesis is also presented. A classifier based
                  on conditional random fields (CRF) is employed to integrate features and determine
                  the word’s appropriate label. We conducted our preliminary experiment with all features,
                  tracked precision, recall and F-score and we compared with our baseline system. Experimental
                  results of the full combination of all features yield the very encouraging precision,
                  recall and F-score for Good label (F-score: 86.7%), and acceptable scores for Bad
                  label (F-score: 36.8%).
               </p>
               <hr>
               <h4 id="recital-2012-long-005">Application d’un algorithme de traduction statistique à la normalisation de textos</h4>
               			Auteur : Gabriel Bernier-Colborne -
               			Contact : gabriel.bernier-colborne@umontreal.ca<br><p>Ce travail porte sur l’application d’une technique de traduction statistique au problème
                  de la normalisation de textos. La méthode est basée sur l’algorithme de recherche
                  vorace décrit dans (Langlais et al., 2007). Une première normalisation est générée,
                  puis nous appliquons itérativement une fonction qui génère des nouvelles hypothèses
                  à partir de la normalisation courante, et maximisons une fonction de score. Cette
                  méthode fournit une réduction du taux d’erreurs moyen par phrase de 33 % sur le corpus
                  de test, et une augmentation du score BLEU de plus de 30 %. Nous mettons l’accent
                  sur les fonctions qui génèrent la normalisation initiale et sur les opérations permettant
                  de générer des nouvelles hypothèses.
               </p><em>Version anglaise :</em><h4>Applying a Statistical Machine Translation Algorithm to SMS Text Message Normalization</h4>
               <p>We report on the application of a statistical machine translation algorithm to the
                  problem of SMS text message normalization. The technique is based on a greedy search
                  algorithm described in (Langlais et al., 2007). A first normalization is generated,
                  then a function that generates new hypotheses is applied iteratively to a current
                  best guess, while maximizing a scoring function. This method leads to a drop in word
                  error rate of 33% on a held-out test set, and a BLEU score gain of over 30%. We focus
                  on the methods of generating the initial normalization and the operations that allow
                  us to generate new hypotheses.
               </p>
               <hr>
               <h4 id="recital-2012-long-006">Vers la correction automatique de textes bruités: Architecture générale et détermination
                  de la langue d’un mot inconnu
               </h4>
               			Auteur : Marion Baranes -
               			Contact : marion.baranes@viavoo.fr<br><p>Dans ce papier, nous introduisons le problème que pose la correction orthographique
                  sur des corpus de qualité très dégradée tels que les messages publiés sur les forums,
                  les sites d’avis ou les réseaux sociaux. Nous proposons une première architecture
                  de correction qui a pour objectif d’éviter au maximum la sur-correction. Nous présentons,
                  par ailleurs l’implémentation et les résultats d’un des modules de ce système qui
                  a pour but de détecter si un mot inconnu, dans une phrase de langue connue, est un
                  mot qui appartient à cette langue ou non.
               </p><em>Version anglaise :</em><h4>Towards Automatic Spell-Checking of Noisy Texts : General Architecture and Language
                  Identification for Unknown Words
               </h4>
               <p>This paper deals with the problem of spell checking on degraded-quality corpora such
                  as blogs, review sites and social networks. We propose a first architecture of correction
                  which aims at reducing overcorrection, and we describe its implementation. We also
                  report and discuss the results obtained thanks to the module that detects whether
                  an unknown word from a sentence in a known language belongs to this language or not.
               </p>
               <hr>
               <h4 id="recital-2012-long-007">Une plate-forme générique et ouverte pour l’acquisition des expressions polylexicales</h4>
               			Auteur : Carlos Ramisch -
               			Contact : Carlos.Ramisch@imag.fr<br><p>Cet article présente et évalue une plate-forme ouverte et flexible pour l’acquisition
                  automatique d’expressions polylexicales (EPL) à partir des corpus monolingues. Nous
                  commençons par une motivation pratique suivie d’une discussion théorique sur le comportement
                  et les défis posés par les EPL dans les applications de TAL. Ensuite, nous décrivons
                  les modules de notre plate-forme, leur enchaînement et les choix d’implémentation.
                  L’évaluation de la plate-forme a été effectuée à travers une applications : la lexicographie
                  assistée par ordinateur. Cette dernière peut bénéficier de l’acquisition d’EPL puisque
                  les expressions acquises automatiquement à partir des corpus peuvent à la fois accélérer
                  la création et améliorer la qualité et la couverture des ressources lexicales. Les
                  résultats prometteurs encouragent une recherche plus approfondie sur la manière optimale
                  d’intégrer le traitement des EPL dans de nombreuses applications de TAL, notamment
                  dans les systèmes traduction automatique.
               </p><em>Version anglaise :</em><h4>An Open and Generic Framework for the Acquisition of Multiword Expressions</h4>
               <p>In this paper, we present and evaluate an open and flexible methodological framework
                  for the automatic acquisition of multiword expressions (MWEs) from monolingual textual
                  corpora. We start with a pratical motivation followed by a theoretical discussion
                  of the behaviour and of the challenges that MWEs pose for NLP applications. Afterwards,
                  we describe the modules of our framework, the overall pipeline and the design choices
                  of the tool implementing the framework. The evaluation of the framework was performed
                  extrinsically based on an application : computerassisted lexicography. This application
                  can benefit from MWE acquisition because the expressions acquired automatically from
                  corpora can both speed up the creation and improve the quality and the coverage of
                  the lexical resources. The promising results of previous and ongoing experiments encourage
                  further investigation about the optimal way to integrate MWE treatment into NLP applications,
                  and particularly into machine translation systems.
               </p>
               <hr>
               <h4 id="recital-2012-long-008">Système de prédiction de néologismes formels : le cas des N suffixés par -IER dénotant
                  des artefacts
               </h4>
               			Auteur : Aurélie Merlo -
               			Contact : aurelie.merlo@etu.univ-lille3.fr<br><p>Nous présentons ici un système de prédiction de néologismes formels avec pour exemple
                  la génération automatique de néologismes nominaux suffixés par -IER dénotant des artefacts
                  (saladier, brassière, thonier). L’objectif de cet article est double. Il s’agira (i)
                  de mettre en évidence les contraintes de la suffixation par -IER afin de les implémenter
                  dans un système de génération morphologique puis (ii) de montrer qu’il est possible
                  de prédire les néologismes formels. Ce système de prédiction permettrait ainsi de
                  compléter automatiquement les lexiques pour le Traitement Automatique des Langues
                  (TAL).
               </p><em>Version anglaise :</em><h4>Prediction Device of Formal Neologisms : the Case of -IER Suffixed Nouns Denoting
                  Artifacts
               </h4>
               <p>We’ll introduce here a device that can predict neologisms using an example the automatical
                  generation of nominal neologisms suffixed by -IER denoting artifacts (saladier, brassière,
                  thonier). The aim of this article is double. We will first address the - IER suffixation
                  constraints in order to take them into account on the implementation of our morphological
                  generator. Second, we will describe our method to predict formal neologisms. Such
                  a method will permit to automatically enrich NLP lexicons.
               </p>
               <hr>
               <h4 id="recital-2012-long-009">Prémices d’une analyse syntaxique par transition pour des structures de dépendance
                  non-projectives
               </h4>
               			Auteur : Boris Karlov -
               			Contact : bnkarlov@gmail.com<br>
               			Auteur : Ophélie Lacroix -
               			Contact : ophelie.lacroix@univ-nantes.fr<br><p>L’article présente une extension de l’analyseur traditionnel en dépendances par transitions
                  adapté aux dépendances discontinues et les premiers résultats de son entraînement
                  sur un corpus de structures de dépendances de phrases en français. Les résultats des
                  premières expérimentations vont servir de base pour le choix des traits des configurations
                  de calcul bien adaptés aux dépendances discontinues pour améliorer l’apprentissage
                  des dépendances tête.
               </p><em>Version anglaise :</em><h4>Beginnings of a Transition-Based Parsing for Non-Projectives Dependency Structures</h4>
               <p>This paper presents an extension of the traditional transition-based dependency parser
                  adapted to discontinuous dependencies and the first results of its training on a dependency
                  tree corpus of French. The first experimental results will be useful for the choice
                  of parsing configuration features well adapted to discontinuous dependencies in order
                  to ameliorate learning of head dependencies.
               </p>
               <hr>
               <h4 id="recital-2012-long-010">Création d’un multi-arbre à partir d’un texte balisé : l’exemple de l’annotation d’un
                  corpus d’oral spontané
               </h4>
               			Auteur : Julie Belião -
               			Contact : julie@beliao.fr<br><p>Dans cette étude, nous nous intéressons au problème de l’analyse d’un corpus annoté
                  de l’oral. Le système d’annotation considéré est celui introduit par l’équipe des
                  syntacticiens du projet Rhapsodie. La principale problématique qui sous-tend un tel
                  projet est que la base écrite sur laquelle on travaille est en réalité une transcription
                  de l’oral, balisée par les annotateurs de manière à délimiter un ensemble de structures
                  arborescentes. Un tel système introduit plusieurs structures, en particulier macro
                  et micro-syntaxiques. Du fait de leur étroite imbrication, il s’est avéré difficile
                  de les analyser de façon indépendante et donc de travailler sur l’aspect macro-syntaxique
                  indépendamment de l’aspect micro-syntaxique. Cependant, peu d’études jusqu’à présent
                  considèrent ces problèmes conjointement et de manière automatisée. Dans ce travail,
                  nous présentons nos efforts en vue de produire un outil de parsing capable de rendre
                  compte à la fois de l’information micro et macro-syntaxique du texte annoté. Pour
                  ce faire, nous proposons une représentation partant de la notion de multi-arbre et
                  nous montrons comment une telle structure peut être générée à partir de l’annotation
                  et utilisée à des fins d’analyse.
               </p><em>Version anglaise :</em><h4>Creating a Multi-Tree from a Tagged Text : Annotating Spoken French</h4>
               <p>This study focuses on automatic analysis of annotated transcribed speech. The annotation
                  system considered has been recently introduced to address the several limitations
                  of classical syntactic annotations when faced to natural speech transcriptions. It
                  introduces many different components such as embedding, piles, kernels, pre-kernels,
                  discursive markers etc.. All those components are tightly coupled in a complex tree
                  structure and can hardly be considered separately because of their close intrication.
                  Hence, a joint analysis is required but no analysis tool to handle them all together
                  was available yet. In this study, we introduce such an automatic parser of annotated
                  transcriptions of speech and present the corresponding framework based on multi-trees.
                  This framework permits to jointly handle separate aspects of speech such as macro
                  and micro syntactic levels, which are traditionnaly considered separately. Several
                  applications are proposed, including analysis of the transcribed speech by classical
                  parsers designed for written language.
               </p>
               <hr>
               <h4 id="recital-2012-long-011">Construction automatique d’un lexique de modifieurs de polarité</h4>
               			Auteur : Noémi Boubel -
               			Contact : noemi.boubel@uclouvain.be<br><p>La recherche présentée 1 s’inscrit dans le domaine de la fouille d’opinion, domaine
                  qui consiste principalement à déterminer la polarité d’un texte ou d’une phrase. Dans
                  cette optique, le contexte autour d’un mot polarisé joue un rôle essentiel, car il
                  peut modifier la polarité initiale de ce terme. Nous avons choisi d’approfondir cette
                  question et de détecter précisément ces modifieurs de polarité. Une étude exploratoire,
                  décrite dans des travaux antérieurs, nous a permis d’extraire automatiquement des
                  adverbes qui jouent un rôle sur la polarité des adjectifs auxquels ils sont associés
                  et de préciser leur impact. Nous avons ensuite amélioré le système d’extraction afin
                  de construire automatiquement un lexique de structures lexico-syntaxiques modifiantes
                  associées au type d’impact qu’elles ont sur un terme polarisé. Nous présentons ici
                  le fonctionnement du système actuel ainsi que l’évaluation du lexique obtenu.
               </p><em>Version anglaise :</em><h4>Automatic Construction of a Contextual Valence Shifters Lexicon</h4>
               <p>The research presented in this paper takes place in the field of Opinion Mining, which
                  is mainly devoted to assigning a positive or negative label to a text or a sentence.
                  The context of a highly polarized word plays an essential role, as it can modify its
                  original polarity. The present work addresses this issue and focuses on the detection
                  of polarity shifters. In a previous study, we have automatically extracted adverbs
                  impacting the polarity of the adjectives they are associated to and qualified their
                  influence. The extraction system has then been improved to automatically build a lexicon
                  of contextual valence shifters. This lexicon contains lexico-syntactic patterns combined
                  with the type of influence they have on the valence of the polarized item. The purpose
                  of this paper is to show how the current system works and to present the evaluation
                  of the created lexicon.
               </p>
               <hr>
               <h4 id="recital-2012-long-012">Apport de la diacritisation dans l’analyse morphosyntaxique de l’arabe</h4>
               			Auteur : Ahmed Hamdi -
               			Contact : ahmed.hamdi@lif.univ-mrs.fr<br><p>Ce travail s’inscrit dans le cadre de l’analyse morphologique et syntaxique automatique
                  de la langue arabe. Nous nous intéressons au traitement de la diacritisation et à
                  son apport pour l’analyse morphologique. En effet, la plupart des analyseurs morphologiques
                  et des étiqueteurs morphosyntaxiques existants ignorent les diacritiques présents
                  dans le texte à analyser et commettent des erreurs qui pourraient être évitées. Dans
                  cet article, nous proposons une méthode qui prend en considération les diacritiques
                  lors de l’analyse, et nous montrons que cette prise en compte permet de diminuer considérablement
                  le taux d’erreur de l’analyse morphologique selon le taux de diacritiques du texte
                  traité.
               </p><em>Version anglaise :</em><h4>Apport of Diacritization in Arabic Morpho-Syntactic Analysis</h4>
               <p>This work is concerned with the automatic morphological and syntactical analysis of
                  the Arabic language. It focuses on diacritization and on its contribution to morphological
                  analysis. Most of existing morphological analyzers and syntactical taggers do not
                  take diacritics into account; as a consequence, they make mistakes that could have
                  been avoided. In this paper, we propose a method which process diacritics. We show
                  that doing so reduces considerably the morphological error rate, depending on the
                  diacritics rate in the input text.
               </p>
               <hr>
               <h4 id="recital-2012-long-013">Extraction d’indicateurs de construction collective de connaissances dans la formation
                  en ligne
               </h4>
               			Auteur : Alexandre Baudrillart -
               			Contact : alexandre.baudrillart@u-grenoble3.fr<br><p>Dans le cadre d’apprentissages humains assistés par des environnements informatiques,
                  les techniques de TAL ne sont que rarement employées ou restreintes à des tâches ou
                  des domaines spécifiques comme l’ALAO (Apprentissage de la Langue Assisté par Ordinateur)
                  où elles sont omniprésentes mais ne concernent que certaines dimensions du TAL. Nous
                  cherchons à explorer les possibilités ou les performances des techniques voire des
                  méthodes de TAL pour des systèmes moins spécifiques dès lors qu’une dimension de réseau
                  et de collectivité est présente. Plus particulièrement, notre objectif est d’obtenir
                  des indicateurs sur la construction collective de connaissances, et ses modalités.
                  Ce papier présente la problématique de notre thèse, son contexte, nos motivations
                  ainsi que nos premières réflexions.
               </p><em>Version anglaise :</em><h4>Collaborative Knowledge Building Indicators Extraction in Distance Learning</h4>
               <p>Natural Language Processing techniques are still not very much used within the field
                  of Technology Enhanced Learning. They are restricted to specific tasks or domains
                  such as CALL (standing for Computer Assisted Language Learning) in which they are
                  ubiquitous but do not match every linguistic aspect they could process. We are seeking
                  to explore possibilities or performances of thoses techniques for less specific systems
                  including a network or community aspect. More precisely, our goal is to get indicators
                  about collective knowledge building and its modalities. This paper presents the problem
                  and the background of our thesis problem, as well as our motivation and our first
                  reflections.
               </p>
               <hr>
               <h4 id="recital-2012-long-014">État de l’art : mesures de similarité sémantique locales et algorithmes globaux pour
                  la désambiguïsation lexicale à base de connaissances
               </h4>
               			Auteur : Andon Tchechmedjiev -
               			Contact : andon.tchechmedjiev@imag.fr<br><p>Dans cet article, nous présentons les principales méthodes non supervisées à base
                  de connaissances pour la désambiguïsation lexicale. Elles sont composées d’une part
                  de mesures de similarité sémantique locales qui donnent une valeur de proximité entre
                  deux sens de mots et, d’autre part, d’algorithmes globaux qui utilisent les mesures
                  de similarité sémantique locales pour trouver les sens appropriés des mots selon le
                  contexte à l’échelle de la phrase ou du texte.
               </p><em>Version anglaise :</em><h4>State of the art : Local Semantic Similarity Measures and Global Algorithmes for Knowledge-based
                  Word Sense Disambiguation
               </h4>
               <p>We present the main methods for unsupervised knowledge-based word sense disambiguation.
                  On the one hand, at the local level, we present semantic similarity measures, which
                  attempt to quantify the semantic proximity between two word senses. On the other hand,
                  at the global level, we present algorithms which use local semantic similarity measures
                  to assign the appropriate senses to words depending on their context, at the scale
                  of a text or of a corpus.
               </p>
               <hr>
               <h4 id="recital-2012-long-015">Compression textuelle sur la base de règles issues d'un corpus de sms</h4>
               			Auteur : Arnaud Kirsch -
               			Contact : arnaud.kirsch@student.uclouvain.be<br><p>La présente recherche cherche à réduire la taille de messages textuels sur la base
                  de techniques de compression observées, pour la plupart, dans un corpus de sms. Ce
                  papier explique la méthodologie suivie pour établir des règles de contraction. Il
                  présente ensuite les 33 règles retenues, et illustre les quatre niveaux de compression
                  proposés par deux exemples concrets, produits automatiquement par un premier prototype.
                  Le but de cette recherche n'est donc pas de produire de "l'écrit-sms", mais d'élaborer
                  un procédé de compression capable de produire des textes courts et compréhensibles
                  à partir de n'importe quelle source textuelle en français. Le terme "d'essentialisation"
                  est proposé pour désigner cette approche de réduction textuelle.
               </p><em>Version anglaise :</em><h4>Textual Compression Based on Rules Arising from a Corpus of Text Messages</h4>
               <p>The present research seeks to reduce the size of text messages on the basis of compression
                  techniques observed mostly in a corpus of sms. This paper explains the methodology
                  followed to establish compression rules. It then presents the 33 considered rules,
                  and illustrates the four suggested levels of compression with two practical examples,
                  automatically generated by a first prototype. This research’s main purpose is not
                  to produce "sms-language", but consists in designing a textual compression process
                  able to generate short and understandable texts from any textual source in French.
                  The term of "essentialization" is proposed to describe this approach of textual reduction.
               </p>
               <hr>
               <h4 id="recital-2012-long-016">Pour un étiquetage automatique des séquences verbales figées : état de l’art et approche
                  transformationnelle
               </h4>
               			Auteur : Aurélie Joseph -
               			Contact : joseph.aurelie@gmail.com<br><p>Cet article présente une approche permettant de reconnaitre automatiquement dans un
                  texte des séquences verbales figées (casser sa pipe, briser la glace, prendre en compte)
                  à partir d’une ressource. Cette ressource décrit chaque séquence en termes de possibilités
                  et de restrictions transformationnelles. En effet, les séquences figées ne le sont
                  pas complètement et nécessitent une description exhaustive afin de ne pas extraire
                  seulement les formes canoniques. Dans un premier temps nous aborderons les approches
                  traditionnelles permettant d’extraire des séquences phraséologiques. Par la suite,
                  nous expliquerons comment est constituée notre ressource et comment celle-ci est utilisée
                  pour un traitement automatique.
               </p><em>Version anglaise :</em><h4>For an Automatic Fixed Verbal Sequence Tagging: State of the Art and Transformational
                  Approach
               </h4>
               <p>This article presents a resource-based method aiming at automatically recognizing
                  fixed verbal sequences in French (i.e casser sa pipe, briser la glace, prendre en
                  compte) inside a text. This resource describes each sequence from the view-point of
                  transformational possibilities and restrictions. Fixed sequences are not totally fixed
                  and an exhaustive description is necessary to not only extract canonical forms. We
                  will first describe some transformational approaches that are able to extract phraseological
                  sequences. The building of the resource will be then addressed followed by our approach
                  to automatically recognize fixed sequences in corpora.
               </p>
               <hr>
               <h4 id="recital-2012-long-017">L’analyse de l’émotion dans les forums de santé</h4>
               			Auteur : Céline Battaïa -
               			Contact : celine.battaia@u-grenoble3.fr<br><p>Les travaux sur l’émotion dans les forums sont nombreux en Linguistique et Psychologie.
                  L’objectif de cette contribution est de proposer une analyse de l’émotion dans les
                  forums de santé selon l’angle des Sciences de l’Information et de la Communication
                  mais également selon une approche interdisciplinaire. Il s’agira ici, d’étudier l’émotion
                  comme un critère de pertinence lorsque des personnes malades effectuent des recherches
                  dans les forums. Ce papier introduit la méthodologie utilisée en traitement automatique
                  de la langue afin de répondre à cette interrogation. Ainsi, le travail présenté abordera
                  l’exploitation d’un corpus de messages de forums, la catégorisation semi-supervisée
                  et l’utilisation du logiciel NooJ pour traiter de manière automatique les données.
               </p><em>Version anglaise :</em><h4>Analysis of Emotion in Health Fora</h4>
               <p>Studies about emotion in fora are numerous in Linguistics and Psychology. This contribution
                  approaches this subject from an Information and Communication Sciences point of view,
                  and studies emotion as a criteron of pertinence for patients in a health forum. This
                  paper introduces the empirical step of automatic language processing in order to answer
                  this question, and uses data processing on the corpus of forum messages, semi-supervised
                  categorisation of messages and use of software NooJ for Natural Language Processing.
               </p>
               <hr>
               <h4 id="recital-2012-long-018">Peuplement d’une ontologie modélisant le comportement d’un environnement intelligent
                  guidé par l’extraction d’instances de relations
               </h4>
               			Auteur : Driss Sadoun -
               			Contact : driss.sadoun@limsi.fr<br><p>Nous présentons une approche de peuplement d’ontologie dont le but est de modéliser
                  le comportement de composants logiciels afin de faciliter le passage de descriptions
                  d’exigences en langue naturelle à des spécifications formelles. L’ontologie que nous
                  cherchons à peupler a été conçue à partir des connaissances du domaine de la domotique
                  et est initialisée à partir d’une description de la configuration physique d’un environnement
                  intelligent. Notre méthode est guidée par l’extraction d’instances de relations permettant
                  par là-même d’extraire les instances de concepts liés par ces relations. Nous construisons
                  des règles d’extraction à partir d’éléments issus de l’analyse syntaxique de descriptions
                  de besoins utilisateurs et de ressources terminologiques associées aux concepts et
                  relations de l’ontologie. Notre approche de peuplement se distingue par sa finalité
                  qui n’est pas d’extraire toutes les instances décrivant un domaine mais d’extraire
                  des instances pouvant participer sans conflit à un des multiples fonctionnements décrit
                  par des utilisateurs.
               </p><em>Version anglaise :</em><h4>Population of an Ontology Modeling the Behavior of an Intelligent Environment Guided
                  by Instance Relation Extractions
               </h4>
               <p>We present an approach for ontology population, which aims at modeling the behavior
                  of software components, for enabling a transition from natural language requirements
                  to formal specifications. The ontology was designed based on the knowledge of the
                  domotic domain and is initialized from a description of a physical configuration of
                  an intelligent environment. Our method focuses on extracting relation instances which
                  allows the extraction of concept instances linked by these relations. We built extraction
                  rules using elements coming from syntactic analysis of user need descriptions, semantic
                  and terminological resources linked to the knowledge contained in the ontology. Our
                  approach for ontology population, distinguishes itself by its purpose, which is not
                  to extract all instances describing a domain but to extract instances that can participate
                  without any conflict to one of the mutiple operation decribed by users.
               </p>
               <hr>
               <h4 id="recital-2012-long-019">De l'utilisation du dialogue naturel pour masquer les QCM au sein des jeux sérieux</h4>
               			Auteur : Franck Dernoncourt -
               			Contact : franck.dernoncourt@lip6.fr<br><p>Une des principales faiblesses des jeux sérieux à l'heure actuelle est qu'ils incorporent
                  très souvent des questionnaires à choix multiple (QCM). Or, aucune étude n'a démontré
                  que les QCM sont capables d'évaluer précisément le niveau de compréhension des apprenants.
                  Au contraire, certaines études ont montré expérimentalement que permettre à l'apprenant
                  d'entrer une phrase libre dans le programme au lieu de simplement cocher une réponse
                  dans un QCM rend possible une évaluation beaucoup plus fine des compétences de l'apprenant.
                  Nous proposons donc de concevoir un agent conversationnel capable de comprendre des
                  énoncés en langage naturel dans un cadre sémantique restreint, cadre correspondant
                  au domaine de compétence testé chez l'apprenant. Cette fonctionnalité est destinée
                  à permettre un dialogue naturel avec l'apprenant, en particulier dans le cadre des
                  jeux sérieux. Une telle interaction en langage naturel a pour but de masquer les QCM
                  sous-jacents. Cet article présente notre approche.
               </p><em>Version anglaise :</em><h4>Of the Use of Natural Dialogue to Hide MCQs in Serious Games</h4>
               <p>A major weakness of serious games at the moment is that they often incorporate multiple
                  choice questionnaires (MCQs). However, no study has demonstrated that MCQs can accurately
                  assess the level of understanding of a learner. On the contrary, some studies have
                  experimentally shown that allowing the learner to input a free-text answer in the
                  program instead of just selecting one answer in an MCQ allows a much finer evaluation
                  of the learner's skills. We therefore propose to design a conversational agent that
                  can understand statements in natural language within a narrow semantic context corresponding
                  to the area of competence on which we assess the learner. This feature is intended
                  to allow a natural dialogue with the learner, especially in the context of serious
                  games. Such interaction in natural language aims to hide the underlying MCQs. This
                  paper presents our approach.
               </p>
               <hr>
               <h4 id="recital-2012-long-020">ResTS : Système de Résumé Automatique des Textes d’Opinions basé sur Twitter et SentiWordNet</h4>
               			Auteur : Jihene Jmal -
               			Contact : fer.jmal_jihene@hotmail.fr<br><p>Comme le E-commerce est devenu de plus en plus populaire, le nombre de commentaires
                  des internautes est en croissance constante. Les opinions sur le Web affectent nos
                  choix et nos décisions. Il s’avère alors indispensable de traiter une quantité importante
                  de critiques des clients afin de présenter à l’utilisateur l’information dont il a
                  besoin dans la forme la plus appropriée. Dans cet article, nous présentons ResTS,
                  un nouveau système de résumé automatique de textes d’opinions basé sur les caractéristiques
                  des produits. Notre approche vise à transformer les critiques des utilisateurs en
                  des scores qui mesurent le degré de satisfaction des clients pour un produit donné
                  et pour chacune de ses caractéristiques. Ces scores sont compris entre 0 et 1 et peuvent
                  être utilisés pour la prise de décision. Nous avons étudié les opinions véhiculées
                  par les noms, les adjectifs, les verbes et les adverbes, contrairement aux recherches
                  précédentes qui utilisent essentiellement les adjectifs. Les résultats expérimentaux
                  préliminaires montrent que notre méthode est comparable aux méthodes classiques de
                  résumé automatique basées sur les caractéristiques des produits.
               </p><em>Version anglaise :</em><h4>System of Customer Review Summarization using Twitter and SentiWordNet</h4>
               <p>As E-commerce is becoming more and more popular, the number of customer reviews raises
                  rapidly. Opinions on the Web affect our choices and decisions. Thus, it is more efficient
                  to automatically process a mixture of reviews and prepare to the customer the required
                  information in an appropriate form. In this paper, we present ResTS, a new system
                  of feature-based opinion summarization. Our approach aims to turn the customer reviews
                  into scores that measure the customer satisfaction for a given product and its features.
                  These scores are between 0 and 1 and can be used for decision making and then help
                  users in their choices. We investigated opinions extracted from nouns, adjectives,
                  verbs and adverbs contrary to previous research which use only adjectives. Experimental
                  results show that our method performs comparably to classic feature-based summarization
                  methods.
               </p>
               <hr>
               <h4 id="recital-2012-long-021">Typologie des questions à réponses multiples pour un système de question-réponse</h4>
               			Auteur : Mathieu-Henri Falco -
               			Contact : falco@limsi.fr<br><p>L’évaluation des systèmes de question-réponse lors des campagnes repose généralement
                  sur la validité d’une réponse individuelle supportée par un passage (question factuelle)
                  ou d’un groupe de réponses toutes contenues dans un même passage (questions listes).
                  Ce cadre évaluatif empêche donc de fournir un ensemble de plusieurs réponses individuelles
                  et ne permet également pas de fournir des réponses provenant de documents différents.
                  Ce recoupement inter-documents peut être necessaire pour construire une réponse composée
                  de plusieurs éléments afin d’être le plus complet possible. De plus une grande majorité
                  de questions formulées au singulier et semblant n’attendre qu’une seule réponse se
                  trouve être des questions possédant plusieurs réponses correctes. Nous présentons
                  ici une typologie des questions à réponses multiples ainsi qu’un aperçu sur les problèmes
                  posés à un système de question-réponse par ce type de question.
               </p><em>Version anglaise :</em><h4>Typology of Multiple Answer Questions for a Question-answering System</h4>
               <p>The evaluation campaigns of question-answering systems are generally based on the
                  validity of an individual answer supported by a passage (for a factual question) or
                  a group of answers coming all from a same supporting passage (for a list question).
                  This framework does not allow the possibily to answer with a set of answers, nor with
                  answers gathered from several documents. This cross-checking can be needed for building
                  an answer composed of several elements in order to be as accurate as possible. Besides
                  a large majority of questions with a singular form seems to be answered with a single
                  answer whereas they can be satisfied with many. We present here a typology of questions
                  with multiple answers and an overview of problems encountered by a question-answering
                  system with this kind of questions.
               </p>
               <hr>
               <h4 id="recital-2012-long-022">Adaptation d’un système de reconnaissance d’entités nommées pour le français à l’anglais
                  à moindre coût
               </h4>
               			Auteur : Mohamed Hatmi -
               			Contact : mohamed.hatmi@univ-nantes.fr<br><p>La portabilité entre les langues des systèmes de reconnaissance d’entités nommées
                  est coûteuse en termes de temps et de connaissances linguistiques requises. L’adaptation
                  des systèmes symboliques souffrent du coût de développement de nouveaux lexiques et
                  de la mise à jour des règles contextuelles. D’un autre côté, l’adaptation des systèmes
                  statistiques se heurtent au problème du coût de préparation d’un nouveau corpus d’apprentissage.
                  Cet article étudie l’intérêt et le coût associé pour porter un système existant de
                  reconnaissance d’entités nommées pour du texte bien formé vers une autre langue. Nous
                  présentons une méthode peu coûteuse pour porter un système symbolique dédié au français
                  vers l’anglais. Pour ce faire, nous avons d’une part traduit automatiquement l’ensemble
                  des lexiques de mots déclencheurs au moyen d’un dictionnaire bilingue. D’autre part,
                  nous avons manuellement modifié quelques règles de manière à respecter la syntaxe
                  de la langue anglaise. Les résultats expérimentaux sont comparés à ceux obtenus avec
                  un système de référence développé pour l’anglais.
               </p><em>Version anglaise :</em><h4>Adapting a French Named Entity Recognition System to English with Minimal Costs</h4>
               <p>Cross-language portability of Named Entity Recognition systems requires linguistic
                  expertise and needs human effort. Adapting symbolic systems suffers from the cost
                  of developing new lexicons and updating grammar rules. Porting statistical systems
                  on the other hand faces the problem of the high cost of annotation of new training
                  corpus. This paper examines the cost of adapting a rule-based Named Entity Recognition
                  system designed for well-formed text to another language. We present a low-cost method
                  to adapt a French rule-based Named Entity Recognition system to English. We first
                  solve the problem of lexicon adaptation to English by simply translating the French
                  lexical resources. We then get to the task of grammar adaptation by slightly modifying
                  the grammar rules. Experimental results are compared to a state-of-the-art English
                  system.
               </p>
               <hr>
               <h4 id="recital-2012-long-023">Analyse automatique de discours en langue des signes : Représentation et traitement
                  de l’espace de signation
               </h4>
               			Auteur : Monia Ben Mlouka -
               			Contact : mlouka@irit.fr<br><p>En langue des signes, l’espace est utilisé pour localiser et faire référence à certaines
                  entités dont l’emplacement est important pour la compréhension du sens. Dans cet article,
                  nous proposons une représentation informatique de l’espace de signation et les fonctions
                  de création et d’accès associées, afin d’analyser les gestes manuels et non manuels
                  qui contribuent à la localisation et au référencement des signes et de matérialiser
                  leur effet. Nous proposons une approche bi-directionnelle qui se base sur l’analyse
                  de données de capture de mouvement de discours en langue des signes dans le but de
                  caractériser les événements de localisation et de référencement.
               </p><em>Version anglaise :</em><h4>Automatic Analysis of Discourse in Sign Language : Signing Space Representation and
                  Processing
               </h4>
               <p>In sign language, signing space is used to locate and refer to entities whose locations
                  are important for understanding the meaning. In this paper, we propose a computer-based
                  representation of the signing space and their associated functions. It aims to analyze
                  manual and non-manual gestures, that contribute to locating and referencing signs,
                  and to make real their effect. For that, we propose an approach based on the analysis
                  of motion capture data of entities’ assignment and activation events in the signing
                  space.
               </p>
               <hr>
               <h4 id="recital-2012-long-024">État de l’art : l’influence du domaine sur la classification de l’opinion, Dis-moi
                  de quoi tu parles, je te dirai ce que tu penses
               </h4>
               			Auteur : Morgane Marchand -
               			Contact : morgane.marchand@cea.fr<br><p>L’intérêt pour la fouille d’opinion s’est développé en même temps que se sont répandus
                  les blogs, forums et autres plate-formes où les internautes peuvent librement exprimer
                  leur opinion. La très grande quantité de données disponibles oblige à avoir recours
                  à des traitements automatiques de fouille d’opinion. Cependant, la manière dont les
                  gens expriment leur avis change selon ce dont ils parlent. Les distributions des mots
                  utilisés sont différentes d’un domaine à l’autre. Aussi, il est très difficile d’obtenir
                  un classifieur d’opinion fonctionnant sur tous les domaines. De plus, on ne peut appliquer
                  sans adaptation sur un domaine cible un classifieur entraîné sur un domaine source
                  différent. L’objet de cet article est de recenser les moyens de résoudre ce problème
                  difficile.
               </p><em>Version anglaise :</em><h4>State of the Art : Influence of Domain on Opinion Classification</h4>
               <p>The interest in opinion mining has grown concurrently with blogs, forums, and others
                  platforms where the internauts can freely write about their opinion on every topic.
                  As the amounts of available data are increasingly huge, the use of automatic methods
                  for opinion mining becomes imperative. However, sentiment is expressed differently
                  in different domains : words distributions can indeed differ significantly. An effective
                  global opinion classifier is therefore hard to develop. Moreover, a classifier trained
                  on a source domain can’t be used without adaptation on a target domain. This article
                  aims to describe the state-of-the-art methods used to solve this difficult task.
               </p>
               <hr>
               <h4 id="recital-2012-long-025">État de l’art sur l’acquisition de relations sémantiques entre termes : contextualisation
                  des relations de synonymie
               </h4>
               			Auteur : Mounira Manser -
               			Contact : manser.mounira@gmail.com<br><p>L’accès au contenu des textes de spécialité est une tâche difficile à réaliser. Cela
                  nécessite la définition de méthodes automatiques ou semi-automatiques pour identifier
                  des relations sémantiques entre les termes que contiennent ces textes. Nous distinguons
                  les approches de TAL permettant d’acquérir ces relations suivant deux types d’information
                  : la structure interne des termes ou le contexte de ces termes en corpus. Afin d’améliorer
                  la qualité des relations acquises et faciliter leur réutilisation en corpus, nous
                  nous intéressons à la prise en compte du contexte dans une méthode d’acquisition de
                  relations de synonymie basée sur l’utilisation de la structure interne des termes.
                  Nous présentons les résultats d’une expérience préliminaire tenant compte de l’usage
                  des termes dans un corpus biomédical en anglais. Nous donnons quelques pistes de travail
                  pour définir des contraintes sémantiques sur les relations de synonymie acquises.
               </p><em>Version anglaise :</em><h4>State of the Art on the Acquisition of Semantic Relations between Terms : Contextualisation
                  of the Synonymy Relations
               </h4>
               <p>Accessing to the context of specialised texts is a crucial but difficult task. It
                  requires automatic or semi-automatic methods dedicated to the identification of semantic
                  relations between terms appearing in the texts. NLP approaches for acquiring semantic
                  relations between terms can be distinguished according to the type of information
                  : the internal structure of the terms and the term context. In order to improve the
                  quality of the acquired synonymy relations and their reusability in other corpora,
                  we aim at taking into account the context into an approach based on the internal structure
                  of the terms. We present the results of a preliminary experiment taking into account
                  the use of the terms in a English biomedical corpora. This experiment will be helpful
                  to add semantic constraints to the already acquired synonymy relations.
               </p>
               <hr>
               <h4 id="recital-2012-long-026">Extraction de PCFG et analyse de phrases pré-typées</h4>
               			Auteur : Noémie-Fleur Sandillon-Rezer -
               			Contact : nfsr@labri.fr<br><p>Cet article explique la chaîne de traitement suivie pour extraire une grammaire PCFG
                  à partir du corpus de Paris VII. Dans un premier temps cela nécessite de transformer
                  les arbres syntaxiques du corpus en arbres de dérivation d’une grammaire AB, ce que
                  nous effectuons en utilisant un transducteur d’arbres généralisé ; il faut ensuite
                  extraire de ces arbres une PCFG. Le transducteur d’arbres généralisé est une variation
                  des transducteurs d’arbres classiques et c’est l’extraction de la grammaire à partir
                  des arbres de dérivation qui donnera l’aspect probabiliste à la grammaire. La PCFG
                  extraite est utilisée via l’algorithme CYK pour l’analyse de phrases.
               </p><em>Version anglaise :</em><h4>PCFG Extraction and Pre-typed Sentences Analysis</h4>
               <p>This article explains the way we extract a PCFG from the Paris VII treebank. Firslty,
                  we need to transform the syntactic trees of the corpus into derivation trees. The
                  transformation is done with a generalized tree transducer, a variation of the usual
                  top-down tree transducers, and gives as result some derivation trees for an AB grammar.
                  Secondely, we have to extract a PCFG from the derivation trees. For this, we assume
                  that the derivation trees are representative of the grammar. The extracted grammar
                  is used, via the CYK algorithm, for sentence analysis.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="RECITAL'2013">2013 - 
               					RECITAL'2013 - Président(s) :  
               					Florian Boudin - 
               	Loïc Barrault - 
               	 à 
               					Sables d'Olonne
            </h2>
            
            				Il y a 18 articles référencés. 
            
            				<br>
            			Statistiques :
            			
            <ul>
               <li>25 articles long soumis dont 18 acceptés. </li>
            </ul><br>
            			Il n'y a aucun meilleur article. 
            		
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="recital-2013-long-001">Acquisition de lexique bilingue d’expressions polylexicales: Une application à la
                  traduction automatique statistique
               </h4>
               			Auteur : Dhouha Bouamor -
               			Contact : dhouha.bouamor@cea.fr<br><p>Cet article décrit une méthode permettant d’acquérir un lexique bilingue d’expressions
                  polylexicales (EPLS) à partir d’un corpus parallèle français-anglais. Nous identifions
                  dans un premier temps les EPLS dans chaque partie du corpus parallèle. Ensuite, nous
                  proposons un algorithme d’alignement assurant la mise en correspondance bilingue d’EPLS.
                  Pour mesurer l’apport du lexique construit, une évaluation basée sur la tâche de Traduction
                  Automatique Statistique (TAS) est menée. Nous étudions les performances de trois stratégies
                  dynamiques et d’une stratégie statique pour intégrer le lexique bilingue d’expressions
                  polylexicales dans un système de TAS. Les expériences menées dans ce cadre montrent
                  que ces unités améliorent significativement la qualité de traduction.
               </p><em>Version anglaise :</em><h4>Mining a Bilingual Lexicon of MultiWord Expressions : A Statistical Machine Translation
                  Evaluation Perspective
               </h4>
               <p>This paper describes a method aiming to construct a bilingual lexicon of MultiWord
                  Expressions (MWES) from a French-English parallel corpus. We first extract monolingual
                  MWES from each part of the parallel corpus. The second step consists in acquiring
                  bilingual correspondences of MWEs. In order to assess the quality of the mined lexicon,
                  a Statistical Machine Translation (SMT) task-based evaluation is conducted. We investigate
                  the performance of three dynamic strategies and of one static strategy to integrate
                  the mined bilingual MWES lexicon in a SMT system. Experimental results show that such
                  a lexicon significantly improves the quality of translation.
               </p>
               <hr>
               <h4 id="recital-2013-long-002">Quelques variations sur les mesures de comparabilité quantitatives et évaluations
                  sur des corpus comparables Français-Anglais synthétiques
               </h4>
               			Auteur : Guiyao Ke -
               			Contact : guiyao.ke@univ-ubs.fr<br><p>Dans la suite des travaux de (Li et Gaussier, 2010) nous abordons dans cet article
                  l'analyse d'une famille de mesures quantitatives de comparabilité pour la construction
                  ou l'évaluation des corpus comparables. Après avoir rappelé la définition de la mesure
                  de comparabilité proposée par (Li et Gaussier, 2010), nous développons quelques variantes
                  de cette mesure basées principalement sur la prise en compte des fréquences d'occurrences
                  des entrées lexicales et du nombre de leurs traductions. Nous comparons leurs avantages
                  et inconvénients respectifs dans le cadre d'expérimentations basées sur la dégradation
                  progressive du corpus parallèle Europarl par remplacement de blocs selon la méthodologie
                  suivie par (Li et Gaussier, 2010). L'impact sur ces mesures des taux de couverture
                  des dictionnaires bilingues vis-à-vis des blocs considérés est également examiné.
               </p><em>Version anglaise :</em><h4>Some variations on quantitative comparability measures and evaluations on synthetic
                  French-English comparable corpora
               </h4>
               <p>Following the pioneering work by (Li et Gaussier, 2010) we address in this paper the
                  analysis of a family of quantitative measures of comparability dedicated to the construction
                  or evaluation of comparable corpora. After recalling the definition of the comparability
                  measure proposed by (Li et Gaussier, 2010), we develop some variants of this measure
                  based primarily on the consideration of the occurrence frequency of lexical entries
                  and the number of their translations. We compare the respective advantages and disadvantages
                  of these variants in the context of an experiments based on the progressive degradation
                  of the Europarl parallel corpus, by replacing blocks according to the methodology
                  followed by (Li et Gaussier, 2010). The impact of the coverage of bilingual dictionaries
                  on these measures is also discussed.
               </p>
               <hr>
               <h4 id="recital-2013-long-003">Inférence grammaticale guidée par clustering</h4>
               			Auteur : Noémie-Fleur Sandillon-Rezer -
               			Contact : nfsr@labri.fr<br><p>Dans cet article, nous nous focalisons sur la manière d’utiliser du clustering hiérarchique
                  pour apprendre une grammaire AB à partir d’arbres de dérivation partiels. Nous décrirons
                  brièvement les grammaires AB ainsi que les arbres de dérivation dont nous nous servons
                  comme entrée pour l’algorithme, puis la manière dont nous extrayons les informations
                  des corpus arborés pour l’étape de clustering. L’algorithme d’unification, dont le
                  pivot est le cluster, sera décrit et les résultats analysés en détails.
               </p><em>Version anglaise :</em><h4>Clustering for categorial grammar induction</h4>
               <p>In this article, we describe the way we use hierarchical clustering to learn an AB
                  grammar from partial derivation trees. We describe AB grammars and the derivation
                  trees we use as input for the clustering, then the way we extract information from
                  Treebanks for the clustering. The unification algorithm, based on the information
                  extracted from our cluster, will be explained and the results discussed.
               </p>
               <hr>
               <h4 id="recital-2013-long-004">Améliorer l’extraction et la description d’expressions polylexicales grâce aux règles
                  transformationnelles
               </h4>
               			Auteur : Aurélie Joseph -
               			Contact : joseph.aurelie@gmail.com<br><p>Cet article présente une méthodologie permettant d’extraire et de décrire des locutions
                  verbales vis-à-vis de leur comportement transformationnel. Plusieurs objectifs sont
                  ciblés : 1) extraire automatiquement les expressions phraséologiques et en particulier
                  les expressions figées, 2) décrire linguistiquement le comportement des phraséologismes
                  3) comparer les méthodes statistiques et notre approche et enfin 4) montrer l’importance
                  de ces expressions dans un outil de classification de textes.
               </p><em>Version anglaise :</em><h4>Enhance Multiword Expressions Extraction and Description with Transformational Rules</h4>
               <p>This paper presents a methodology to extract and describe verbal multiword expressions
                  using their transformational behavior. Several objectives are targeted: 1) automatically
                  extracting MWE and especially frozen expression, 2) describing linguistically their
                  MWE behavior, 3) comparing statistical methods and our approach, and finally 4) showing
                  the importance of MWE in a text classification tool.
               </p>
               <hr>
               <h4 id="recital-2013-long-005">Construction de corpus multilingues : état de l’art</h4>
               			Auteur : Manuela Yapomo -
               			Contact : yapomodomkem@etu.unistra.fr<br><p>Les corpus multilingues sont extensivement exploités dans plusieurs branches du traitement
                  automatique des langues. Cet article présente une vue d’ensemble des travaux en construction
                  automatique de ces corpus. Nous traitons ce sujet en donnant premièrement un aperçu
                  de différentes perceptions de la comparabilité. Nous examinons ensuite les principales
                  approches de calcul de similarité, de construction et d’évaluation développées dans
                  le domaine. Nous observons que Le calcul de la similarité textuelle se fait généralement
                  sur la base de statistiques de corpus, de la structure de ressources ontologiques
                  ou de la combinaison de ces deux approches. Dans un cadre multilingue avec l’utilisation
                  d’un dictionnaire multilingue ou d’un traducteur automatique, de nombreux problèmes
                  apparaissent. L’exploitation d’une ressource ontologique multilingue semble être une
                  solution. En classification, la problématique de l’ajout de documents à la base initiale
                  sans affecter la qualité des clusters demeure ouverte.
               </p><em>Version anglaise :</em><h4>Multilingual document clustering : state of the art</h4>
               <p>Multilingual corpora are extensively exploited in several branches of natural language
                  processing. This paper presents an overview of works in the automatic construction
                  of such corpora. We address this topic by first providing an overview of different
                  perceptions of comparability. We then examine the main approaches to similarity computation,
                  construction and evaluation developed in the field. We notice that the measurement
                  of the textual similarity is usually based on corpus statistics or the structure of
                  ontological resources or on a combination of these two approaches. In a multilingual
                  framework, with the use of a multilingual dictionary or a machine translator, many
                  problems arise. The exploitation of a multilingual ontological ressource seems to
                  be a worthy option. In clustering, the problem of adding documents to the initial
                  base without affecting the quality of clusters remains open.
               </p>
               <hr>
               <h4 id="recital-2013-long-006">Développement de ressources pour l’entrainement et l’utilisation de l’étiqueteur morphosyntaxique
                  TreeTagger sur l’arabe
               </h4>
               			Auteur : Dhaou Ghoul -
               			Contact : Dhaou.Ghoul@gmail.com<br><p>Dans cet article, nous présentons les étapes du développement de ressources pour l’entraînement
                  et l’utilisation d’un nouvel outil de l’étiquetage morphosyntaxique de la langue arabe.
                  Nous avons mis en oeuvre un système basé sur l'étiqueteur stochastique TreeTagger,
                  réputé pour son efficacité et la généricité de son architecture. Pour ce faire, nous
                  avons commencé par la constitution de notre corpus de travail. Celui-ci nous a d'abord
                  servi à réaliser l'étape de segmentation lexicale. Dans un second temps, ce corpus
                  a permis d'effectuer l'entrainement de TreeTagger, grâce à un premier étiquetage réalisé
                  avec l'étiqueteur ASVM 1.0, suivi d'une phase de correction manuelle. Nous détaillons
                  ainsi les prétraitements requis, et les différentes étapes de la phase d'apprentissage
                  avec cet outil. Nous terminons par une évaluation sommaire des résultats, à la fois
                  qualitative et quantitative. Cette évaluation, bien que réalisée sur un corpus de
                  test de taille modeste, montre que nos premiers résultats sont encourageants.
               </p><em>Version anglaise :</em><h4>Development of resources for training and the use of the tagger TreeTagger on Arabic</h4>
               <p>In this paper, we present the steps of the development of resources for training and
                  the use of a new tool for the part-of-speech tagging of Arabic. We implemented a tagging
                  system based on TreeTagger, a generic stochastic tagging tool, very popular for its
                  efficiency. First of all, we began by gathering a working corpus, large enough to
                  ensure a general linguistic coverage. This corpus has been used to implement the tokenization
                  process, as well as to train TreeTagger. We first present our method of tokenization,
                  then we describe all the steps of the preprocessing and training process, using ASVM
                  1.0 to yield a raw POS tagging that was subsequently manually corrected. Finally,
                  we implemented a straightforward evaluation of the outputs, both in a quantitative
                  and qualitative way, on a small test corpus. Though restricted, this evaluation showed
                  really encouraging results.
               </p>
               <hr>
               <h4 id="recital-2013-long-007">Détection de polarité d’opinions dans les forums en langue arabe par fusion de plusieurs
                  SVM
               </h4>
               			Auteur : Amel Ziani -
               			Contact : Z_amel1911@live.fr<br>
               			Auteur : Nabiha Azizi -
               			Contact : yamina.tlili@univ-annaba.org<br>
               			Auteur : Yamina Tlili Guiassa -
               			Contact : nabiha.azizi@univ-annaba.org<br><p>Cet article décrit notre contribution sur la détection de polarité d’opinions en langue
                  arabe par apprentissage supervisé. En effet le système proposé comprend trois phases:
                  le prétraitement du corpus, l’extraction des caractéristiques et la classification.
                  Pour la deuxième phase, nous utilisons vingt caractéristiques dont les principales
                  sont l’émotivité, la réflexivité, l’adressage et la polarité. La phase de classification
                  représente dans notre travail la combinaison des plusieurs classifieurs SVMs (Machine
                  à Vecteur de Support) pour résoudre le problème multi classes. Nous avons donc analysés
                  les deux stratégies de SVM multi classes qui sont : « un contre tous » et « un contre
                  un » afin de comparer les résultats et améliorer la performance du système global.
               </p><em>Version anglaise :</em><h4>Polarity Opinion Detection in Arabic Forums by Fusing Multiple SVMs</h4>
               <p>This article describes our contribution on the polarity’s detection of opinions in
                  Arabian language by supervised training. Indeed the proposed system consists of three
                  phases: the pretreatment of the corpus, the extraction of the features and the classification.
                  For the second phase, we use twenty features of which the main are emotionalism, the
                  reflexivity, the adressage and the polarity. The phase of classification represents
                  in our work the combination of the several SVMs (Support Vector Machine),to solve
                  the multi class problem. We analyzed the two strategies of the SVMs multi class that
                  are: "one against all" and "one against one" in order to compare the results and to
                  improve the performance of the global system.
               </p>
               <hr>
               <h4 id="recital-2013-long-008">État de l’art des méthodes d’extraction automatique de termes-clés</h4>
               			Auteur : Adrien Bougouin -
               			Contact : adrien.bougouin@univ-nantes.fr<br><p>Cet article présente les principales méthodes d’extraction automatique de termes-clés.
                  La tâche d’extraction automatique de termes-clés consiste à analyser un document pour
                  en extraire les expressions (phrasèmes) les plus représentatives de celui-ci. Les
                  méthodes d’extraction automatique de termes-clés sont réparties en deux catégories
                  : les méthodes supervisées et les méthodes non supervisées. Les méthodes supervisées
                  réduisent la tâche d’extraction de termes-clés à une tâche de classification binaire
                  (tous les phrasèmes sont classés parmi les termesclés ou les non termes-clés). Cette
                  classification est possible grâce à une phase préliminaire d’apprentissage, phase
                  qui n’est pas requise par les méthodes non-supervisées. Ces dernières utilisent des
                  caractéristiques (traits) extraites du document analysé (et parfois d’une collection
                  de documents de références) pour vérifier des propriétés permettant d’identifier ses
                  termes-clés.
               </p><em>Version anglaise :</em><h4>State of the Art of Automatic Keyphrase Extraction Methods</h4>
               <p>This article presents the state of the art of the automatic keyphrase extraction methods.
                  The aim of the automatic keyphrase extraction task is to extract the most representative
                  terms of a document. Automatic keyphrase extraction methods can be divided into two
                  categories : supervised methods and unsupervised methods. For supervised methods,
                  the task is reduced to a binary classification where terms are classified as keyphrases
                  or non keyphrases. This classification requires a learning step which is not required
                  by unsupervised methods. The unsupervised methods use features extracted from the
                  analysed document (sometimes a document collection) to check properties which allow
                  keyphrase identification.
               </p>
               <hr>
               <h4 id="recital-2013-long-009">Influence de l’étiquetage syntaxique des têtes sur l’analyse en dépendances discontinues
                  du français
               </h4>
               			Auteur : Ophélie Lacroix -
               			Contact : ophelie.lacroix@univ-nantes.fr<br><p>Dans cet article nous souhaitons mettre en évidence l’utilité d’un étiquetage syntaxique
                  appliqué en amont d’une analyse syntaxique en dépendances. Les règles de la grammaire
                  catégorielle de dépendances du français utilisées pour l’analyse gèrent les dépendances
                  discontinues et les relations syntaxiques à longue distance. Une telle méthode d’analyse
                  génère un nombre conséquent de structures de dépendances et emploie un temps d’analyse
                  trop important. Nous voulons alors montrer qu’une méthode locale d’étiquetage peut
                  diminuer l’ampleur de ces difficultés et par la suite aider à résoudre le problème
                  global de désambiguïsation d’analyse en dépendances. Nous adaptons alors une méthode
                  d’étiquetage aux catégories de la grammaire catégorielle de dépendance. Nous obtenons
                  ainsi une pré-sélection des têtes des dépendances permettant de réduire l’ambiguïté
                  de l’analyse et de voir que les résultats locaux d’une telle méthode permettent de
                  trouver des relations distantes de dépendances.
               </p><em>Version anglaise :</em><h4>On the Effect of Head Tagging on Parsing Discontinuous Dependencies in French</h4>
               <p>In this paper we want to show the strong impact of syntactic tagging on syntactic
                  dependency parsing. The rules of categorial dependency grammar used to parse French
                  deal with discontinuous dependencies and long distance syntactic relations. Such parsing
                  method produces a substantial number of dependency structures and takes too much parsing
                  time. We want to show that a local tagging method can reduce these problems and help
                  to solve the global problem of dependency parsing disambiguation. Then we adapt a
                  tagging method to types of the categorial dependency grammar. We obtain a dependency-head
                  pre-selection allowing to reduce parsing ambiguity and to see that we can find distant
                  relation of dependencies through local results of such method.
               </p>
               <hr>
               <h4 id="recital-2013-long-010">Une approche linguistique pour l'extraction des connaissances dans un texte arabe</h4>
               			Auteur : Houda Saadane -
               			Contact : houda.saadane@e.u-grenoble3.fr<br><p>Nous présentons dans cet article un système d'extraction de connaissances en arabe,
                  fondé sur une analyse morphosyntaxique profonde. Ce système reconnaît les mots simples,
                  les expressions idiomatiques, les mots composés et les entités nommées. L'analyse
                  identifie aussi les relations syntaxiques de dépendance et traite les formes passives
                  et actives. L’extraction des connaissances est propre à l’application et utilise des
                  règles d’extraction sémantiques qui s'appuient sur le résultat de l'analyse morphosyntaxique.
                  A ce niveau, le type de certaines entités nommées peut être révisé. L'extraction se
                  base, dans nos expérimentations, sur une ontologie dans le domaine de la sécurité.
                  Le RDF (Resource Description Framework) produit est ensuite traité pour regrouper
                  les informations qui concernent un même événement ou une même entité nommée. Les informations
                  ainsi extraites peuvent alors aider à appréhender les informations contenues dans
                  un ensemble de textes, alimenter une base de connaissances, ou bien servir à  des
                  outils de veille.
               </p><em>Version anglaise :</em><h4>A linguistic approach for knowledge extraction from an Arabic text</h4>
               <p>We present in this paper a knowledge extraction system for Arabic. The information
                  extraction is based on a deep morphosyntactic analysis. It also recognizes single
                  words, idiomatic expressions, compounds and named entities. The analysis also identifies
                  dependency relations, verb tenses and passive/active forms. Information extraction
                  is application-independent and uses extraction rules that rely on the result of the
                  morphosyntactic analysis. At this level, some named entity categories can be reconsidered.
                  This extraction is based in our experimentations on the security ontology. The Resource
                  Description Framework (RDF) obtained is then processed to gather information concerning
                  a single event or named entity. The information extracted can help to understand the
                  information contained in a set of texts, to infer knowledge into a knowledge base,
                  or be used for monitoring tools.
               </p>
               <hr>
               <h4 id="recital-2013-long-011">Extraction des mots simples du lexique scientifique transdisciplinaire dans les écrits
                  de sciences humaines : une première expérimentation
               </h4>
               			Auteur : Sylvain Hatier -
               			Contact : sylvain.hatier@u-grenoble3.fr<br><p>Nous présentons dans cet article les premiers résultats de nos travaux sur l'extraction
                  de mots simples appartenant au lexique scientifique transdisciplinaire sur un corpus
                  analysé morpho-syntaxiquement composé d'articles de recherche en sciences humaines
                  et sociales. La ressource générée sera utilisée lors de l'indexation automatique de
                  textes comme filtre d'exclusion afin d'isoler ce lexique de la terminologie. Nous
                  comparons plusieurs méthodes d'extraction et montrons qu'un premier lexique de mots
                  simples peut être dégagé et que la prise en compte des unités polylexicales ainsi
                  que de la distribution seront nécessaires par la suite afin d'extraire l'ensemble
                  de la phraséologie transdisciplinaire.
               </p><em>Version anglaise :</em><h4>Extraction of academic lexicon's simple words in humanities writings</h4>
               <p>This paper presents a first extraction of academic lexicon's simple words in french
                  academic writings in the fields of humanities and social sciences through a corpus
                  study of research articles using morpho-syntactic analysis. This academic lexicon
                  resource will be used for automatic indexing as a stoplist in order to exclude this
                  lexicon from the terminology. We try various extraction methods and show that a first
                  simple words lexicon can be generated but that multiwords expressions and words distribution
                  should be taken into consideration to extract academic phraseology.
               </p>
               <hr>
               <h4 id="recital-2013-long-012">Vers une identification automatique du chiasme de mots</h4>
               			Auteur : Marie Dubremetz -
               			Contact : marie.dubremetz@lingfil.uu.se<br><p>Cette recherche porte sur le chiasme de mots : figure de style jouant sur la réversion
                  (ex. « Bonnet blanc, blanc bonnet »). Elle place le chiasme dans la problématique
                  de sa reconnaissance automatique : qu’est-ce qui le définit et comment un ordinateur
                  peut le trouver ? Nous apportons une description formelle du phénomène. Puis nous
                  procédons à la constitution d’une liste d’exemples contextualisés qui nous sert au
                  test des hypothèses. Nous montrons ainsi que l’ajout de contraintes formelles (contrôle
                  de la ponctuation et omission des mots vides) pénalise très peu le rappel et augmente
                  significativement la précision de la détection. Nous montrons aussi que la lemmatisation
                  occasionne peu d’erreurs pour le travail d’extraction mais qu’il n’en est pas de même
                  pour la racinisation. Enfin nous mettons en évidence que l’utilisation d’un thésaurus
                  apporte quelques résultats pertinents.
               </p><em>Version anglaise :</em><h4>Towards an automatic identification of chiasmus of words</h4>
               <p>This article summarises the study of the rhetorical figure “chiasmus” (e.g : “Quitters
                  never win and winners never quit.”). We address the problem of its computational identification.
                  How can a computer identify this automatically ? For this purpose this article will
                  provide a formal description of the phenomenon. First, we put together an annotated
                  text for testing our hypothesis. At the end we demonstrate that the use of stopword
                  lists and the identification of the punctuation improve the precision of the results
                  with very little impact on the recall. We discover also that using lemmatization improves
                  the results but stemming doesn’t. Finally we see that a French thesaurus provided
                  us with good results on the most elaborate form of chiasmus.
               </p>
               <hr>
               <h4 id="recital-2013-long-013">Représentation des connaissances du DEC: Concepts fondamentaux du formalisme des Graphes
                  d’Unités
               </h4>
               			Auteur : Maxime Lefrançois -
               			Contact : maxime.lefrancois@inria.fr<br><p>Dans cet article nous nous intéressons au choix d’un formalisme de représentation
                  des connaissances qui nous permette de représenter, manipuler, interroger et raisonner
                  sur des connaissances linguistiques du Dictionnaire Explicatif et Combinatoire (DEC)
                  de la Théorie Sens-Texte. Nous montrons que ni les formalismes du web sémantique ni
                  le formalisme des Graphes conceptuels n’est adapté pour cela, et justifions l’introduction
                  d’un nouveau formalisme dit des Graphes d’Unités. Nous introduisons la hiérarchie
                  des Types d’Unités au coeur du formalisme, et présentons les Graphes d’Unités ainsi
                  que la manière dont on peut les utiliser pour représenter certains aspects du DEC.
               </p><em>Version anglaise :</em><h4>ECD Knowledge Representation : Fundamental Concepts of the Unit Graphs Framework</h4>
               <p>In this paper we are interested in the choice of a knowledge representation formalism
                  that enables the representation, manipulation, query, and reasoning over linguistic
                  knowledge of the Explanatory and Combinatorial Dictionary (ECD) of the Meaning-Text
                  Theory. We show that neither the semantic web formalisms nor the Conceptual Graphs
                  Formalism suit our needs, and justify the introduction of a new formalism denoted
                  Unit Graphs. We introduce the core of this formalism which is the Unit Types hierarchy,
                  and present Unit Graphs and how one may use them to represent aspects of the ECD.
               </p>
               <hr>
               <h4 id="recital-2013-long-014">Vers un système générique de réécriture de graphes pour l’enrichissement de structures
                  syntaxiques
               </h4>
               			Auteur : Corentin Ribeyre -
               			Contact : corentin.ribeyre@inria.fr<br><p>Ce travail présente une nouvelle approche pour injecter des dépendances profondes
                  (sujet des verbes à contrôle, partage du sujet en cas d’ellipses, ...) dans un corpus
                  arboré présentant un schéma d’annotation surfacique et projectif. Nous nous appuyons
                  sur un système de réécriture de graphes utilisant des techniques de programmation
                  par contraintes pour produire des règles génériques qui s’appliquent aux phrases du
                  corpus. Par ailleurs, nous testons la généricité des règles en utilisant des sorties
                  de trois analyseurs syntaxiques différents, afin d’évaluer la dégradation exacte de
                  l’application des règles sur des analyses syntaxiques prédites.
               </p><em>Version anglaise :</em><h4>Towards a generic graph rewriting system to enrich syntactic structures</h4>
               <p>This work aims to present a new approach for injecting deep dependencies (subject
                  of control verbs, subject sharing in case of ellipsis, ...) into a surfacic and projective
                  treebank. We use a graph rewriting system with constraint programming techniques for
                  producing generic rules which can be easily applied to a treebank. Moreover, we are
                  testing the genericity of our rules by using output of three different parsers to
                  evaluate how the rules behave on predicted parse trees.
               </p>
               <hr>
               <h4 id="recital-2013-long-015">État de l'art de l'induction de sens: une voie vers la désambiguïsation lexicale pour
                  les langues peu dotées
               </h4>
               			Auteur : Mohammad Nasiruddin -
               			Contact : mohammad.nasiruddin@imag.fr<br><p>La désambiguïsation lexicale, le processus qui consiste à automatiquement identifier
                  le ou les sens possible d'un mot polysémique dans un contexte donné, est une tâche
                  fondamentale pour le Traitement Automatique des Langues (TAL). Le développement et
                  l'amélioration des techniques de désambiguïsation lexicale ouvrent de nombreuses perspectives
                  prometteuses pour le TAL. En effet, cela pourrait conduire à un changement paradigmatique
                  en permettant de réaliser un premier pas vers la compréhension des langues naturelles.
                  En raison du manque de ressources langagières, il est parfois difficile d'appliquer
                  des techniques de désambiguïsation à des langues peu dotées. C'est pourquoi, nous
                  nous intéressons ici, à enquêter sur comment avoir un début de recherche sur la désambiguïsation
                  lexicale pour les langues peu dotées, en particulier en exploitant des techniques
                  d'induction des sens de mots, ainsi que quelques suggestions de pistes intéressantes
                  à explorer.
               </p><em>Version anglaise :</em><h4>A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation
                  for Under-Resourced Languages
               </h4>
               <p>Word Sense Disambiguation (WSD), the process of automatically identifying the meaning
                  of a polysemous word in a sentence, is a fundamental task in Natural Language Processing
                  (NLP). Progress in this approach to WSD opens up many promising developments in the
                  field of NLP and its applications. Indeed, improvement over current performance levels
                  could allow us to take a first step towards natural language understanding. Due to
                  the lack of lexical resources it is sometimes difficult to perform WSD for under-resourced
                  languages. This paper is an investigation on how to initiate research in WSD for under-resourced
                  languages by applying Word Sense Induction (WSI) and suggests some interesting topics
                  to focus on.
               </p>
               <hr>
               <h4 id="recital-2013-long-016">Génération des corpus en dialecte tunisien pour la modélisation de langage d'un système
                  de reconnaissance
               </h4>
               			Auteur : Rahma Boujelbane -
               			Contact : Rahma.boujelbane@gmail.com<br><p>Ces derniers temps, vu la situation préoccupante du monde arabe, les dialectes arabes
                  et notamment le dialecte tunisien est devenu de plus en plus utilisé dans les interviews,
                  les journaux télévisés et les émissions de débats. Cependant, cette situation présente
                  des conséquences négatives importantes pour le Traitement Automatique du Langage Naturel
                  (TALN): depuis que les dialectes parlés ne sont pas officiellement écrits et n’ont
                  pas d’orthographe standard, il est très coûteux d'obtenir des corpus adéquats à utiliser
                  pour des outils de TALN. Par conséquent, il n’existe pas des corpus parallèles entre
                  l’Arabe Standard Moderne(ASM) et le Dialecte Tunisien (DT). Dans ce travail, nous
                  proposons une méthode pour la création d’un lexique bilingue ASM–DT et un processus
                  pour la génération automatique de corpus dialectaux. Ces ressources vont servir à
                  la construction d’un modèle de langage pour les journaux télévisés tunisiens, afin
                  de l’intégrer dans un Système de Reconnaissance Automatique de Parole (SRAP).
               </p><em>Version anglaise :</em><h4>Generation of tunisian dialect corpora for adapting language models</h4>
               <p>Lately, given the serious situation in the Arab world, the Arab dialects such as Tunisian
                  dialect became increasingly used and represented in the interviews, news and debate
                  programs. However, this situation presents negative consequences for Natural Language
                  Processing (NLP): Since dialects are not officially written and have no orthographic
                  standard, it is very costly to obtain adequate corpora to train NLP tools. Therefore,
                  it does not even exist parallel corpora between Standard Arabic (MSA) and Tunisian
                  Dialect(TD). In this work, we propose a method for the creation of a bilingual lexicon
                  MSA-TD and an automatic process for generating dialectal corpora. These resources
                  will be used to build a language model for Tunisian news, in order to integrate it
                  into an Automatic Speech Recognition (ASR).
               </p>
               <hr>
               <h4 id="recital-2013-long-017">Détection automatique des sessions de recherche par similarité des résultats provenant
                  d’une collection de documents externe
               </h4>
               			Auteur : Simon Leva -
               			Contact : sleva@univ-tlse2.fr<br>
               			Auteur : Nicolas Faessel -
               			Contact : nicolas.faessel@irit.fr<br><p>Les utilisateurs d’un système de recherche d’information mettent en oeuvre des comportements
                  de recherche complexes tels que la reformulation de requête et la recherche multitâche
                  afin de satisfaire leurs besoins d’information. Ces comportements de recherche peuvent
                  être observés à travers des journaux de requêtes, et constituent des indices permettant
                  une meilleure compréhension des besoins des utilisateurs. Dans cette perspective,
                  il est nécessaire de regrouper au sein d’une même session de recherche les requêtes
                  reliées à un même besoin d’information. Nous proposons une méthode de détection automatique
                  des sessions exploitant la collection de documents WIKIPÉDIA, basée sur la similarité
                  des résultats renvoyés par l’interrogation de cette collection afin d’évaluer la similarité
                  entre les requêtes. Cette méthode obtient de meilleures performances que les approches
                  temporelle et lexicale traditionnellement employées pour la détection de sessions
                  séquentielles, et peut être appliquée à la détection de sessions imbriquées. Ces expérimentations
                  ont été réalisées sur des données provenant du portail OpenEdition.
               </p><em>Version anglaise :</em><h4>Automatic search session detection exploiting results similarity from an external
                  document collection
               </h4>
               <p>Search engines users apply complex search behaviours such as query reformulation and
                  multitasking search to satisfy their information needs. These search behaviours may
                  be observed through query logs, and constitute clues allowing a better understanding
                  of users’ needs. In this perspective, it is decisive to group queries related to the
                  same information need into a unique search session. We propose an automatic session
                  detection method exploiting the WIKIPEDIA documents collection, based on the similarity
                  between the results returned for each query pair to estimate the similarity between
                  queries. This method shows better performance than both temporal and lexical approaches
                  traditionally used for successive session detection, and can be applied as well to
                  multitasking search session detection. These experiments were conducted on a dataset
                  originating from the OpenEdition Web portal.
               </p>
               <hr>
               <h4 id="recital-2013-long-018">Une approche mixte morpho-syntaxique et statistique pour la reconnaissance d'entités
                  nommées en langue chinoise
               </h4>
               			Auteur : Zhen Wang -
               			Contact : zhen.wang@geolsemantics.com<br><p>Cet article présente une approche mixte, morpho-syntaxique et statistique, pour la
                  reconnaissance d'entités nommées en langue chinoise dans un système d'extraction automatique
                  d'information. Le processus se divise principalement en trois étapes : la première
                  génère des noms propres potentiels à l'aide de règles morphologiques ; la deuxième
                  utilise un modèle de langue afin de sélectionner le meilleur résultat ; la troisième
                  effectue la reconnaissance d'entités nommées grâce à une analyse syntaxique locale.
                  Cette dernière permet une reconnaissance automatique d'entités nommées plus pertinente
                  et plus complète.
               </p><em>Version anglaise :</em><h4>A Mixed Morpho-Syntactic and Statistical Approach to Chinese Named Entity Recognition</h4>
               <p>This paper presents a morpho-syntactic and statistical approach for Chinese named
                  entity recognition which is a part of an automatic system for information extraction.
                  The process is divided into three steps : first, the generation of possible proper
                  nouns is based on morphological rules; second a language model is used to select the
                  best result, and last, a local syntactic parsing performs the named entity recognition.
                  Syntactic parsing makes named entity recognition more relevant and more complete.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="TALN'2007">2007 - 
               					TALN'2007 - Président(s) :  
               					Nabil Hathout - 
               	Philippe Muller - 
               	 à 
               					Toulouse
            </h2>
            
            				Il y a 72 articles référencés. 
            
            				
            					Il n'y a aucune information sur les articles soumis/acceptés.
            		<br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#taln-2007-long-009">taln-2007-long-009</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="taln-2007-long-001"></h4>
               			Auteur : Maria Georgescul -
               			Contact : maria.georgescul@eti.unige.ch<br>
               			Auteur : Alexander Clarck -
               			Contact : alexc@cs.rhul.ac.uk<br>
               			Auteur : Susan Armstrong -
               			Contact : susan.armstrong@issco.unige.ch<br><p>Dans cet article, nous traitons de la segmentation automatique des textes en épisodes
                  thématiques non superposés et ayant une structure linéaire. Notre étude porte sur
                  l’utilisation des traits lexicaux, acoustiques et syntaxiques et sur l’influence de
                  ces traits sur la performance d’un système automatique de segmentation thématique.
                  Nous appliquons notre approche, basée sur des machines à vecteurs support, à des transcriptions
                  des dialogues multilocuteurs.
               </p><em>Version anglaise :</em><h4>Exploiting structural meeting-specific features for topic segmentation</h4>
               <p>In this article we address the task of automatic text structuring into linear and
                  non-overlapping thematic episodes. Our investigation reports on the use of various
                  lexical, acoustic and syntactic features, and makes a comparison of how these features
                  influence performance of automatic topic segmentation. Using datasets containing multi-party
                  meeting transcriptions, we base our experiments on a proven state-of-the-art approach
                  using support vector classification.
               </p>
               <hr>
               <h4 id="taln-2007-long-002">Énergie textuelle de mémoires associatives</h4>
               			Auteur : Silvia Fernández -
               			Contact : silvia.fernandez@univ-avignon.fr<br>
               			Auteur : Eric Sanjuan -
               			Contact : eric.sanjuan@univ-avignon.fr<br>
               			Auteur : Juan-Manuel Torres-Moreno -
               			Contact : juan-manuel.torres@univ-avignon.fr<br><p>Dans cet article, nous présentons une approche de réseaux de neurones inspirée de
                  la physique statistique de systèmes magnétiques pour étudier des problèmes fondamentaux
                  du Traitement Automatique de la Langue Naturelle. L’algorithme modélise un document
                  comme un système de neurones où l’on déduit l’énergie textuelle. Nous avons appliqué
                  cette approche aux problèmes de résumé automatique et de détection de frontières thématiques.
                  Les résultats sont très encourageants.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we present a neural networks approach, inspired by statistical physics
                  of magnetic systems, to study fundamental problems in Natural Language Processing.
                  The algorithm models documents as neural network whose textual energy is studied.
                  We obtained good results on the application of this method to automatic summarization
                  and thematic borders detection.
               </p>
               <hr>
               <h4 id="taln-2007-long-003">Une expérience d’extraction de relations sémantiques à partir de textes dans le domaine
                  médical
               </h4>
               			Auteur : Mehdi Embarek -
               			Contact : embarekm@zoe.cea.fr<br>
               			Auteur : Olivier Ferret -
               			Contact : ferreto@zoe.cea.fr<br><p>Dans cet article, nous présentons une méthode permettant d’extraire à partir de textes
                  des relations sémantiques dans le domaine médical en utilisant des patrons linguistiques.
                  La première partie de cette méthode consiste à identifier les entités entre lesquelles
                  les relations visées interviennent, en l’occurrence les maladies, les examens, les
                  médicaments et les symptômes. La présence d’une des relations sémantiques visées dans
                  les phrases contenant un couple de ces entités est ensuite validée par l’application
                  de patrons linguistiques préalablement appris de manière automatique à partir d’un
                  corpus annoté. Nous rendons compte de l’évaluation de cette méthode sur un corpus
                  en Français pour quatre relations.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this article, we present a method to extract semantic relations automatically in
                  the medical domain using linguistic patterns. This method consists first in identifying
                  the entities that are part of the relations to extract, that is to say diseases, exams,
                  treatments, drugs and symptoms. Thereafter, sentences that contain these entities
                  are extracted and the presence of a semantic relation is validated by applying linguistic
                  patterns that were automatically learnt from an annotated corpus. We report the results
                  of an evaluation of our extraction method on a French corpus for four relations.
               </p>
               <hr>
               <h4 id="taln-2007-long-004">Identifier les pronoms anaphoriques et trouver leurs antécédents : l’intérêt de la
                  classification bayésienne
               </h4>
               			Auteur : Davy Weissenbacher -
               			Contact : dw@lipn.univ-paris13.fr<br>
               			Auteur : Adeline Nazarenko -
               			Contact : nazarenko@lipn.univ-paris13.fr<br><p>On oppose souvent en TAL les systèmes à base de connaissances linguistiques et ceux
                  qui reposent sur des indices de surface. Chaque approche a ses limites et ses avantages.
                  Nous proposons dans cet article une nouvelle approche qui repose sur les réseaux bayésiens
                  et qui permet de combiner au sein d’une même représentation ces deux types d’informations
                  hétérogènes et complémentaires. Nous justifions l’intérêt de notre approche en comparant
                  les performances du réseau bayésien à celles des systèmes de l’état de l’art, sur
                  un problème difficile du TAL, celui de la résolution d’anaphore.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In NLP, a traditional distinction opposes linguistically-based systems and knowledge-poor
                  ones, which mainly rely on surface clues. Each approach has its drawbacks and its
                  advantages. In this paper, we propose a new approach based on Bayes Networks that
                  allows to combine both types of information. As a case study, we focus on the anaphora
                  resolution which is known as a difficult NLP problem. We show that our bayesain system
                  performs better than a state-of-the art one for this task.
               </p>
               <hr>
               <h4 id="taln-2007-long-005">Régler les règles d’analyse morphologique</h4>
               			Auteur : Bruno Cartoni -
               			Contact : bruno.cartoni@eti.unige.ch<br><p>Dans cet article, nous présentons différentes contraintes mécaniques et linguistiques
                  applicables à des règles d’analyse des mots inconnus afin d’améliorer la performance
                  d’un analyseur morphologique de l’italien. Pour mesurer l’impact de ces contraintes,
                  nous présentons les résultats d’une évaluation de chaque contrainte qui prend en compte
                  les gains et les pertes qu’elle engendre. Nous discutons ainsi de la nécessaire évaluation
                  de chaque réglage apporté aux règles afin d’en déterminer la pertinence.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this article, we present various constraints, mechanical and linguistic, that can
                  be applied to analysing rules for unknown words in order to improve the performance
                  of a morphological analyser for Italian. To measure the impact of these constraints,
                  we present an evaluation for each constraint, taking into account the gains and losses
                  which they generate. We then discuss the need to evaluate any fine-tuning of these
                  kinds of rules in order to decide whether they are appropriate or not.
               </p>
               <hr>
               <h4 id="taln-2007-long-006">Structures de traits typées et morphologie à partitions</h4>
               			Auteur : François Barthélemy -
               			Contact : barthe@cnam.fr<br><p>Les structures de traits typées sont une façon abstraite et agréable de représenter
                  une information partielle. Dans cet article, nous montrons comment la combinaison
                  de deux techniques relativement classiques permet de définir une variante de morphologie
                  à deux niveaux intégrant harmonieusement des structures de traits et se compilant
                  en une machine finie. La première de ces techniques est la compilation de structure
                  de traits en expressions régulières, la seconde est la morphologie à partition. Nous
                  illustrons au moyen de deux exemples l’expressivité d’un formalisme qui rapproche
                  les grammaires à deux niveaux des grammaires d’unification.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Feature Structures are an abstract and convenient way of representing partial information.
                  In this paper, we show that the combination of two relatively classical techniques
                  makes possible the definition of a variant of two-level morphology which integrates
                  harmoniously feature structures and compiles into finite-state machines. The first
                  technique is the compilation of feature structures into regular expressions, the second
                  one is partition-based morphology. Two examples are given, which show that our formalism
                  is close to unification grammars.
               </p>
               <hr>
               <h4 id="taln-2007-long-007">Analyse morphosémantique des composés savants : transposition du français à l’anglais</h4>
               			Auteur : Louise Deléger -
               			Contact : louise.deleger@spim.jussieu.fr<br>
               			Auteur : Fiammetta Namer -
               			Contact : fiammetta.namer@univ-nancy2.fr<br>
               			Auteur : Pierre Zweigenbaum -
               			Contact : pz@limsi.fr<br><p>La plupart des vocabulaires spécialisés comprennent une part importante de lexèmes
                  morphologiquement complexes, construits à partir de racines grecques et latines, qu’on
                  appelle « composés savants ». Une analyse morphosémantique permet de décomposer et
                  de donner des définitions à ces lexèmes, et semble pouvoir être appliquée de façon
                  similaire aux composés de plusieurs langues. Cet article présente l’adaptation d’un
                  analyseur morphosémantique, initialement dédié au français (DériF), à l’analyse de
                  composés savants médicaux anglais, illustrant ainsi la similarité de structure de
                  ces composés dans des langues européennes proches. Nous exposons les principes de
                  cette transposition et ses performances. L’analyseur a été testé sur un ensemble de
                  1299 lexèmes extraits de la terminologie médicale WHO-ART : 859 ont pu être décomposés
                  et définis, dont 675 avec succès. Outre une simple transposition d’une langue à l’autre,
                  la méthode montre la potentialité d’un système multilingue.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Medical language, as many technical languages, is rich with morphologically complex
                  words, many of which take their roots in Greek and Latin – in which case they are
                  called neoclassical compounds. Morphosemantic analysis can help generate decompositions
                  and definitions of such words, and is likely to be similarly applicable to compounds
                  from different languages. This paper reports work on the adaptation of a morphosemantic
                  analyzer dedicated to French (DériF) to analyze English medical neoclassical compounds,
                  and shows the similarity in structure of compounds from related European languages.
                  It presents the principles of this transposition and its current performance. The
                  analyzer was tested on a set of 1,299 compounds extracted from theWHO-ART terminology:
                  859 could be decomposed and defined, 675 of which successfully. Aside from simple
                  transposition from one language to another, the method also emphasizes the potentiality
                  for a multilingual system.
               </p>
               <hr>
               <h4 id="taln-2007-long-008"></h4>
               			Auteur : Oana Frunza -
               			Contact : ofrunza@site.uottawa.ca<br>
               			Auteur : Diana Inkpen -
               			Contact : diana@site.uottawa.ca<br><p>Les congénères sont des mots qui ont au moins un sens en commun entre deux langues
                  en plus d‘avoir une orthographie semblable. La reconnaissance de ce type de mots permet
                  aux apprenants de langue seconde ou étrangère d‘enrichir plus rapidement leur vocabulaire
                  et d‘améliorer leur compréhension écrite. Toutefois, les faux amis sont des paires
                  de mots qui à l‘écrit ont des similarités, mais ils ont des significations différentes.
                  Pour leur part, les congénères partiels sont des mots qui ont la même signification
                  dans certains contextes dans chacune des deux langues. Cet article présente une méthode
                  pour la classification automatique des paires des mots classées en congénères ou faux
                  amis, en utilisant des mesures de similarité orthographiques et des méthodes d‘apprentissage
                  automatique. Ainsi, nous construisons des listes complètes des congénères et des faux
                  amis entre les deux langues. Nous désambiguisons les congénères partiels dans des
                  contextes spécifiques. Nos méthodes sont évaluées pour le français et l‘anglais, mais
                  elles seraient applicables à d‘autres paires des langues. Nous avons construit un
                  outil qui prend ces listes et marque dans un texte français les mots qui ont des congénères
                  ou des faux amis en anglais, dans le but d‘aider les apprenants en français langue
                  seconde ou étrangère à améliorer leur compréhension écrite et à développer une meilleure
                  rétention.
               </p><em>Version anglaise :</em><h4>A tool for detecting French-English cognates and false friends</h4>
               <p>Cognates are pairs of words in different languages similar in spelling and meaning.
                  They can help a second-language learner on the tasks of vocabulary expansion and reading
                  comprehension. False friends are pairs of words that have similar spelling but different
                  meanings. Partial cognates are pairs of words in two languages that have the same
                  meaning in some, but not all contexts. In this article we present a method to automatically
                  classify a pair of words as cognates or false friends, by using several measures of
                  orthographic similarity as features for classification. We use this method to create
                  complete lists of cognates and false friends between two languages. We also disambiguate
                  partial cognates in context. We applied all our methods to French and English, but
                  they can be applied to other pairs of languages as well. We built a tool that takes
                  the produced lists and annotates a French text with equivalent English cognates or
                  false friends, in order to help second-language learners improve their reading comprehension
                  skills and retention rate.
               </p>
               <hr>
               <h4 id="taln-2007-long-009">Enrichissement d’un lexique bilingue par analogie</h4>
               			Auteur : Philippe Langlais -
               			Contact : felipe@iro.umontreal.ca<br>
               			Auteur : Alexandre Patry -
               			Contact : patryale@iro.umontreal.ca<br><p>La présence de mots inconnus dans les applications langagières représente un défi
                  de taille bien connu auquel n’échappe pas la traduction automatique. Les systèmes
                  professionnels de traduction offrent à cet effet à leurs utilisateurs la possibilité
                  d’enrichir un lexique de base avec de nouvelles entrées. Récemment, Stroppa et Yvon
                  (2005) démontraient l’intérêt du raisonnement par analogie pour l’analyse morphologique
                  d’une langue. Dans cette étude, nous montrons que le raisonnement par analogie offre
                  également une réponse adaptée au problème de la traduction d’entrées lexicales inconnues.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Unknown words are a well-known hindrance to natural language applications. In particular,
                  they drastically impact machine translation quality. An easy way out commercial translation
                  systems usually offer their users is the possibility to add unknown words and their
                  translations into a dedicated lexicon. Recently, Stroppa et Yvon (2005) shown how
                  analogical learning alone deals nicely with morphology in different languages. In
                  this study we show that analogical learning offers as well an elegant and efficient
                  solution to the problem of identifying potential translations of unknown words.
               </p>
               <hr>
               <h4 id="taln-2007-long-010">Inférence de règles de réécriture pour la traduction de termes biomédicaux</h4>
               			Auteur : Vincent Claveau -
               			Contact : Vincent.Claveau@irisa.fr<br><p>Dans le domaine biomédical, le caractère multilingue de l’accès à l’information est
                  un problème d’importance. Dans cet article nous présentons une technique originale
                  permettant de traduire des termes simples du domaine biomédical de et vers de nombreuses
                  langues. Cette technique entièrement automatique repose sur l’apprentissage de règles
                  de réécriture à partir d’exemples et l’utilisation de modèles de langues. Les évaluations
                  présentées sont menées sur différentes paires de langues (français-anglais, espagnol-portugais,
                  tchèque-anglais, russe-anglais...). Elles montrent que cette approche est très efficace
                  et offre des performances variables selon les langues mais très bonnes dans l’ensemble
                  et nettement supérieures à celles disponibles dans l’état de l’art. Les taux de précision
                  de traductions s’étagent ainsi de 57.5% pour la paire russe-anglais jusqu’à 85% pour
                  la paire espagnol-portugais et la paire françaisanglais.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In the biomedical domain, offering a multilingual access to specialized information
                  is a major issue. In this paper, we present an original approach to translate simple
                  biomedical terms between several languages. This fully automatic approach is based
                  on a machine learning technique inferring rewriting rules and on language models.
                  The experiments that are presented are done onn several language pairs (French-English,
                  Spanish-Portuguese, Czech-English, Russian-English...). They demonstrate the efficiency
                  of our approach by yielding translation performances that vary according to the languages
                  but are always very good and better than those of state-of-art techniques. Indeed,
                  the translation precision rates go from 57.5% for translation from Russian to English
                  up to 85% for Spanish-Portuguese and French-English language pairs.
               </p>
               <hr>
               <h4 id="taln-2007-long-011">TiLT correcteur de SMS : évaluation et bilan qualitatif</h4>
               			Auteur : Émilie Guimier De Neef -
               			Contact : emilie.guimierdeneef@orange-ftgroup.com<br>
               			Auteur : Arnaud Debeurme -
               			Contact : arnaud.debeurme@orange-ftgroup.com<br>
               			Auteur : Jungyeul Park -
               			Contact : jungyeul.park@orange-ftgroup.com<br><p>Nous présentons le logiciel TiLT pour la correction des SMS et évaluons ses performances
                  sur le corpus de SMS du DELIC. L'évaluation utilise la distance de Jaccard et la mesure
                  BLEU. La présentation des résultats est suivie d'une analyse qualitative du système
                  et de ses limites.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents TiLT system which allows us to correct spelling errors in SMS
                  messages to standard French. We perform Jaccard and Bleu metrics for its evaluation
                  using the DELIC SMS corpus as a reference. We discuss qualitative analyses of system
                  and its limits.
               </p>
               <hr>
               <h4 id="taln-2007-long-012">Vers un méta-EDL complet, puis un EDL universel pour la TAO</h4>
               			Auteur : Hong-Thai Nguyen -
               			Contact : Hong-Thai.Nguyen@imag.fr<br>
               			Auteur : Christian Boitet -
               			Contact : Christian.Boitet@imag.fr<br><p>Un “méta-EDL” (méta-Environnement de Développement Linguiciel) pour la TAO permet
                  de piloter à distance un ou plusieurs EDL pour construire des systèmes de TAO hétérogènes.
                  Partant de CASH, un méta-EDL dédié à Ariane-G5, et de WICALE 1.0, un premier méta-EDL
                  générique mais aux fonctionnalités minimales, nous dégageons les problèmes liés à
                  l’ajout de fonctionnalités riches comme l’édition et la navigation en local, et donnons
                  une solution implémentée dans WICALE 2.0. Nous y intégrons maintenant une base lexicale
                  pour les systèmes à « pivot lexical », comme UNL/U++. Un but à plus long terme est
                  de passer d’un tel méta-EDL générique multifonctionnel à un EDL « universel », ce
                  qui suppose la réingénierie des compilateurs et des moteurs des langages spécialisés
                  pour la programmation linguistique (LSPL) supportés par les divers EDL.
               </p><em>Version anglaise :</em><h4></h4>
               <p>A “meta-EDL” (meta-Environment for Developing Lingware) for MT allows to pilot one
                  or more distant EDL in order to build heterogeneous MT systems. Starting from CASH,
                  a meta-EDL dedicated to Ariane-G5, and from WICALE 1.0, a first meta-EDL, generic
                  but offering minimal functionalities, we study the problems arising when adding rich
                  functionalities such as local editing and navigation, and give a solution implemented
                  in WICALE 2.0. We are now integrating to it a lexical database for MT systems relying
                  on a “lexical pivot”, such as UNL/U++. A longer-term goal is to evolve from such a
                  multifunctional generic meta-EDL to a “universal” EDL, which would imply the reengineering
                  of the compilers and engines of the specialized languages (SLLPs) supported by the
                  various EDLs.
               </p>
               <hr>
               <h4 id="taln-2007-long-013">Aides à la navigation dans un corpus de transcriptions d’oral</h4>
               			Auteur : Frederik Cailliau -
               			Contact : cailliau@sinequa.com<br>
               			Auteur : Claude De Loupy -
               			Contact : loupy@syllabs.com<br><p>Dans cet article, nous évaluons les performances de fonctionnalités d’aide à la navigation
                  dans un contexte de recherche dans un corpus audio. Nous montrons que les particularités
                  de la transcription et, en particulier les erreurs, conduisent à une dégradation parfois
                  importante des performances des outils d’analyse. Si la navigation par concepts reste
                  dans des niveaux d’erreur acceptables, la reconnaissance des entités nommées, utilisée
                  pour l’aide à la lecture, voit ses performances fortement baisser. Notre remise en
                  doute de la portabilité de ces fonctions à un corpus oral est néanmoins atténuée par
                  la nature même du corpus qui incite à considérer que toute méthodes permettant de
                  réduire le temps d’accès à l’information est pertinente, même si les outils utilisés
                  sont imparfaits.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we evaluate the performances of navigation facilities within the context
                  of information retrieval performed on an audio corpus. We show that the issues about
                  transcription, especially the errors, lead to a sometimes important deterioration
                  of the performances of the analysing tools. While the navigation by concepts remains
                  within an acceptable error rate, the recognition of named entities used in fast reading
                  undergo a performance drop. Our caution to the portability of these functions to a
                  speech corpus is attenuated by the nature of the corpus: access time to a speech corpus
                  can be very long, and therefore all methods that reduce access time are good to take.
               </p>
               <hr>
               <h4 id="taln-2007-long-014">Une grammaire du français pour une théorie descriptive et formelle de la langue</h4>
               			Auteur : Marie-Laure Guénot -
               			Contact : marie-laure.guenot@u-bordeaux3.fr<br><p>Dans cet article, nous présentons une grammaire du français qui fait l’objet d’un
                  modèle basé sur des descriptions linguistiques de corpus (provenant notamment des
                  travaux de l’Approche Pronominale) et représentée selon le formalisme des Grammaires
                  de Propriétés. Elle constitue une proposition nouvelle parmi les grammaires formelles
                  du français, participant à la mise en convergence de la variété des travaux de description
                  linguistique, et de la diversité des possibilités de représentation formelle. Cette
                  grammaire est mise à disposition publique sur le Centre de Ressources pour la Description
                  de l’Oral en tant que ressource pour la représentation et l’analyse.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper I present a grammar for French, which is the implementation of a linguistic
                  model based on corpus descriptions (notably coming from Approche Pronominale) and
                  represented into the Property Grammars formalism. It accounts for a new proposition
                  among formal grammars, taking part into the works that aim to promote convergence
                  between the various researchs of descriptive linguistics and the diversity of formal
                  representation possibilities. It is freely available on the Spoken Data Resource Center
                  (CRDO), as a representation and analysis resource.
               </p>
               <hr>
               <h4 id="taln-2007-long-015">Architecture compositionnelle pour les dépendances croisées</h4>
               			Auteur : Alexandre Dikovsky -
               			Contact : Alexandre.Dikovsky@univ-nantes.fr<br><p>L’article présente les principes généraux sous-jacent aux grammaires catégorielles
                  de dépendances : une classe de grammaires de types récemment proposée pour une description
                  compositionnelle et uniforme des dépendances continues et discontinues. Ces grammaires
                  très expressives et analysées en temps polynomial, adoptent naturellement l’architecture
                  multimodale et expriment les dépendances croisées illimitées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article presents the general principles underlying the categorial dependency
                  grammars : a class of type logical grammars recently introduced as a compositional
                  and uniform definition of continuous and discontinuous dependences. These grammars
                  are very expressive, are parsed in a reasonable polynomial time, naturally adopt the
                  multimodal architecture and explain unlimited cross-serial dependencies.
               </p>
               <hr>
               <h4 id="taln-2007-long-016">SemTAG, une architecture pour le développement et l’utilisation de grammaires d’arbres
                  adjoints à portée sémantique
               </h4>
               			Auteur : Claire Gardent -
               			Contact : gardent@loria.fr<br>
               			Auteur : Yannick Parmentier -
               			Contact : parmenti@loria.fr<br><p>Dans cet article, nous présentons une architecture logicielle libre et ouverte pour
                  le développement de grammaires d’arbres adjoints à portée sémantique. Cette architecture
                  utilise un compilateur de métagrammaires afin de faciliter l’extension et la maintenance
                  de la grammaire, et intègre un module de construction sémantique permettant de vérifier
                  la couverture aussi bien syntaxique que sémantique de la grammaire. Ce module utilise
                  un analyseur syntaxique tabulaire généré automatiquement à partir de la grammaire
                  par le système DyALog. Nous présentons également les résultats de l’évaluation d’une
                  grammaire du français développée au moyen de cette architecture.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we introduce a free and open software architecture for the development
                  of Tree Adjoining Grammars equipped with semantic information. This architecture uses
                  a metagrammar compiler to facilitate the grammar extension and maintnance, and includes
                  a semantic construction module allowing to check both the syntactic and semantic coverage
                  of the grammar. This module uses a tabular syntactic parser generated automatically
                  from this grammar using the DyALog system. We also give the results of the evaluation
                  of a real-size TAG for French developed using this architecture.
               </p>
               <hr>
               <h4 id="taln-2007-long-017">Utiliser des classes de sélection distributionnelle pour désambiguïser les adjectifs</h4>
               			Auteur : Fabienne Venant -
               			Contact : fabienne.venant@ens.fr<br><p>La désambiguïsation lexicale présente un intérêt considérable pour un nombre important
                  d’applications, en traitement automatique des langues comme en recherche d'information.
                  Nous proposons un modèle d’un genre nouveau, fondé sur la théorie de la construction
                  dynamique du sens (Victorri et Fuchs, 1996). Ce modèle donne une place centrale à
                  la polysémie et propose une représentation géométrique du sens. Nous présentons ici
                  une application de ce modèle à la désambiguïsation automatique des adjectifs. La méthode
                  utilisée s'appuie sur une pré-désambiguïsation du nom régissant l'adjectif, par le
                  biais de classes de sélection distributionnelle. Elle permet aussi de prendre en compte
                  les positions relatives du nom et de l'adjectif (postpostion ou antéposition) dans
                  le calcul du sens.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Automatic word sense disambiguation represents an important issue for many applications,
                  in Natural Language Processing as in Information Retrieval. We propose a new kind
                  of model, within the framework of Dynamical Construction of Meaning (Victorri and
                  Fuchs, 1996). This model gives a central place to polysemy and proposes a geometric
                  representation of meaning. We present here an application of this model to adjective
                  sense disambiguation. The method we used relies on a pre-disambiguation of the noun
                  used with the adjective under study, using distributionnal classes. It can also take
                  into account the changes in the meaning of the adjective, whether it is placed before
                  or after the noun.
               </p>
               <hr>
               <h4 id="taln-2007-long-018"></h4>
               			Auteur : Véronique Malaisé -
               			Contact : vmalaise@few.vu.nl<br>
               			Auteur : Luit Gazendam -
               			Contact : Luit.Gazendam@telin.nl<br>
               			Auteur : Hennie Brugman -
               			Contact : Hennie.Brugman@mpi.nl<br><p>La relation voir/employé pour d’un thesaurus est souvent plus complexe que la (para-)synonymie
                  recommandée par l’ISO-2788, standard décrivant le contenu de ces vocabulaires contrôlés.
                  Le fait qu’un non descripteur puisse renvoyer à plusieurs descripteurs (seuls les
                  descripteurs sont pertinents dans le cadre de l’indexation contrôlée) fait que cette
                  relation est complexe à utiliser dans un contexte d’annotation automatique : elle
                  génère des cas d’ambiguité. Dans ce papier, nous présentons CARROT, un algorithme
                  que nous avons mis au point pour classer les résultats de notre chaîne de traitements
                  pour l’Extraction d’Information, et son utilisation dans le cadre de la sélection
                  du descripteur pertinent lorsque plusieurs choix sont possibles. Cette sélection s’adresse
                  à des documentalistes, dans le but de simplifier et d’accélérer leur travail, et se
                  base sur la structure de leur thesaurus. Nous arrivons à un succès de 95 % dans nos
                  suggestions ; nous discutons ces résultats et présentons des perspectives à cette
                  expérimentation.
               </p><em>Version anglaise :</em><h4>Disambiguating automatic semantic annotation based on a thesaurus structure</h4>
               <p>The use/use for relationship a thesaurus is usually more complex than the (para-)
                  synonymy recommended in the ISO-2788 standard describing the content of these controlled
                  vocabularies. The fact that a non preferred term can refer to multiple preferred terms
                  (only the latter are relevant in controlled indexing) makes this relationship difficult
                  to use in automatic annotation applications : it generates ambiguity cases. In this
                  paper, we present the CARROT algorithm, meant to rank the output of our Information
                  Extraction pipeline, and how this algorithm can be used to select the relevant preferred
                  term out of different possibilities. This selection is meant to provide suggestions
                  of keywords to human annotators, in order to ease and speed up their daily process
                  and is based on the structure of their thesaurus. We achieve a 95 % success, and discuss
                  these results along with perspectives for this experiment.
               </p>
               <hr>
               <h4 id="taln-2007-long-019">Repérage de sens et désambiguïsation dans un contexte bilingue</h4>
               			Auteur : Marianna Apidianaki -
               			Contact : Marianna.Apidianaki@linguist.jussieu.fr<br><p>Les besoins de désambiguïsation varient dans les différentes applications du Traitement
                  Automatique des Langues (TAL). Dans cet article, nous proposons une méthode de désambiguïsation
                  lexicale opératoire dans un contexte bilingue et, par conséquent, adéquate pour la
                  désambiguïsation au sein d’applications relatives à la traduction. Il s’agit d’une
                  méthode contextuelle, qui combine des informations de cooccurrence avec des informations
                  traductionnelles venant d’un bitexte. L’objectif est l’établissement de correspondances
                  de traduction au niveau sémantique entre les mots de deux langues. Cette méthode étend
                  les conséquences de l’hypothèse contextuelle du sens dans un contexte bilingue, tout
                  en admettant l’existence d’une relation de similarité sémantique entre les mots de
                  deux langues en relation de traduction. La modélisation de ces correspondances de
                  granularité fine permet la désambiguïsation lexicale de nouvelles occurrences des
                  mots polysémiques de la langue source ainsi que la prédiction de la traduction la
                  plus adéquate pour ces occurrences.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Word Sense Disambiguation (WSD) needs vary greatly in different Natural Language Processing
                  (NLP) applications. In this article, we propose a WSD method which operates in a bilingual
                  context and is, thus, adequate for disambiguation in applications relative to translation.
                  It is a contextual method which combines cooccurrence information with translation
                  information found in a bitext. The goal is the establishment of translation correspondences
                  at the sense level between the lexical items of two languages. This method extends
                  the consequences of the contextual hypothesis in a bilingual framework assuming, at
                  the same time, the existence of a semantic similarity relation between words of two
                  languages being in a translation relation. The modelling of fine-grained correspondences
                  allows for the disambiguation of new occurrences of the polysemous source language
                  lexical items as well as for the prediction of the most adequate translation for those
                  occurrences.
               </p>
               <hr>
               <h4 id="taln-2007-long-020">PrepLex : un lexique des prépositions du français pour l’analyse syntaxique</h4>
               			Auteur : Karën Fort -
               			Contact : Karen.Fort@loria.fr<br>
               			Auteur : Bruno Guillaume -
               			Contact : Bruno.Guillaume@loria.fr<br><p>PrepLex est un lexique des prépositions du français. Il contient les informations
                  utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant
                  différentes sources d’informations lexicales disponibles. Ce lexique met également
                  en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition
                  des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence
                  des verbes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>PrepLex is a lexicon of French prepositions which provides all the information needed
                  for parsing. It was built by comparing and merging several authoritative lexical sources.
                  This lexicon also shows the prepositions or classes of prepositions that appear in
                  verbs subcategorization frames.
               </p>
               <hr>
               <h4 id="taln-2007-long-021">Comparaison du Lexique-Grammaire des verbes pleins et de DICOVALENCE : vers une intégration
                  dans le Lefff
               </h4>
               			Auteur : Laurence Danlos -
               			Contact : laurence.danlos@linguist.jussieu.fr<br>
               			Auteur : Benoît Sagot -
               			Contact : benoit.sagot@inria.fr<br><p>Cet article compare le Lexique-Grammaire des verbes pleins et DICOVALENCE, deux ressources
                  lexicales syntaxiques pour le français développées par des linguistes depuis de nombreuses
                  années. Nous étudions en particulier les divergences et les empiètements des modèles
                  lexicaux sous-jacents. Puis nous présentons le Lefff , lexique syntaxique à grande
                  échelle pour le TAL, et son propre modèle lexical. Nous montrons que ce modèle est
                  à même d’intégrer les informations lexicales présentes dans le Lexique-Grammaire et
                  dans DICOVALENCE. Nous présentons les résultats des premiers travaux effectués en
                  ce sens, avec pour objectif à terme la constitution d’un lexique syntaxique de référence
                  pour le TAL.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper compares the Lexicon-Grammar of full verbs and DICOVALENCE, two syntactic
                  lexical resources for French developed by linguists for numerous years. We focus on
                  differences and overlaps between both underlying lexical models. Then we introduce
                  the Lefff , large-coverage syntactic lexicon for NLP, and its own lexical model. We
                  show that this model is able to integrate lexical information present in the Lexicon-Grammar
                  and in DICOVALENCE. We describe the results of the first work done in this direction,
                  the long term goal being the consitution of a high-quality syntactic lexicon for NLP.
               </p>
               <hr>
               <h4 id="taln-2007-long-022">Dictionnaires électroniques et étiquetage syntactico-sémantique</h4>
               			Auteur : Pierre-André Buvet -
               			Contact : Pierre-Andre.Buvet@lli.univ-paris13.fr<br>
               			Auteur : Emmanuel Cartier -
               			Contact : Emmanuel.Cartier@lli.univ-paris13.fr<br>
               			Auteur : Fabrice Issac -
               			Contact : Fabrice.Issac@lli.univ-paris13.fr<br>
               			Auteur : Salah Mejri -
               			Contact : Salah.Mejri@lli.univ-paris13.fr<br><p>Nous présentons dans cet article le prototype d’un système d’étiquetage syntactico-sémantique
                  des mots qui utilise comme principales ressources linguistiques différents dictionnaires
                  du laboratoire Lexiques, Dictionnaires, Informatique (LDI). Dans un premier temps,
                  nous mentionnons des travaux sur le même sujet. Dans un deuxième temps, nous faisons
                  la présentation générale du système. Dans un troisième temps, nous exposons les principales
                  caractéristiques des dictionnaires syntactico-sémantiques utilisés. Dans un quatrième
                  temps, nous détaillons un exemple de traitement.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present in this paper a syntactico-semantics tagger prototype which uses as first
                  linguistic resources various dictionaries elaborated at LDI. First, we mention several
                  related works. Second, we present the overall sketch of the system. Third, we expose
                  the main characteristics of the syntactico-semantic dictionaries implied in the processes.
                  Last, using an example, we explicit the main stages of the analysis.
               </p>
               <hr>
               <h4 id="taln-2007-long-023">Un analyseur hybride pour la détection et la correction des erreurs cachées sémantiques
                  en langue arabe
               </h4>
               			Auteur : Chiraz Ben Othmane Zribi -
               			Contact : Chiraz.benothmane@riadi.rnu.tn<br>
               			Auteur : Hanène Mejri -
               			Contact : Hanene.mejri@riadi.rnu.tn<br>
               			Auteur : Mohamed Ben Ahmed -
               			Contact : Mohamed.benahmed@riadi.rnu.tn<br><p>Cet article s’intéresse au problème de la détection et de la correction des erreurs
                  cachées sémantiques dans les textes arabes. Ce sont des erreurs orthographiques produisant
                  des mots lexicalement valides mais invalides sémantiquement. Nous commençons par décrire
                  le type d’erreur sémantique auquel nous nous intéressons. Nous exposons par la suite
                  l’approche adoptée qui se base sur la combinaison de plusieurs méthodes, tout en décrivant
                  chacune de ces méthodes. Puis, nous évoquons le contexte du travail qui nous a mené
                  au choix de l’architecture multi-agent pour l’implémentation de notre système. Nous
                  présentons et commentons vers la fin les résultats de l’évaluation dudit système.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we address the problem of detecting and correcting hidden semantic
                  spelling errors in Arabic texts. Hidden semantic spelling errors are morphologically
                  valid words causing invalid semantic irregularities. After the description of this
                  type of errors, we propose and argue the combined method that we adopted in this work
                  to realize a hybrid spell checker for detecting and correcting hidden spelling errors.
                  Afterward, we present the context of this work and show the multi-agent architecture
                  of our system. Finally, we expose and comment the obtained results.
               </p>
               <hr>
               <h4 id="taln-2007-long-024">Résolution de la référence dans des dialogues homme-machine : évaluation sur corpus
                  de deux approches symbolique et probabiliste
               </h4>
               			Auteur : Alexandre Denis -
               			Contact : alexandre.denis@loria.fr<br>
               			Auteur : Frédéric Béchet -
               			Contact : frederic.bechet@univ-avignon.fr<br>
               			Auteur : Matthieu Quignard -
               			Contact : matthieu.quignard@loria.fr<br><p>Cet article décrit deux approches, l’une numérique, l’autre symbolique, traitant le
                  problème de la résolution de la référence dans un cadre de dialogue homme-machine.
                  L’analyse des résultats obtenus sur le corpus MEDIA montre la complémentarité des
                  deux systèmes développés : robustesse aux erreurs et hypothèses multiples pour l’approche
                  numérique ; modélisation de phénomènes complexes et interprétation complète pour l’approche
                  symbolique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents two approaches, one symbolic, the other one probabilistic, for
                  processing reference resolution in the framework of human-machine spoken dialogues.
                  The results obtained by both systems on the French MEDIA corpus points out the complementarity
                  of the two approaches : robustness and multiple hypotheses generation for the probabilistic
                  one ; global interpretation and modeling of complex phenomenon for the symbolic one.
               </p>
               <hr>
               <h4 id="taln-2007-long-025">Annotation précise du français en sémantique de rôles par projection cross-linguistique</h4>
               			Auteur : Sebastian Padó -
               			Contact : pado@coli.uni-sb.de<br>
               			Auteur : Guillaume Pitel -
               			Contact : Guillaume.Pitel@loria.fr<br><p>Dans le paradigme FrameNet, cet article aborde le problème de l’annotation précise
                  et automatique de rôles sémantiques dans une langue sans lexique FrameNet existant.
                  Nous évaluons la méthode proposée par Padó et Lapata (2005, 2006), fondée sur la projection
                  de rôles et appliquée initialement à la paire anglais-allemand. Nous testons sa généralisabilité
                  du point de vue (a) des langues, en l'appliquant à la paire (anglais-français) et
                  (b) de la qualité de la source, en utilisant une annotation automatique du côté anglais.
                  Les expériences montrent des résultats à la hauteur de ceux obtenus pour l'allemand,
                  nous permettant de conclure que cette approche présente un grand potentiel pour réduire
                  la quantité de travail nécessaire à la création de telles ressources dans de nombreuses
                  langues.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper considers the task of the automatic induction of role-semantic annotations
                  for new languages with high precision. To this end we test the generalisability of
                  the language-independent, projection-based annotation framework introduced by Padó
                  and Lapata (2005, 2006) by (a) applying it to a new, more distant, language pair (English-French),
                  and (b), using automatic, and thus noisy, input annotation. We show that even under
                  these conditions, high-quality role annotations for French can be obtained that rival
                  existing results for German. We conclude that the framework has considerable potential
                  in reducing the manual effort involved in creating role-semantic resources for a wider
                  range of languages.
               </p>
               <hr>
               <h4 id="taln-2007-long-026">Élaboration automatique d’un dictionnaire de cooccurrences grand public</h4>
               			Auteur : Simon Charest -
               			Contact : developpement@druide.com<br>
               			Auteur : Éric Brunelle -
               			Contact : developpement@druide.com<br>
               			Auteur : Jean Fontaine -
               			Contact : developpement@druide.com<br>
               			Auteur : Bertrand Pelletier -
               			Contact : developpement@druide.com<br><p>Antidote RX, un logiciel d’aide à la rédaction grand public, comporte un nouveau dictionnaire
                  de 800 000 cooccurrences, élaboré essentiellement automatiquement. Nous l’avons créé
                  par l’analyse syntaxique détaillée d’un vaste corpus et par la sélection automatique
                  des cooccurrences les plus pertinentes à l’aide d’un test statistique, le rapport
                  de vraisemblance. Chaque cooccurrence est illustrée par des exemples de phrases également
                  tirés du corpus automatiquement. Les cooccurrences et les exemples extraits ont été
                  révisés par des linguistes. Nous examinons les choix d’interface que nous avons faits
                  pour présenter ces données complexes à un public non spécialisé. Enfin, nous montrons
                  comment nous avons intégré les cooccurrences au correcteur d’Antidote pour améliorer
                  ses performances.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Antidote is a complete set of software reference tools for writing French that includes
                  an advanced grammar checker. Antidote RX boasts a new dictionary of 800,000 cooccurrences
                  created mostly automatically. The approach we chose is based on the syntactic parsing
                  of a large corpus and the automatic selection of the most relevant co-occurrences
                  using a statistical test, the log-likelihood ratio. Example sentences illustrating
                  each cooccurrence in context are also automatically selected. The extracted co-occurrences
                  and examples were revised by linguists. We examine the various choices that were made
                  to present this complex data to a non-specialized public. We then show how we use
                  the cooccurrence data to improve the performance of Antidote’s grammar checker.
               </p>
               <hr>
               <h4 id="taln-2007-long-027">Les vecteurs conceptuels, un outil complémentaire aux réseaux lexicaux</h4>
               			Auteur : Didier Schwab -
               			Contact : didier@cs.usm.my<br>
               			Auteur : Lim Lian Tze -
               			Contact : liantze@cs.usm.my<br>
               			Auteur : Mathieu Lafourcade -
               			Contact : lafourcade@lirmm.fr<br><p>Fréquemment utilisés dans le Traitement Automatique des Langues Naturelles, les réseaux
                  lexicaux font aujourd’hui l’objet de nombreuses recherches. La plupart d’entre eux,
                  et en particulier le plus célèbre WordNet, souffrent du manque d’informations syntagmatiques
                  mais aussi d’informations thématiques (« problème du tennis »). Cet article présente
                  les vecteurs conceptuels qui permettent de représenter les idées contenues dans un
                  segment textuel quelconque et permettent d’obtenir une vision continue des thématiques
                  utilisées grâce aux distances calculables entre eux. Nous montrons leurs caractéristiques
                  et en quoi ils sont complémentaires des réseaux lexico-sémantiques. Nous illustrons
                  ce propos par l’enrichissement des données de WordNet par des vecteurs conceptuels
                  construits par émergence.
               </p><em>Version anglaise :</em><h4></h4>
               <p>There is currently much research in natural language processing focusing on lexical
                  networks. Most of them, in particular the most famous, WordNet, lack syntagmatic information
                  and but also thematic information (« Tennis Problem »). This article describes conceptual
                  vectors that allows the representation of ideas in any textual segment and offers
                  a continuous vision of related thematics, based on the distances between these thematics.
                  We show the characteristics of conceptual vectors and explain how they complement
                  lexico-semantic networks. We illustrate this purpose by adding conceptual vectors
                  to WordNet by emergence.
               </p>
               <hr>
               <h4 id="taln-2007-long-028">Alignements monolingues avec déplacements</h4>
               			Auteur : Julien Bourdaillet -
               			Contact : julien.bourdaillet@lip6.fr<br>
               			Auteur : Jean-Gabriel Ganascia -
               			Contact : jean-gabriel.ganascia@lip6.fr<br><p>Ce travail présente une application d’alignement monolingue qui répond à une problématique
                  posée par la critique génétique textuelle, une école d’études littéraires qui s’intéresse
                  à la genèse textuelle en comparant les différentes versions d’une oeuvre. Ceci nécessite
                  l’identification des déplacements, cependant, le problème devient ainsi NP-complet.
                  Notre algorithme heuristique est basé sur la reconnaissance des homologies entre séquences
                  de caractères. Nous présentons une validation expérimentale et montrons que notre
                  logiciel obtient de bons résultats ; il permet notamment l’alignement de livres entiers.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a monolingual alignment application that addresses a problem which
                  occurs in textual genetic criticism, a humanities discipline of literary studies which
                  compares texts’ versions to understand texts’ genesis. It requires the move detection,
                  but this characteristic makes the problem NP-complete. Our heuristic algorithm is
                  based on pattern matching in character sequences. We present an experimental validation
                  where we show that our application obtains good results ; in particular it enables
                  whole book alignment.
               </p>
               <hr>
               <h4 id="taln-2007-long-029">Confondre le coupable : corrections d’un lexique suggérées par une grammaire</h4>
               			Auteur : Lionel Nicolas -
               			Contact : lnicolas@i3s.unice.fr<br>
               			Auteur : Jacques Farré -
               			Contact : jf@i3s.unice.fr<br>
               			Auteur : Éric Villemonte De La Clergerie -
               			Contact : Eric.De_La_Clergerie@inria.fr<br><p>Le succès de l’analyse syntaxique d’une phrase dépend de la qualité de la grammaire
                  sous-jacente mais aussi de celle du lexique utilisé. Une première étape dans l’amélioration
                  des lexiques consiste à identifier les entrées lexicales potentiellement erronées,
                  par exemple en utilisant des techniques de fouilles d’erreurs sur corpus (Sagot &amp;
                  Villemonte de La Clergerie, 2006). Nous explorons ici l’étape suivante : la suggestion
                  de corrections pour les entrées identifiées. Cet objectif est atteint au travers de
                  réanalyses des phrases rejetées à l’étape précédente, après modification des informations
                  portées par les entrées suspectées. Un calcul statistique sur les nouveaux résultats
                  permet ensuite de mettre en valeur les corrections les plus pertinentes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Successful parsing depends on the quality of the underlying grammar but also on the
                  quality of the lexicon. A first step towards the improvement of lexica consists in
                  identifying potentially erroneous lexical entries, for instance by using error mining
                  techniques on corpora (Sagot &amp; Villemonte de La Clergerie, 2006). we explores the
                  next step, namely the suggestion of corrections for those entries. This is achieved
                  by parsing the sentences rejected at the previous step anew, after modifying the information
                  carried by the suspected entries. Afterwards, a statistical computation on the parsing
                  results exhibits the most relevant corrections.
               </p>
               <hr>
               <h4 id="taln-2007-long-030">Ambiguïté de portée et approche fonctionnelle des grammaires d’arbres adjoints</h4>
               			Auteur : Sylvain Pogodalla -
               			Contact : sylvain.pogodalla@loria.fr<br><p>En s’appuyant sur la notion d’arbre de dérivation des Grammaires d’Arbres Adjoints
                  (TAG), cet article propose deux objectifs : d’une part rendre l’interface entre syntaxe
                  et sémantique indépendante du langage de représentation sémantique utilisé, et d’autre
                  part offrir un noyau qui permette le traitement sémantique des ambiguïtés de portée
                  de quantificateurs sans utiliser de langage de représentation sous-spécifiée.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Relying on the derivation tree of the Tree Adjoining Grammars (TAG), this paper has
                  to goals : on the one hand, to make the syntax/semantics interface independant from
                  the semantic representation language, and on the other hand to propose an architecture
                  that enables the modeling of scope ambguities without using underspecified representation
                  formalisms.
               </p>
               <hr>
               <h4 id="taln-2007-long-031">Évaluer SYNLEX</h4>
               			Auteur : Ingrid Falk -
               			Contact : Ingrid.Falk-@loria.fr<br>
               			Auteur : Gil Francopoulo -
               			Contact : Gil.Francopoulo@wanadoo.fr<br>
               			Auteur : Claire Gardent -
               			Contact : Claire.Gardent@loria.fr<br><p>SYNLEX est un lexique syntaxique extrait semi-automatiquement des tables du LADL.
                  Comme les autres lexiques syntaxiques du français disponibles et utilisables pour
                  le TAL (LEFFF, DICOVALENCE), il est incomplet et n’a pas fait l’objet d’une évaluation
                  permettant de déterminer son rappel et sa précision par rapport à un lexique de référence.
                  Nous présentons une approche qui permet de combler au moins partiellement ces lacunes.
                  L’approche s’appuie sur les méthodes mises au point en acquisition automatique de
                  lexique. Un lexique syntaxique distinct de SYNLEX est acquis à partir d’un corpus
                  de 82 millions de mots puis utilisé pour valider et compléter SYNLEX. Le rappel et
                  la précision de cette version améliorée de SYNLEX sont ensuite calculés par rapport
                  à un lexique de référence extrait de DICOVALENCE.
               </p><em>Version anglaise :</em><h4></h4>
               <p>SYNLEX is a syntactic lexicon extracted semi-automatically from the LADL tables. Like
                  the other syntactic lexicons for French which are both available and usable for NLP
                  (LEFFF, DICOVALENCE), it is incomplete and its recall and precision wrt a gold standard
                  are unknown. We present an approach which goes some way towards adressing these shortcomings.
                  The approach draws on methods used for the automatic acquisition of syntactic lexicons.
                  First, a new syntactic lexicon is acquired from an 82 million words corpus. This lexicon
                  is then used to validate and extend SYNLEX. Finally, the recall and precision of the
                  extended version of SYNLEX is computed based on a gold standard extracted from DICOVALENCE.
               </p>
               <hr>
               <h4 id="taln-2007-long-032">Analyse automatique vs analyse interactive : un cercle vertueux pour la voyellation,
                  l’étiquetage et la lemmatisation de l’arabe
               </h4>
               			Auteur : Fathi Debili -
               			Contact : fathi.debili@wanadoo.fr<br>
               			Auteur : Zied Ben Tahar -
               			Contact : bentaharzied@gmail.com<br>
               			Auteur : Emna Souissi -
               			Contact : emna.souissi@planet.tn<br><p>Comment produire de façon massive des textes annotés dans des conditions d’efficacité,
                  de reproductibilité et de coût optimales ? Plutôt que de corriger les sorties d’analyse
                  automatique moyennant des outils d’éditions éventuellement dédiés, ainsi qu’il estcommunément
                  préconisé, nous proposons de recourir à des outils d’analyse interactive où la correction
                  manuelle est au fur et à mesure prise en compte par l’analyse automatique. Posant
                  le problème de l’évaluation de ces outils interactifs et du rendement de leur ergonomie
                  linguistique, et proposant pour cela une métrique fondée sur le calcul du coût qu’exigent
                  ces corrections exprimé en nombre de manipulations (frappe au clavier, clic de souris,
                  etc.), nous montrons, au travers d’un protocole expérimental simple orienté vers la
                  voyellation, l’étiquetage et la lemmatisation de l’arabe, que paradoxalement, les
                  meilleures performances interactives d’un système ne sont pas toujours corrélées à
                  ses meilleures performances automatiques. Autrement dit, que le comportement linguistique
                  automatique le plus performant n’est pas toujours celui qui assure, dès lors qu’il
                  y a contributions manuelles, le meilleur rendement interactif.
               </p><em>Version anglaise :</em><h4></h4>
               <p>How can we massively produce annotated texts, with optimal efficiency, reproducibility
                  and cost? Rather than correcting the output of automatic analysis by means of possibly
                  dedicated tools, as is currently suggested, we find it more advisable to use interactive
                  tools for analysis, where manual editing is fed in real time into automatic analysis.
                  We address the issue of evaluating these tools, along with their performance in terms
                  of linguistic ergonomy, and propose a metric for calculating the cost of editing as
                  a number of keystrokes and mouse clicks. We show, by way of a simple protocol addressing
                  Arabic vowellation, tagging and lemmatization, that, surprisingly, the best interactive
                  performance of a system is not always correlated to its best automatic performance.
                  In other words, the most performing automatic linguistic behavior of a system is not
                  always yielding the best interactive behavior, when manual editing is involved.
               </p>
               <hr>
               <h4 id="taln-2007-long-033">Évaluation des stades de développement en français langue étrangère</h4>
               			Auteur : Jonas Granfeldt -
               			Contact : Jonas.Granfeldt@rom.lu.se<br>
               			Auteur : Pierre Nugues -
               			Contact : Pierre.Nugues@cs.lth.se<br><p>Cet article décrit un système pour définir et évaluer les stades de développement
                  en français langue étrangère. L’évaluation de tels stades correspond à l’identification
                  de la fréquence de certains phénomènes lexicaux et grammaticaux dans la production
                  des apprenants et comment ces fréquences changent en fonction du temps. Les problèmes
                  à résoudre dans cette démarche sont triples : identifier les attributs les plus révélateurs,
                  décider des points de séparation entre les stades et évaluer le degré d’efficacité
                  des attributs et de la classification dans son ensemble. Le système traite ces trois
                  problèmes. Il se compose d’un analyseur morphosyntaxique, appelé Direkt Profil, auquel
                  nous avons relié un module d’apprentissage automatique. Dans cet article, nous décrivons
                  les idées qui ont conduit au développement du système et son intérêt. Nous présentons
                  ensuite le corpus que nous avons utilisé pour développer notre analyseur morphosyntaxique.
                  Enfin, nous présentons les résultats sensiblement améliorés des classificateurs comparé
                  aux travaux précédents (Granfeldt et al., 2006). Nous présentons également une méthode
                  de sélection de paramètres afin d’identifier les attributs grammaticaux les plus appropriés.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes a system to define and evaluate stages of development in second
                  language French. The task of identifying such stages can be formulated as identifying
                  the frequency of some lexical and grammatical features in the learners’ production
                  and how they vary over time. The problems in this procedure are threefold : identify
                  the relevant features, decide on cutoff points for the stages, and evaluate the degree
                  of efficiency of the attributes and of the overall classification. The system addresses
                  these three problems. It consists of a morphosyntactic analyzer called Direkt Profil
                  and a machine-learning module connected to it. We first describe the usefulness and
                  rationale behind the development of the system. We then present the corpus we used
                  to develop our morphosyntactic analyzer called Direkt Profil. Finally, we present
                  new and substantially improved results on training machine-learning classifiers compared
                  to previous experiments (Granfeldt et al., 2006). We also introduce a method of attribute
                  selection in order to identify the most relevant grammatical features.
               </p>
               <hr>
               <h4 id="taln-2007-long-034">Apprentissage non supervisé de familles morphologiques par classification ascendante
                  hiérarchique
               </h4>
               			Auteur : Delphine Bernhard -
               			Contact : Delphine.Bernhard@imag.fr<br><p>Cet article présente un système d’acquisition de familles morphologiques qui procède
                  par apprentissage non supervisé à partir de listes de mots extraites de corpus de
                  textes. L’approche consiste à former des familles par groupements successifs, similairement
                  aux méthodes de classification ascendante hiérarchique. Les critères de regroupement
                  reposent sur la similarité graphique des mots ainsi que sur des listes de préfixes
                  et de paires de suffixes acquises automatiquement à partir des corpus traités. Les
                  résultats obtenus pour des corpus de textes de spécialité en français et en anglais
                  sont évalués à l’aide de la base CELEX et de listes de référence construites manuellement.
                  L’évaluation démontre les bonnes performances du système, indépendamment de la langue,
                  et ce malgré la technicité et la complexité morphologique du vocabulaire traité.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article describes a method for the unsupervised acquisition of morphological
                  families using lists of words extracted from text corpora. It proceeds by incrementally
                  grouping words in families, similarly to agglomerative hierarchical clustering methods.
                  Clustering criteria rely on graphical similarity as well as lists of prefixes and
                  suffix pairs which are automatically acquired from the target corpus. Results obtained
                  for specialised text corpora in French and English are evaluated using the CELEX database
                  and manually built reference lists. The evaluation shows that the system perfoms well
                  for both languages, despite the morphological complexity of the technical vocabulary
                  used for the evaluation.
               </p>
               <hr>
               <h4 id="taln-2007-long-035">Enchaînements verbaux – étude sur le temps et l'aspect utilisant des techniques d’apprentissage
                  non supervisé
               </h4>
               			Auteur : Catherine Recanati -
               			Contact : Catherine.Recanati@lipn.univ-paris13.fr<br>
               			Auteur : Nicoleta Rogovschi -
               			Contact : Nicoleta.Rogovschi@lipn.univ-paris13.fr<br><p>L’apprentissage non supervisé permet la découverte de catégories initialement inconnues.
                  Les techniques actuelles permettent d'explorer des séquences de phénomènes alors qu'on
                  a tendance à se focaliser sur l'analyse de phénomènes isolés ou sur la relation entre
                  deux phénomènes. Elles offrent ainsi de précieux outils pour l'analyse de données
                  organisées en séquences, et en particulier, pour la découverte de structures textuelles.
                  Nous présentons ici les résultats d’une première tentative de les utiliser pour inspecter
                  les suites de verbes provenant de phrases de récits d’accident de la route. Les verbes
                  étaient encodés comme paires (cat, temps), où cat représente la catégorie aspectuelle
                  d’un verbe, et temps son temps grammatical. L’analyse, basée sur une approche originale,
                  a fourni une classification des enchaînements de deux verbes successifs en quatre
                  groupes permettant de segmenter les textes. Nous donnons ici une interprétation de
                  ces groupes à partir de statistiques sur des annotations sémantiques indépendantes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Unsupervised learning allows the discovery of initially unknown categories. Current
                  techniques make it possible to explore sequences of phenomena whereas one tends to
                  focus on the analysis of isolated phenomena or on the relation between two phenomena.
                  They offer thus invaluable tools for the analysis of sequential data, and in particular,
                  for the discovery of textual structures. We report here the results of a first attempt
                  at using them for inspecting sequences of verbs coming from sentences of French accounts
                  of road accidents. Verbs were encoded as pairs (cat, tense) – where cat is the aspectual
                  category of a verb, and tense its grammatical tense. The analysis, based on an original
                  approach, provided a classification of the links between two successive verbs into
                  four distinct groups (clusters) allowing texts segmentation. We give here an interpretation
                  of these clusters by using statistics on semantic annotations independent of the training
                  process.
               </p>
               <hr>
               <h4 id="taln-2007-long-036">D-STAG : un formalisme pour le discours basé sur les TAG synchrones</h4>
               			Auteur : Laurence Danlos -
               			Contact : Laurence.Danlos@linguist.jussieu.fr<br><p>Nous proposons D-STAG, un formalisme pour le discours qui utilise les TAG synchrones.
                  Les analyses sémantiques produites par D-STAG sont des structures de discours hiérarchiques
                  annotées de relations de discours coordonnantes ou subordonnantes. Elles sont compatibles
                  avec les structures de discours produites tant en RST qu’en SDRT. Les relations de
                  discours coordonnantes et subordonnantes sont modélisées respectivement par les opérations
                  de substitution et d’adjonction introduites en TAG.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose D-STAG, a framework which uses Synchronous TAG for discourse. D-STAG semantic
                  analyses are hierarchical discourse structures richly annotated with coordinating
                  and subordinating discourse relations. They are compatible both with RST and SDRT
                  discourse structures. Coordinating and subordinating relations are respectively modeled
                  with the TAG substitution and adjunction operations.
               </p>
               <hr>
               <h4 id="taln-2007-long-037">Collocation translation based on sentence alignment and parsing</h4>
               			Auteur : Violeta Seretan -
               			Contact : Violeta.Seretan@lettres.unige.ch<br>
               			Auteur : Éric Wehrli -
               			Contact : Eric.Wehrli@lettres.unige.ch<br><p>Bien que de nombreux efforts aient été déployés pour extraire des collocations à partir
                  de corpus de textes, seule une minorité de travaux se préoccupent aussi de rendre
                  le résultat de l’extraction prêt à être utilisé dans les applications TAL qui pourraient
                  en bénéficier, telles que la traduction automatique. Cet article décrit une méthode
                  précise d’identification de la traduction des collocations dans un corpus parallèle,
                  qui présente les avantages suivants : elle peut traiter des collocation flexibles
                  (et pas seulement figées) ; elle a besoin de ressources limitées et d’un pouvoir de
                  calcul raisonnable (pas d’alignement complet, pas d’entraînement) ; elle peut être
                  appliquée à plusieurs paires des langues et fonctionne même en l’absence de dictionnaires
                  bilingues. La méthode est basée sur l’information syntaxique provenant du parseur
                  multilingue Fips. L’évaluation effectuée sur 4000 collocations de type verbe-objet
                  correspondant à plusieurs paires de langues a montré une précision moyenne de 89.8%
                  et une couverture satisfaisante (70.9%). Ces résultats sont supérieurs à ceux enregistrés
                  dans l’évaluation d’autres méthodes de traduction de collocations.
               </p><em>Version anglaise :</em><h4></h4>
               <p>To date, substantial efforts have been devoted to the extraction of collocations from
                  text corpora. However, only a few works deal with the subsequent processing of results
                  in order for these to be successfully integrated into the NLP applications that could
                  benefit from them (e.g., machine translation). This paper presents an accurate method
                  for identifying translation equivalents of collocations in parallel text, whose main
                  strengths are that : it can handle flexible (not only rigid) collocations ; it only
                  requires limited resources and computation (no full alignment, no training needed)
                  ; it deals with several language pairs, and it can even work when no bilingual dictionary
                  is available. The method relies heavily on syntactic information provided by the Fips
                  multilingual parser. Evaluation performed on 4000 verb-object collocations for different
                  language pairs showed an average accuracy of 89.8% and a reasonable coverage (70.9%).
                  These figures are higher that those reported in the evaluation of related work in
                  collocation translation.
               </p>
               <hr>
               <h4 id="taln-2007-long-038">Utilisation d’une approche basée sur la recherche cross-lingue d’information pour
                  l’alignement de phrases à partir de textes bilingues Arabe-Français
               </h4>
               			Auteur : Nasredine Semmar -
               			Contact : nasredine.semmar@cea.fr<br>
               			Auteur : Christian Fluhr -
               			Contact : christian.fluhr@cea.fr<br><p>L’alignement de phrases à partir de textes bilingues consiste à reconnaître les phrases
                  qui sont traductions les unes des autres. Cet article présente une nouvelle approche
                  pour aligner les phrases d’un corpus parallèle. Cette approche est basée sur la recherche
                  crosslingue d’information et consiste à construire une base de données des phrases
                  du texte cible et considérer chaque phrase du texte source comme une requête à cette
                  base. La recherche crosslingue utilise un analyseur linguistique et un moteur de recherche.
                  L’analyseur linguistique traite aussi bien les documents à indexer que les requêtes
                  et produit un ensemble de lemmes normalisés, un ensemble d’entités nommées et un ensemble
                  de mots composés avec leurs étiquettes morpho-syntaxiques. Le moteur de recherche
                  construit les fichiers inversés des documents en se basant sur leur analyse linguistique
                  et retrouve les documents pertinents à partir de leur indexes. L’aligneur de phrases
                  a été évalué sur un corpus parallèle Arabe-Français et les résultats obtenus montrent
                  que 97% des phrases ont été correctement alignées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Sentence alignment consists in identifying correspondences between sentences in one
                  language and sentences in the other language. This paper describes a new approach
                  to aligning sentences from a parallel corpora. This approach is based on cross-language
                  information retrieval and consists in building a database of sentences of the target
                  text and considering each sentence of the source text as a query to that database.
                  Cross-language information retrieval uses a linguistic analyzer and a search engine.
                  The linguistic analyzer processes both documents to be indexed and queries to produce
                  a set of normalized lemmas, a set of named entities and a set of nominal compounds
                  with their morpho-syntactic tags. The search engine builds the inverted files of the
                  documents on the basis of their linguistic analysis and retrieves the relevant documents
                  from the indexes. An evaluation of the sentence aligner was performed based on a Arabic
                  to French parallel corpus and results show that 97% of sentences were correctly aligned.
               </p>
               <hr>
               <h4 id="taln-2007-poster-001">Désambiguïsation lexicale automatique : sélection automatique d’indices</h4>
               			Auteur : Laurent Audibert -
               			Contact : laurent.audibert@lipn.univ-paris13.fr<br><p>Nous exposons dans cet article une expérience de sélection automatique des indices
                  du contexte pour la désambiguïsation lexicale automatique. Notre point de vue est
                  qu’il est plus judicieux de privilégier la pertinence des indices du contexte plutôt
                  que la sophistication des algorithmes de désambiguïsation utilisés. La sélection automatique
                  des indices par le biais d’un algorithme génétique améliore significativement les
                  résultats obtenus dans nos expériences précédentes tout en confortant des observations
                  que nous avions faites sur la nature et la répartition des indices les plus pertinents.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article describes an experiment on automatic features selection for word sense
                  disambiguation. Our point of view is that word sense disambiguation success is more
                  dependent on the features used to represent the context in which an ambiguous word
                  occurs than on the sophistication of the learning techniques used. Automatic features
                  selection using a genetic algorithm improves significantly our last experiment bests
                  results and is consistent with the observations we have made on the nature and space
                  distribution of the most reliable features.
               </p>
               <hr>
               <h4 id="taln-2007-poster-002">Représenter la dynamique énonciative et modale de textes</h4>
               			Auteur : Delphine Battistelli -
               			Contact : Delphine.Battistelli@paris4.sorbonne.fr<br>
               			Auteur : Marie Chagnoux -
               			Contact : Marie.Chagnoux@free.fr<br><p>Nous proposons d’exposer ici une méthodologie d’analyse et de représentation d’une
                  des composantes de la structuration des textes, celle liée à la notion de prise en
                  charge énonciative. Nous mettons l’accent sur la structure hiérarchisée des segments
                  textuels qui en résulte ; nous la représentons d’une part sous forme d’arbre et d’autre
                  part sous forme de graphe. Ce dernier permet d’appréhender la dynamique énonciative
                  et modale de textes comme un cheminement qui s’opère entre différents niveaux de discours
                  dans un texte au fur et à mesure de sa lecture syntagmatique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a methodological framework for analyzing and representing the concept of
                  commitment, which is one of the features characterizing textual structure. We emphasize
                  the hierarchical structure of textual segments commitment conveys to. We represent
                  it first as a tree and then as a graph. The latter enables us to access the modal
                  and enunciative textual dynamics, as it shows the path followed through different
                  discursive levels during the syntagmatic reading of a text.
               </p>
               <hr>
               <h4 id="taln-2007-poster-003">Segmentation en super-chunks</h4>
               			Auteur : Olivier Blanc -
               			Contact : oblanc@univ-mlv.fr<br>
               			Auteur : Matthieu Constant -
               			Contact : mconstan@univ-mlv.fr<br>
               			Auteur : Patrick Watrin -
               			Contact : watrin@univ-mlv.fr<br><p>Depuis l’analyseur développé par Harris à la fin des années 50, les unités polylexicales
                  ont peu à peu été intégrées aux analyseurs syntaxiques. Cependant, pour la plupart,
                  elles sont encore restreintes aux mots composés qui sont plus stables et moins nombreux.
                  Toutefois, la langue est remplie d’expressions semi-figées qui forment également des
                  unités sémantiques : les expressions adverbiales et les collocations. De même que
                  pour les mots composés traditionnels, l’identification de ces structures limite la
                  complexité combinatoire induite par l’ambiguïté lexicale. Dans cet article, nous détaillons
                  une expérience qui intègre ces notions dans un processus de segmentation en super-chunks,
                  préalable à l’analyse syntaxique. Nous montrons que notre chunker, développé pour
                  le français, atteint une précision et un rappel de 92,9 % et 98,7 %, respectivement.
                  Par ailleurs, les unités polylexicales réalisent 36,6 % des attachements internes
                  aux constituants nominaux et prépositionnels.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Since Harris’ parser in the late 50’s, multiword units have been progressively integrated
                  in parsers. Nevertheless, in the most part, they are still restricted to compound
                  words, that are more stable and less numerous. Actually, language is full of semi-frozen
                  expressions that also form basic semantic units : semi-frozen adverbial expressions
                  (e.g. time), collocations. Like compounds, the identification of these structures
                  limits the combinatorial complexity induced by lexical ambiguity. In this paper, we
                  detail an experiment that largely integrates these notions in a procedure of segmentation
                  into super-chunks, preliminary to a parser. We show that the chunker, developped for
                  French, reaches 92.9% precision and 98.7% recall. Moreover, multiword units realize
                  36.6% of the attachments within nominal and prepositional phrases.
               </p>
               <hr>
               <h4 id="taln-2007-poster-004">Détection et prédiction de la satisfaction des usagers dans les dialogues Personne-Machine</h4>
               			Auteur : Narjès Boufaden -
               			Contact : Narjes.Boufaden@crim.ca<br>
               			Auteur : Truong Le Hoang -
               			Contact : LeHoang.Truong@crim.ca<br>
               			Auteur : Pierre Dumouchel -
               			Contact : Pierre.Dumouchel@crim.ca<br><p>Nous étudions le rôle des entités nommées et marques discursives de rétroaction pour
                  la tâche de classification et prédiction de la satisfaction usager à partir de dialogues.
                  Les expériences menées sur 1027 dialogues Personne-Machine dans le domaine des agences
                  de voyage montrent que les entités nommées et les marques discursives n’améliorent
                  pas de manière significative le taux de classification des dialogues. Par contre,
                  elles permettent une meilleure prédiction de la satisfaction usager à partir des premiers
                  tours de parole usager.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We study the usefulness of named entities and acknowldgment words for user satisfaction
                  classification and prediction from Human-Computer dialogs. We show that named entities
                  and acknowledgment words do not enhance baseline classification performance. However,
                  they allow a better prediction of user satisfaction in the beginning of the dialogue.
               </p>
               <hr>
               <h4 id="taln-2007-poster-005">Les ellipses dans un système de traduction automatique de la parole</h4>
               			Auteur : Pierrette Bouillon -
               			Contact : Pierrette.Bouillon@issco.unige.ch<br>
               			Auteur : Manny Rayner -
               			Contact : Emmanuel.Rayner@issco.unige.ch<br>
               			Auteur : Marianne Starlander -
               			Contact : Marianne.Starlander@eti.unige.ch<br>
               			Auteur : Marianne Santaholma -
               			Contact : Marianne.Santaholma@eti.unige.ch<br><p>Dans tout dialogue, les phrases elliptiques sont très nombreuses. Dans cet article,
                  nous évaluons leur impact sur la reconnaissance et la traduction dans le système de
                  traduction automatique de la parole MedSLT. La résolution des ellipses y est effectuée
                  par une méthode robuste et portable, empruntée aux systèmes de dialogue homme-machine.
                  Cette dernière exploite une représentation sémantique plate et combine des techniques
                  linguistiques (pour construire la représentation) et basées sur les exemples (pour
                  apprendre sur la base d’un corpus ce qu’est une ellipse bien formée dans un sous-domaine
                  donné et comment la résoudre).
               </p><em>Version anglaise :</em><h4></h4>
               <p>Elliptical phrases are frequent in all genres of dialogue. In this paper, we describe
                  an evaluation of the speech understanding component of the MedSLT medical speech translation
                  system, which focusses on the contrast between system performance on elliptical phrases
                  and full utterances. Ellipsis resolution in the system is handled by a robust and
                  portable method, adapted from similar methods commonly used in spoken dialogue systems,
                  which exploits the flat representation structures used. The resolution module combines
                  linguistic methods, used to construct the representations, with an example-based approach
                  to defining the space of well-formed ellipsis resolutions in a subdomain.
               </p>
               <hr>
               <h4 id="taln-2007-poster-006">Analyse automatique de sondages téléphoniques d’opinion</h4>
               			Auteur : Nathalie Camelin -
               			Contact : nathalie.camelin@univ-avignon.fr<br>
               			Auteur : Frédéric Béchet -
               			Contact : frederic.bechet@univ-avignon.fr<br>
               			Auteur : Géraldine Damnati -
               			Contact : geraldine.damnati@francetelecom.com<br>
               			Auteur : Renato De Mori -
               			Contact : renato.demori@univ-avignon.fr<br><p>Cette étude présente la problématique de l’analyse automatique de sondages téléphoniques
                  d’opinion. Cette analyse se fait en deux étapes : tout d’abord extraire des messages
                  oraux les expressions subjectives relatives aux opinions de utilisateurs sur une dimension
                  particulière (efficacité, accueil, etc.) ; puis sélectionner les messages fiables,
                  selon un ensemble de mesures de confiance, et estimer la distribution des diverses
                  opinions sur le corpus de test. Le but est d’estimer une distribution aussi proche
                  que possible de la distribution de référence. Cette étude est menée sur un corpus
                  de messages provenant de vrais utilisateurs fournis par France Télécom R&amp;D.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper introduces the context of the automatic analysis of opinion telephone surveys.
                  This analysis is done by means of two stages : firstly the subjective expressions,
                  related to the expression of an opinion on a particular dimension (efficiency, courtesy,
                  . . . ), are extracted from the audio messages ; secondly the reliable messages, according
                  to a set of confidence measures, are selected and the distribution of the positive
                  and negative opinions in these messages is estimated. The goal is to obtain a distribution
                  as close as possible to the reference one. This study is carried on a telephone survey
                  corpus, provided by France Télécom R&amp;D, obtained in real field conditions.
               </p>
               <hr>
               <h4 id="taln-2007-poster-007">Une réalisateur de surface basé sur une grammaire réversible</h4>
               			Auteur : Claire Gardent -
               			Contact : Claire.Gardent@loria.fr<br>
               			Auteur : Éric Kow -
               			Contact : Eric.Kow@loria.fr<br><p>En génération, un réalisateur de surface a pour fonction de produire, à partir d’une
                  représentation conceptuelle donnée, une phrase grammaticale. Les réalisateur existants
                  soit utilisent une grammaire réversible et des méthodes statistiques pour déterminer
                  parmi l’ensemble des sorties produites la plus plausible ; soit utilisent des grammaires
                  spécialisées pour la génération et des méthodes symboliques pour déterminer la paraphrase
                  la plus appropriée à un contexte de génération donné. Dans cet article, nous présentons
                  GENI, un réalisateur de surface basé sur une grammaire d’arbres adjoints pour le français
                  qui réconcilie les deux approches en combinant une grammaire réversible avec une sélection
                  symbolique des paraphrases.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In generation, a surface realiser takes as input a conceptual representation and outputs
                  a grammatical sentence. Existing realisers fall into two camps. Either they are based
                  on a reversible grammar and use statistical filtering to determine among the several
                  outputs the most plausible one. Or they combine a grammar tailored for generation
                  and a symbolic means of choosing the paraphrase most appropriate to a given generation
                  context. In this paper, we present GENI, a surface realiser based on a Tree Adjoining
                  Grammar for French which reconciles both approaches in that (i) the grammar used is
                  réversible and (ii) paraphrase selection is based on symbolic means.
               </p>
               <hr>
               <h4 id="taln-2007-poster-008">Analyse des échecs d’une approche pour traiter les questions définitoires soumises
                  à un système de questions/réponses
               </h4>
               			Auteur : Laurent Gillard -
               			Contact : laurent.gillard@univ-avignon.fr<br>
               			Auteur : Patrice Bellot -
               			Contact : patrice.bellot@univ-avignon.fr<br>
               			Auteur : Marc El-Bèze -
               			Contact : marc.elbeze@univ-avignon.fr<br><p>Cet article revient sur le type particulier des questions définitoires étudiées dans
                  le cadre des campagnes d’évaluation des systèmes de Questions/Réponses. Nous présentons
                  l’approche développée suite à notre participation à la campagne EQueR et son évaluation
                  lors de QA@CLEF 2006. La réponse proposée est la plus représentative des expressions
                  présentes en apposition avec l’objet à définir, sa sélection est faite depuis des
                  indices dérivés de ces appositions. Environ 80% de bonnes réponses sont trouvées sur
                  les questions définitoires des volets francophones de CLEF. Les cas d’erreurs rencontrés
                  sont analysés et discutés en détail.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper proposes an approach to deal with definitional question answering. Our
                  system extracts answers to these questions from appositives appearing closed to the
                  subject to define. Results are presented for CLEF campaigns. Next, failures are discussed.
               </p>
               <hr>
               <h4 id="taln-2007-poster-009">Caractérisation des discours scientifiques et vulgarisés en français, japonais et
                  russe
               </h4>
               			Auteur : Lorraine Goeuriot -
               			Contact : lorraine.goeuriot@univ-nantes.fr<br>
               			Auteur : Natalia Grabar -
               			Contact : natalia.grabar@biomath.jussieu.fr<br>
               			Auteur : Béatrice Daille -
               			Contact : beatrice.daille@univ-nantes.fr<br><p>L’objectif principal de notre travail consiste à étudier la notion de comparabilité
                  des corpus, et nous abordons cette question dans un contexte monolingue en cherchant
                  à distinguer les documents scientifiques et vulgarisés. Nous travaillons séparément
                  sur des corpus composés de documents du domaine médical dans trois langues à forte
                  distance linguistique (le français, le japonais et le russe). Dans notre approche,
                  les documents sont caractérisés dans chaque langue selon leur thématique et une typologie
                  discursive qui se situe à trois niveaux de l’analyse des documents : structurel, modal
                  et lexical. Le typage des documents est implémenté avec deux algorithmes d’apprentissage
                  (SVMlight et C4.5). L’évaluation des résultats montre que la typologie discursive
                  proposée est portable d’une langue à l’autre car elle permet en effet de distinguer
                  les deux discours. Nous constatons néanmoins des performances très variées selon les
                  langues, les algorithmes et les types de caractéristiques discursives.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The main objective of our study consists to characterise the comparability of corpora,
                  and we address this issue in the monolingual context through the disctinction of expert
                  and non expert documents. We work separately with corpora composed of medical area
                  documents in three languages, which show an important linguistic distance between
                  them (French, Japanese and Russian). In our approach, documents are characterised
                  in each language through their thematic topic and through a discursive typology positioned
                  at three levels of document analysis : structural, modal and lexical. The document
                  typology is implemented with two learning algorithms (SVMlight and C4.5). Evaluation
                  of results shows that the proposed discursive typology can be transposed from one
                  language to another, as it indeed allows to distinguish the two aimed discourses.
                  However, we observe that performances vary a lot according to languages, algorithms
                  and types of discursive characteristics.
               </p>
               <hr>
               <h4 id="taln-2007-poster-010">OGMIOS : une plate-forme d’annotation linguistique de collection de documents issus
                  du Web
               </h4>
               			Auteur : Thierry Hamon -
               			Contact : Thierry.Hamon@lipn.univ-paris13.fr<br>
               			Auteur : Julien Derivière -
               			Contact : Julien.Derivière@lipn.univ-paris13.fr<br>
               			Auteur : Adeline Nazarenko -
               			Contact : Adeline.Nazarenko@lipn.univ-paris13.fr<br><p>L’un des objectifs du projet ALVIS est d’intégrer des informations linguistiques dans
                  des moteurs de recherche spécialisés. Dans ce contexte, nous avons conçu une plate-forme
                  d’enrichissement linguistique de documents issus du Web, OGMIOS, exploitant des outils
                  de TAL existants. Les documents peuvent être en français ou en anglais. Cette architecture
                  est distribuée, afin de répondre aux contraintes liées aux traitements de gros volumes
                  de textes, et adaptable, pour permettre l’analyse de sous-langages. La plate-forme
                  est développée en Perl et disponible sous forme de modules CPAN. C’est une structure
                  modulaire dans lequel il est possible d’intégrer de nouvelles ressources ou de nouveaux
                  outils de TAL. On peut ainsi définir des configuration différentes pour différents
                  domaines et types de collections. Cette plateforme robuste permet d’analyser en masse
                  des données issus du web qui sont par essence très hétérogènes. Nous avons évalué
                  les performances de la plateforme sur plusieurs collections de documents. En distribuant
                  les traitements sur vingt machines, une collection de 55 329 documents du domaine
                  de la biologie (106 millions de mots) a été annotée en 35 heures tandis qu’une collection
                  de 48 422 dépêches relatives aux moteurs de recherche (14 millions de mots) a été
                  annotée en 3 heures et 15 minutes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In the context of the ALVIS project, which aims at integrating linguistic information
                  in topic-specific search engines, we developed an NLP architecture, OGMIOS, to linguistically
                  annotate large collections of web documents with existing NLP tools. Documents can
                  be written in French or English. The distributed architecture allows us to take into
                  account constraints related to the scalability problem of Natural Language Processing
                  and the domain specific tuning of the linguistic analysis. The platform is developed
                  in Perl and is available as CPAN modules. It is a modularized framework where new
                  resources or NLP tools can be integrated. Then, various configurations are easy to
                  define for various domains and collections. This platform is robust to massively analyse
                  web document collections which are heterogeneous in essence. We carried out experiments
                  on two different collections of web documents on 20 computers. A 55,329 web documents
                  collection dealing with biology (106 millions of words) has been annotated in 35 hours,
                  whereas a 48,422 search engine news collection (14 millions of word) has been annotated
                  in 3 hours and 15 minutes.
               </p>
               <hr>
               <h4 id="taln-2007-poster-011">Les Lexiques-Miroirs. Du dictionnaire bilingue au graphe multilingue</h4>
               			Auteur : Sébastien Haton -
               			Contact : sebastien.haton@atilf.fr<br>
               			Auteur : Jean-Marie Pierrel -
               			Contact : jean-marie.pierrel@atilf.fr<br><p>On observe dans les dictionnaires bilingues une forte asymétrie entre les deux parties
                  d’un même dictionnaire et l’existence de traductions et d’informations « cachées »,
                  i.e. pas directement visibles à l’entrée du mot à traduire. Nous proposons une méthodologie
                  de récupération des données cachées ainsi que la « symétrisation » du dictionnaire
                  grâce à un traitement automatique. L’étude d’un certain nombre de verbes et de leurs
                  traductions en plusieurs langues a conduit à l’intégration de toutes les données,
                  visibles ou cachées, au sein d’une base de données unique et multilingue. L’exploitation
                  de la base de données a été rendue possible par l’écriture d’un algorithme de création
                  de graphe synonymique qui lie dans un même espace les mots de langues différentes.
                  Le programme qui en découle permettra de générer des dictionnaires paramétrables directement
                  à partir du graphe.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Lexical asymmetry and hidden data, i.e. not directly visible into one lexical entry,
                  are phenomena peculiar to most of the bilingual dictionaries. Our purpose is to establish
                  a methodology to highlight both phenomena by extracting hidden data from the dictionary
                  and by re-establishing symmetry between its two parts. So we studied a large number
                  of verbs and integrated them into a unique multilingual database. At last, our database
                  is turned into a “multilexical” graph thanks to an algorithm, which is binding together
                  words from different languages into the same semantic space. This will allow us to
                  generate automatically dictionaries straight from the graph.
               </p>
               <hr>
               <h4 id="taln-2007-poster-012">Traduction, restructurations syntaxiques et grammaires de correspondance</h4>
               			Auteur : Sylvain Kahane -
               			Contact : sk@ccr.jussieu.fr<br><p>Cet article présente une nouvelle formalisation du modèle de traduction par transfert
                  de la Théorie Sens-Texte. Notre modélisation utilise les grammaires de correspondance
                  polarisées et fait une stricte séparation entre les modèles monolingues, un lexique
                  bilingue minimal et des règles de restructuration universelles, directement associées
                  aux fonctions lexicales syntaxiques.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a new formalisation of transfer-based translation model of the
                  Meaning-Text Theory. Our modelling is based on polarized correspondence grammars and
                  observes a strict separation between monolingual models, the bilingual lexicon and
                  universal restructuring rules, directly associated with syntactic lexical functions.
               </p>
               <hr>
               <h4 id="taln-2007-poster-013">Modélisation des paradigmes de flexion des verbes arabes selon la norme LMF - ISO
                  24613
               </h4>
               			Auteur : Aïda Khemakhem -
               			Contact : khemakhem.aida@gnet.tn<br>
               			Auteur : Bilel Gargouri -
               			Contact : Bilel.Gargouri@fsegs.rnu.tn<br>
               			Auteur : Abdelhamid Abdelwahed -
               			Contact : abdelhamid.abdelwahed@yahoo.fr<br>
               			Auteur : Gil Francopoulo -
               			Contact : gil.francopoulo@wanadoo.fr<br><p>Dans cet article, nous spécifions les paradigmes de flexion des verbes arabes en respectant
                  la version 9 de LMF (Lexical Markup Framework), future norme ISO 24613 qui traite
                  de la standardisation des bases lexicales. La spécification de ces paradigmes se fonde
                  sur une combinaison des racines et des schèmes. En particulier, nous mettons en relief
                  les terminaisons de racines sensibles aux ajouts de suffixes et ce, afin de couvrir
                  les situations non considérées dans les travaux existants. L’élaboration des paradigmes
                  de flexion verbale que nous proposons est une description en intension d'ArabicLDB
                  (Arabic Lexical DataBase) qui est une base lexicale normalisée pour la langue arabe.
                  Nos travaux sont illustrés par la réalisation d’un conjugueur des verbes arabes à
                  partir d'ArabicLDB.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we specify the inflected paradigms of Arabic verbs with respect to
                  the version 9 of LMF (Lexical Markup Framework) which is the expected ISO 24613 standard
                  dealing with the standardisation of lexical databases. The specification of these
                  paradigms is based on a combination of schemes and roots. In particular, we highlight
                  the role of root endings that is not considered in other researches and that may generate
                  erroneous forms while concatenating suffixes. The development of verbal inflected
                  paradigms that we propose is an intentional component of ArabicLDB (Arabic Lexical
                  DataBase) which is a normalized Arabic lexical database that we developed according
                  to LMF. Our works are illustrated by the realization of a conjugation tool for Arabic
                  verbs using ArabicLDB.
               </p>
               <hr>
               <h4 id="taln-2007-poster-014">Du bruit, du silence et des ambiguïtés : que faire du TAL pour l'apprentissage des
                  langues ?
               </h4>
               			Auteur : Olivier Kraif -
               			Contact : Olivier.Kraif@u-grenoble3.fr<br>
               			Auteur : Claude Ponton -
               			Contact : Claude.Ponton@u-grenoble3.fr<br><p>Nous proposons une nouvelle approche pour l’intégration du TAL dans les systèmes d’apprentissage
                  des langues assisté par ordinateur (ALAO), la stratégie « moinsdisante ». Cette approche
                  tire profit des technologies élémentaires mais fiables du TAL et insiste sur la nécessité
                  de traitements modulaires et déclaratifs afin de faciliter la portabilité et la prise
                  en main didactique des systèmes. Basé sur cette approche, ExoGen est un premier prototype
                  pour la génération automatique d’activités lacunaires ou de lecture d’exemples. Il
                  intègre un module de repérage et de description des réponses des apprenants fondé
                  sur la comparaison entre réponse attendue et réponse donnée. L’analyse des différences
                  graphiques, orthographiques et morphosyntaxiques permet un diagnostic des erreurs
                  de type fautes d’orthographe, confusions, problèmes d’accord, de conjugaison, etc.
                  La première évaluation d’ExoGen sur un extrait du corpus d’apprenants FRIDA produit
                  des résultats prometteurs pour le développement de cette approche « moins-disante
                  », et permet d'envisager un modèle d'analyse performant et généralisable à une grande
                  variété d'activités.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents the so-called "moins-disante" strategy, a new approach for NLP
                  integrating in Computer Assisted Language Learning (CALL) systems. It is based on
                  the implementation of basic but reliable NLP techniques, and put emphasis on declarative
                  and modular processing, for the sake of portability and didactic implementation. Based
                  on this approach, ExoGen is a prototype for generating activities such as gap filling
                  exercises. It integrates a module for error detection and description, which checks
                  learners' answers against expected ones. Through the analysis of graphic, orthographic
                  and morphosyntactic differences, it is able to diagnose problems like spelling errors,
                  lexical mix-up, error prone agreement, bad conjugations, etc. The first evaluation
                  of ExoGen outputs, based on the FRIDA learner corpus, has yielded very promising results,
                  paving the way for the development of an efficient and general model tailored to a
                  wide variety of activities.
               </p>
               <hr>
               <h4 id="taln-2007-poster-015">Extraction automatique de cadres de sous-catégorisation verbale pour le français à
                  partir d’un corpus arboré
               </h4>
               			Auteur : Anna Kupsc -
               			Contact : akupsc@univ-paris3.fr<br><p>Nous présentons une expérience d’extraction automatique des cadres de souscatégorisation
                  pour 1362 verbes français. Nous exploitons un corpus journalistique richement annoté
                  de 15 000 phrases dont nous extrayons 12 510 occurrences verbales. Nous évaluons dans
                  un premier temps l’extraction des cadres basée sur la fonction des arguments, ce qui
                  nous fournit 39 cadres différents avec une moyenne de 1.54 cadres par lemme. Ensuite,
                  nous adoptons une approche mixte (fonction et catégorie syntaxique) qui nous fournit
                  dans un premier temps 925 cadres différents, avec une moyenne de 3.44 cadres par lemme.
                  Plusieurs méthodes de factorisation, neutralisant en particulier les variantes de
                  réalisation avec le passif ou les pronoms clitiques, sont ensuite appliquées et nous
                  permettent d’aboutir à 235 cadres différents avec une moyenne de 1.94 cadres par verbe.
                  Nous comparons brièvement nos résultats avec les travaux existants pour le français
                  et pour l’anglais.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present our work on automatic extraction of subcategorisation frames for 1362 French
                  verbs. We use a treebank of 15000 sentences from which we extract 12510 verb occurrences.
                  We evaluate the results based on a functional representation of frames and we acquire
                  39 different frames, 1.54 per lemma on average. Then, we adopt a mixed representation
                  (functions and categories), which leads to 925 different frames, 3.44 frames on average.
                  We investigate several methods to reduce the ambiguity (e.g., neutralisation of passive
                  forms or clitic arguments), which allows us to arrive at 235 frames, with 1.94 frames
                  per lemma on average. We present a brief comparison with the existing work on French
                  and English.
               </p>
               <hr>
               <h4 id="taln-2007-poster-016">Vers une formalisation des décompositions sémantiques dans la Grammaire d’Unification
                  Sens-Texte
               </h4>
               			Auteur : François Lareau -
               			Contact : francois.lareau@umontreal.ca<br><p>Nous proposons une formalisation de la décomposition du sens dans le cadre de la Grammaire
                  d’Unification Sens-Texte. Cette formalisation vise une meilleure intégration des décompositions
                  sémantiques dans un modèle global de la langue. Elle repose sur un jeu de saturation
                  de polarités qui permet de contrôler la construction des représentations décomposées
                  ainsi que leur mise en correspondance avec des arbres syntaxiques qui les expriment.
                  Le formalisme proposé est illustré ici dans une perspective de synthèse, mais il s’applique
                  également en analyse.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a formal representation of meaning decomposition in the framework of the
                  Meaning-Text Unification Grammar. The proposed technique aims at offering a better
                  integration of such semantic decompositions into a global model of the language. It
                  relies on the saturation of polarities to control the construction of decomposed respresentations
                  as well as their mapping to the syntactic trees that express them. The proposed formalism
                  is discussed from the viewpoint of generation, but it applies to analysis as well.
               </p>
               <hr>
               <h4 id="taln-2007-poster-017">Systèmes de questions-réponses : vers la validation automatique des réponses</h4>
               			Auteur : Anne-Laure Ligozat -
               			Contact : Anne-Laure.Ligozat@limsi.fr<br>
               			Auteur : Brigitte Grau -
               			Contact : Brigitte Grau@limsi.fr<br>
               			Auteur : Isabelle Robba -
               			Contact : Isabelle.Robba@limsi.fr<br>
               			Auteur : Anne Vilnat -
               			Contact : Anne.Vilnat@limsi.fr<br><p>Les systèmes de questions-réponses (SQR) ont pour but de trouver une information précise
                  extraite d’une grande collection de documents comme le Web. Afin de pouvoir comparer
                  les différentes stratégies possibles pour trouver une telle information, il est important
                  d’évaluer ces systèmes. L’objectif d’une tâche de validation de réponses est d’estimer
                  si une réponse donnée par un SQR est correcte ou non, en fonction du passage de texte
                  donné comme justification. En 2006, nous avons participé à une tâche de validation
                  de réponses, et dans cet article nous présentons la stratégie que nous avons utilisée.
                  Celle-ci est fondée sur notre propre système de questions-réponses. Le principe est
                  de comparer nos réponses avec les réponses à valider. Nous présentons les résultats
                  obtenus et montrons les extensions possibles. À partir de quelques exemples, nous
                  soulignons les difficultés que pose cette tâche.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Question answering aims at retrieving precise information from a large collection
                  of documents, typically theWeb. Different techniques can be used to find relevant
                  information, and to compare these techniques, it is important to evaluate question
                  answering systems. The objective of an Answer Validation task is to estimate the correctness
                  of an answer returned by a QA system for a question, according to the text snippet
                  given to support it.We participated in such a task in 2006. In this article, we present
                  our strategy for deciding if the snippets justify the answers. We used a strategy
                  based on our own question answering system, and compared the answers it returned with
                  the answer to judge. We discuss our results, and show the possible extensions of our
                  strategy. Then we point out the difficulties of this task, by examining different
                  examples.
               </p>
               <hr>
               <h4 id="taln-2007-poster-018">Ressources lexicales chinoises pour le TALN</h4>
               			Auteur : Huei-Chi Lin -
               			Contact : lin_huei_chi@yahoo.fr<br>
               			Auteur : Max Silberztein -
               			Contact : max.silberztein@univ-fcomte.fr<br><p>Nous voulons traiter des textes chinois automatiquement ; pour ce faire, nous formalisons
                  le vocabulaire chinois, en utilisant principalement des dictionnaires et des grammaires
                  morphologiques et syntaxiques formalisés avec le logiciel NooJ. Nous présentons ici
                  les critères linguistiques qui nous ont permis de construire dictionnaires et grammaires,
                  sachant que l’application envisagée (linguistique de corpus) nous impose certaines
                  contraintes dans la formalisation des unités de la langue, en particulier des composés.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In order to parse Chinese texts automatically, we need to formalize the Chinese vocabulary
                  by using electronic dictionaries and morphological and syntactic grammars. We have
                  used the NooJ software to enter the formalization. We present here the set of linguistic
                  criteria used to construct these dictionaries and grammars, so that they can be used
                  by corpus-linguistic applications. We focus our discussion on the characterization
                  of Chinese linguistic units, specifically compounds.
               </p>
               <hr>
               <h4 id="taln-2007-poster-019">Étiquetage morpho-syntaxique de textes kabyles</h4>
               			Auteur : Sinikka Loikkanen -
               			Contact : sinikka.loikkanen@helsinki.fi<br><p>Cet article présente la construction d’un étiqueteur morpho-syntaxique développé pour
                  annoter un corpus de textes kabyles (1 million de mots). Au sein de notre projet,
                  un étiqueteur morpho-syntaxique a été développé et implémenté. Ceci inclut un analyseur
                  morphologique ainsi que l’ensemble de règles de désambiguïsation qui se basent sur
                  l’approche supervisée à base de règles. Pour effectuer le marquage, un jeu d’étiquettes
                  morpho-syntaxiques pour le kabyle est proposé. Les résultats préliminaires sont très
                  encourageants. Nous obtenons un taux d’étiquetage réussi autour de 97 % des textes
                  en prose.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes the construction of a morpho-syntactic tagger developed to annotate
                  our Kabyle text corpus (1 million words).Within our project, a part-of-speech tagger
                  has been developed and implemented. That includes a morphological analyser and a set
                  of disambiguation rules based on supervised rule-based tagging. To realise the annotation,
                  a POS tagset for Kabyle is proposed. The first results of tests are very encouraging.
                  At present stage, our tagger reaches 97 % of success.
               </p>
               <hr>
               <h4 id="taln-2007-poster-020">Analyse syntaxique et traitement automatique du syntagme nominal grec moderne</h4>
               			Auteur : Athina Michou -
               			Contact : Athina.Michou@lettres.unige.ch<br><p>Cet article décrit le traitement automatique du syntagme nominal en grec moderne par
                  le modèle d’analyse syntaxique multilingue Fips. L’analyse syntaxique linguistique
                  est focalisée sur les points principaux du DP grec : l’accord entre les constituants
                  fléchis, l’ordre flexible des constituants, la cliticisation sur les noms et le phénomène
                  de la polydéfinitude. Il est montré comment ces phénomènes sont traités et implémentés
                  dans le cadre de l’analyseur syntaxique FipsGreek, qui met en oeuvre un formalisme
                  inspiré de la grammaire générative chomskyenne.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article describes an automatic treatment to the Modern Greek noun phrase in terms
                  of the Fips multilingual syntactic parser. The syntactic analysis focuses on the main
                  issues related to the Greek DP: the agreement among the inflected constituents, the
                  relatively free constituent order, noun cliticisation, and the polydefiniteness phenomenon.
                  The paper discusses how these processes are treated and implemented within the FipsGreek
                  parser, which puts forth a formalism relying on Chomsky’s generative grammar.
               </p>
               <hr>
               <h4 id="taln-2007-poster-021">Apprentissage symbolique de grammaires et traitement automatique des langues</h4>
               			Auteur : Erwan Moreau -
               			Contact : Erwan.Moreau@univ-nantes.fr<br><p>Le modèle de Gold formalise le processus d’apprentissage d’un langage. Nous présentons
                  dans cet article les avantages et inconvénients de ce cadre théorique contraignant,
                  dans la perspective d’applications en TAL. Nous décrivons brièvement les récentes
                  avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Gold’s model formalizes the learning process of a language. In this paper we present
                  the advantages and drawbacks of this restrictive theoretical framework, in the viewpoint
                  of applications to NLP.We briefly describe recent advances in the domain which, in
                  our opinion, raise some important questions.
               </p>
               <hr>
               <h4 id="taln-2007-poster-022">Méthodes d’alignement des propositions : un défi aux traductions croisées</h4>
               			Auteur : Yayoi Nakamura-Delloye -
               			Contact : yayoi@free.fr<br><p>Le présent article décrit deux méthodes d’alignement des propositions : l’une basée
                  sur les méthodes d’appariement des graphes et une autre inspirée de la classification
                  ascendante hiérarchique (CAH). Les deux méthodes sont caractérisées par leur capacité
                  d’alignement des traductions croisées, ce qui était impossible pour beaucoup de méthodes
                  classiques d’alignement des phrases. Contrairement aux résultats obtenus avec l’approche
                  spectrale qui nous paraissent non satisfaisants, l’alignement basé sur la méthode
                  de classification ascendante hiérarchique est prometteur dans la mesure où cette technique
                  supporte bien les traductions croisées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The present paper describes two methods for clauses alignment. The first one uses
                  a graph matching approach, while the second one relies on agglomerative hirerarchical
                  clustering (AHC). Both methods are characterized by the fact they can align cross
                  translations, which was impossible for previous classic sentence alignment methods.
                  Though the results given by the spectral method are unsatisfactory, the method based
                  on AHC is very promising. It handles correctly cross translations.
               </p>
               <hr>
               <h4 id="taln-2007-poster-023">Un Lexique Génératif de référence pour le français</h4>
               			Auteur : Fiammetta Namer -
               			Contact : fiammetta.namer@univ-nancy2.fr<br>
               			Auteur : Pierrette Bouillon -
               			Contact : pierrette.bouillon@issco.unige.ch<br>
               			Auteur : Evelyne Jacquey -
               			Contact : evelyne.jacquey@atilf.fr<br><p>Cet article propose une approche originale visant la construction d’un lexique sémantique
                  de référence sur le français. Sa principale caractéristique est de pouvoir s’appuyer
                  sur les propriétés morphologiques des lexèmes. La méthode combine en effet des résultats
                  d’analyse morphologique (Namer, 2002;2003), à partir de ressources lexicales de grande
                  taille (nomenclatures du TLF) et des méthodologies d’acquisition d’information lexicale
                  déjà éprouvées (Namer 2005; Sébillot 2002). Le format de représentation choisi, dans
                  le cadre du Lexique Génératif, se distingue par ses propriétés d’expressivité et d’économie.
                  Cette approche permet donc d’envisager la construction d’un lexique de référence sur
                  le français caractérisé par une forte homogénéité tout en garantissant une couverture
                  large, tant du point de vue de la nomenclature que du point de vue des contenus sémantiques.
                  Une première validation de la méthode fournit une projection quantitative et qualitative
                  des résultats attendus.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes an original approach aiming at building a reference semantic
                  lexicon for French. Its main characteristic is that of being able to rely on morphological
                  properties. The method thus combines morphological analyses results (Namer 2002;2003;2005)
                  from large scale lexical resources (i.e. TLF word lists) with already tested acquisition
                  methodologies on lexical information (Sébillot, 2002). The representation format,
                  within the Generative Lexicon framework, has been chosen for its expressiveness and
                  economy features. So, this approach allows us to consider building a reference lexicon
                  for French, which is fundamentally homogeneous as well as of a large coverage. A feasability
                  study of the described method provides a projection of expected results, from both
                  quantitative and qualitative points of view.
               </p>
               <hr>
               <h4 id="taln-2007-poster-024">Les résultats de la campagne EASY d’évaluation des analyseurs syntaxiques du français</h4>
               			Auteur : Patrick Paroubek -
               			Contact : pap@limsi.fr<br>
               			Auteur : Anne Vilnat -
               			Contact : anne@limsi.fr<br>
               			Auteur : Isabelle Robba -
               			Contact : isabelle@limsi.fr<br>
               			Auteur : Christelle Ayache -
               			Contact : ayache@elda.fr<br><p>Dans cet article, nous présentons les résultats de la campagne d’évaluation EASY des
                  analyseurs syntaxiques du français. EASY a été la toute première campagne d’évaluation
                  comparative des analyseurs syntaxiques du français en mode boîte noire utilisant des
                  mesures objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Ministère
                  délégué à la Recherche et à l’Éducation, avec le soutien du ministère de délégué à
                  l’industrie et du ministère de la culture et de la communication. Nous exposons tout
                  d’abord la position de la campagne par rapport aux autres projets d’évaluation en
                  analyse syntaxique, puis nous présentos son déroulement, et donnons les résultats
                  des 15 analyseurs participants en fonction des différents types de corpus et des différentes
                  annotations (constituants et relations). Nous proposons ensuite un ensemble de leçons
                  à tirer de cette campagne, en particulier à propos du protocole d’évaluation, de la
                  définition de la segmentation en unités linguistiques, du formalisme et des activités
                  d’annotation, des critères de qualité des données, des annotations et des résultats,
                  et finalement de la notion de référence en analyse syntaxique. Nous concluons en présentant
                  comment les résultats d’EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013)
                  qui vient de débuter et dont l’objectif est d’étiqueter un grand corpus par plusieurs
                  analyseurs en les combinant selon des paramètres issus de l’évaluation.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present the results of the EASY evaluation campaign on parsers of
                  French. EASY has been the very first black-box comparative evaluation campaign for
                  parsers of French, with objective quantitative performance measures. EASY was part
                  of the TECHNOLANGUE program of the Delegate Ministry of Research, jointly supported
                  by the Delegate Ministry of Industry and the ministry of Culture and Communication.
                  After setting EASY in the context of parsing evaluation and giving an account of the
                  campaign, we present the results obtained by 15 parsers according to syntactic relation
                  and subcorpus genre. Then we propose some lessons to draw from this campaign, in particular
                  about the evaluation protocole, the segmenting into linguistic units, the formalism
                  and the annotation activities, the quality criteria to apply for data, annotations
                  and results and finally about the notion of reference for parsing. We conclude by
                  showing how EASY results extend through the PASSAGE project (ANR-06-MDCA-013), which
                  has just started and whose aim is the automatic annotation of a large corpus by several
                  parsers, the combination of which being parametrized by results stemming from evaluation.
               </p>
               <hr>
               <h4 id="taln-2007-poster-025">Modèles statistiques enrichis par la syntaxe pour la traduction automatique</h4>
               			Auteur : Holger Schwenk -
               			Contact : schwenk@limsi.fr<br>
               			Auteur : Daniel Déchelotte -
               			Contact : dechelot@limsi.fr<br>
               			Auteur : Hélène Bonneau-Maynard -
               			Contact : hbm@limsi.fr<br>
               			Auteur : Alexandre Allauzen -
               			Contact : allauzen@limsi.fr<br><p>La traduction automatique statistique par séquences de mots est une voie prometteuse.
                  Nous présentons dans cet article deux évolutions complémentaires. La première permet
                  une modélisation de la langue cible dans un espace continu. La seconde intègre des
                  catégories morpho-syntaxiques aux unités manipulées par le modèle de traduction. Ces
                  deux approches sont évaluées sur la tâche Tc-Star. Les résultats les plus intéressants
                  sont obtenus par la combinaison de ces deux méthodes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Statistical phrase-based translation models are very efficient. In this paper, we
                  present two complementary methods. The first one consists in a a statistical language
                  model that is based on a continuous representation of the words in the vocabulary.
                  By these means we expect to take better advantage of the limited amount of training
                  data. In the second method, morpho-syntactic information is incorporated into the
                  translation model in order to obtain lexical disambiguation. Both approaches are evaluated
                  on the Tc-Star task. Most promising results are obtained by combining both methods.
               </p>
               <hr>
               <h4 id="taln-2007-poster-026">Traitements phrastiques phonétiques pour la réécriture de phrases dysorthographiées</h4>
               			Auteur : Laurianne Sitbon -
               			Contact : laurianne.sitbon@univ-avignon.fr<br>
               			Auteur : Patrice Bellot -
               			Contact : patrice.bellot@univ-avignon.fr<br>
               			Auteur : Philippe Blache -
               			Contact : blache@lpl.univ-aix.fr<br><p>Cet article décrit une méthode qui combine des hypothèses graphémiques et phonétiques
                  au niveau de la phrase, à l’aide d’une réprésentation en automates à états finis et
                  d’un modèle de langage, pour la réécriture de phrases tapées au clavier par des dysorthographiques.
                  La particularité des écrits dysorthographiés qui empêche les correcteurs orthographiques
                  d’être efficaces pour cette tâche est une segmentation en mots parfois incorrecte.
                  La réécriture diffère de la correction en ce sens que les phrases réécrites ne sont
                  pas à destination de l’utilisateur mais d’un système automatique, tel qu’un moteur
                  de recherche. De ce fait l’évaluation est conduite sur des versions filtrées et lemmatisées
                  des phrases. Le taux d’erreurs mots moyen passe de 51 % à 20 % avec notre méthode,
                  et est de 0 % sur 43 % des phrases testées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper introduces a sentence level method combining written correction and phonetic
                  interpretation in order to automatically rewrite sentences typed by dyslexic spellers.
                  The method uses a finite state automata framework and a language model. Dysorthographics
                  refers to incorrect word segmentation which usually causes classical spelling correctors
                  fail. Our approach differs from spelling correction in that we aim to use several
                  rewritings as an expression of the user need in an information retrieval context.
                  Our system is evaluated on questions collected with the help of an orthophonist. The
                  word error rate on lemmatised sentences falls from 51 % to 20 % (falls to 0 % on 43
                  % of sentences).
               </p>
               <hr>
               <h4 id="taln-2007-poster-027">Vers une méthodologie générique de contrôle basée sur la combinaison de sources de
                  jugement
               </h4>
               			Auteur : Grégory Smits -
               			Contact : gsmits@info.unicaen.fr<br>
               			Auteur : Christine Chardenon -
               			Contact : christine.chardenon@orange-ftgroup.com<br><p>Le contrôle des hypothèses concurrentes générées par les différents modules qui peuvent
                  intervenir dans des processus de TALN reste un enjeu important malgré de nombreuses
                  avancées en terme de robustesse. Nous présentons dans cet article une méthodologie
                  générique de contrôle exploitant des techniques issues de l’aide multicritère à la
                  décision. À partir de l’ensemble des critères de comparaison disponibles et la formalisation
                  des préférences d’un expert, l’approche proposée évalue la pertinence relative des
                  différents objets linguistiques générés et conduit à la mise en place d’une action
                  de contrôle appropriée telle que le filtrage, le classement, le tri ou la propagation.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The control of concurrent hypotheses generated by the different modules which compose
                  NLP processes is still an important issue despite advances concerning robustness.
                  In this article, we present a generic methodology of control inspired from multicriteria
                  decision aid methods. Based on available comparison criteria and formalized expert
                  knowledge, the proposed approach evaluate the relevancy of each generated linguistic
                  object and lead to the decision of an appropriate control action such as filtering,
                  ordering, sorting or propagating.
               </p>
               <hr>
               <h4 id="taln-2007-poster-028">Traitement sémantique par analyse distributionnelle des noms transdisciplinaires des
                  écrits scientifiques
               </h4>
               			Auteur : Agnès Tutin -
               			Contact : agnes.tutin@u-grenoble3.fr<br><p>Dans cette étude sur le lexique transdisciplinaire des écrits scientifiques, nous
                  souhaitons évaluer dans quelle mesure les méthodes distributionnelles de TAL peuvent
                  faciliter la tâche du linguiste dans le traitement sémantique de ce lexique. Après
                  avoir défini le champ lexical et les corpus exploités, nous testons plusieurs méthodes
                  basées sur des dépendances syntaxiques et observons les proximités sémantiques et
                  les classes établies. L’hypothèse que certaines relations syntaxiques - en particulier
                  les relations de sous-catégorisation – sont plus appropriées pour établir des classements
                  sémantiques n’apparaît qu’en partie vérifiée. Si les relations de sous-catégorisation
                  génèrent des proximités sémantiques entre les mots de meilleure qualité, cela ne semble
                  pas le cas pour la classification par voisinage.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this study about general scientific lexicon, we aim at evaluating to what extent
                  distributional methods in NLP can enhance the linguist’s task in the semantic treatment.
                  After a definition of our lexical field and a presentation of our corpora, we evaluate
                  several methods based on syntactic dependencies for establishing semantic similarities
                  and semantic classes. Our hypothesis that some syntactic relations – namely subcategorized
                  relations – is more relevant to establish semantic classes does not entirely appears
                  valid. If subcategorized relations produce better semantic links between words, this
                  is not the case with neighbour joigning clustering method.
               </p>
               <hr>
               <h4 id="taln-2007-poster-029">Une expérience de compréhension en contexte de dialogue avec le système LOGUS, approche
                  logique de la compréhension de la langue orale
               </h4>
               			Auteur : Jeanne Villaneau -
               			Contact : Jeanne.Villaneau@univ-ubs.fr<br><p>LOGUS est un système de compréhension de la langue orale dans le cadre d’un dialogue
                  homme-machine finalisé. Il est la mise en oeuvre d’une approche logique qui utilise
                  différents formalismes afin d’obtenir un système robuste mais néanmoins relativement
                  extensible. Cet article décrit essentiellement l’étape de compréhension en contexte
                  de dialogue implémentée sur LOGUS, développée et testée à partir d’un corpus de réservation
                  hôtelière enregistré et annoté lors des travaux du groupe MEDIA du projet technolangue.
                  Il décrit également les différentes interrogations et conclusions que peut susciter
                  une telle expérience et les résultats obtenus par le système dans la résolution des
                  références. Concernant l’approche elle-même, cette expérience semble montrer que le
                  formalisme adopté pour la représentation sémantique des énoncés est bien adapté à
                  la compréhension en contexte.
               </p><em>Version anglaise :</em><h4></h4>
               <p>LOGUS is a spoken language understanding system usable in a man-machine dialogue.
                  It is based on a logical approach where various formalisms are used, in order to achieve
                  a robust but generic and extensible system. Implementation of a context-sensitive
                  understanding is the main topic of this paper. Processing and tests were carried out
                  from a hotel reservation corpus which was recorded and annotated as part of the work
                  handled by the technolangue consortium’s MEDIA subgroup. This paper also describes
                  the various questions raised and conclusions drawn from such an experiment, as well
                  as the results achieved by the system for anaphora resolution. This experiment shows
                  that the formalism used in order to represent the meaning of the utterances is relevant
                  for anaphora resolution and in-context understanding.
               </p>
               <hr>
               <h4 id="taln-2007-poster-030">Évaluation des performances d’un modèle de langage stochastique pour la compréhension
                  de la parole arabe spontanée
               </h4>
               			Auteur : Anis Zouaghi -
               			Contact : Anis.Zouaghi@riadi.rnu.tn<br>
               			Auteur : Mounir Zrigui -
               			Contact : Mounir.Zrigui@fsm.rnu.tn<br>
               			Auteur : Mohamed Ben Ahmed -
               			Contact : Mohamed.Benahmed@riadi.rnu.tn<br><p>Les modèles de Markov cachés (HMM : Hidden Markov Models) (Baum et al., 1970), sont
                  très utilisés en reconnaissance de la parole et depuis quelques années en compréhension
                  de la parole spontanée latine telle que le français ou l’anglais. Dans cet article,
                  nous proposons d’utiliser et d’évaluer la performance de ce type de modèle pour l’interprétation
                  sémantique de la parole arabe spontanée. Les résultats obtenus sont satisfaisants,
                  nous avons atteint un taux d’erreur de l’ordre de 9,9% en employant un HMM à un seul
                  niveau, avec des probabilités tri_grammes de transitions.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The HMM (Hidden Markov Models) (Baum et al., 1970), are frequently used in speech
                  recognition and in the comprehension of foreign spontaneous speech such us the french
                  or the english. In this article, we propose using and evaluating the performance of
                  this model type for the semantic interpretation of the spontaneous arabic speech.
                  The obtained results are satisfying; we have achieved an error score equal to 9.9%,
                  by using HMM with trigrams probabilities transitions.
               </p>
               <hr>
               <h4 id="taln-2007-demo-001">Présentation du logiciel Antidote RX</h4>
               			Auteur : Éric Brunelle -
               			Contact : developpement@druide.com<br>
               			Auteur : Simon Charest -
               			Contact : developpement@druide.com<br><p>Antidote RX est la sixième édition d’Antidote, un logiciel d’aide à la rédaction développé
                  et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur
                  grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques.
                  Il fonctionne sous les systèmes d’exploitation Windows, Mac OS X et Linux.
               </p>
               <hr>
               <h4 id="taln-2007-demo-002">Logiciel Cordial</h4>
               			Auteur : Dominique Laurent<br>
               			Auteur : Sophie Nègre<br>
               			Auteur : Patrick Séguéla<br><p>Cordial est un correcteur efficace et discret enrichi d'un grand nombre de fonctions
                  d'aide à la rédaction et d'analyse de documents. Très riche avec ces multiples dictionnaires
                  et souvent pertinent dans ses propositions, Cordial est un compagnon précieux qui
                  vous permet d'assurer la qualité de vos écrits. La version 2007 de Cordial s'intègre
                  dans un vaste éventail de logiciels comme les traitements de texte (Word, Open Office,
                  Word Perfect...), clients de messagerie (Outlook, Notes, Thunderbird, webmails...)
                  ou navigateurs (Explorer, Mozilla).
               </p>
               <hr>
               <h4 id="taln-2007-demo-003">TransCheck : un vérificateur automatique de traductions</h4>
               			Auteur : Elliott Macklovitch -
               			Contact : macklovi@iro.umontreal.ca<br>
               			Auteur : Guy Lapalme -
               			Contact : lapalme@iro.umontreal.ca<br><p>Nous offrirons une démonstration de la dernière version de TransCheck, un vérificateur
                  automatique de traductions que le RALI est en train de développer. TransCheck prend
                  en entrée deux textes, un texte source dans une langue et sa traduction dans une autre,
                  les aligne au niveau de la phrase et ensuite vérifie les régions alignées pour s’assurer
                  de la présence de certains équivalents obligatoires (p. ex. la terminologie normalisée)
                  et de l’absence de certaines interdictions de traduction (p. ex. des interférences
                  de la langue source). Ainsi, TransCheck se veut un nouveau type d’outil d’aide à la
                  traduction qui pourra à réduire le fardeau de la révision et diminuer le coût du contrôle
                  de la qualité.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We will present a demonstration of the latest version of TransCheck, an automatic
                  translation checker that the RALI is currently developing. TransCheck takes as input
                  two texts, a source text in one language and its translation in another, aligns them
                  at the sentence level and then verifies the aligned regions to ensure that they contain
                  certain obligatory equivalents (e.g. standardized terminology) and do not contain
                  certain prohibited translations (e.g. source language interference). TransCheck is
                  thus intended to be a new type of tool for assisting translators which has the potential
                  to ease the burden of revision and diminish the costs of quality control.
               </p>
               <hr>
               <h4 id="taln-2007-demo-004">Le CNRTL, Centre National de Ressources Textuelles et Lexicales, un outil de mutualisation
                  de ressources linguistiques
               </h4>
               			Auteur : Jean-Marie Pierrel -
               			Contact : Jean-Marie.Pierrel@atilf.fr<br>
               			Auteur : Etienne Petitjean -
               			Contact : Etienne.Petitjean@atilf.fr<br><p>Créé en 2005 à l’initiative du Centre National de la Recherche Scientifique, le CNRTL
                  propose une plate-forme unifiée pour l’accès aux ressources et documents électroniques
                  destinés à l’étude et l’analyse de la langue française. Les services du CNRTL comprennent
                  le recensement, la documentation (métadonnées), la normalisation, l’archivage, l’enrichissement
                  et la diffusion des ressources. La pérennité du service et des données est garantie
                  par le soutien institutionnel du CNRS, l’adossement à un laboratoire de recherche
                  en linguistique et informatique du CNRS et de Nancy Université (ATILF – Analyse et
                  Traitement Informatique de la Langue Française), ainsi que l’intégration dans le réseau
                  européen CLARIN (common language resources and technology infrastructure european).
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="TALN'2008">2008 - 
               					TALN'2008 - Président(s) :  
               					Frédéric Bechet - 
               	Jean-Francois Bonastre - 
               	 à 
               					Avignon
            </h2>
            
            				Il y a 52 articles référencés. 
            
            				
            					Il n'y a aucune information sur les articles soumis/acceptés.
            		<br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#taln-2008-long-014">taln-2008-long-014</a></li>
               <li><a href="#taln-2008-long-027">taln-2008-long-027</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="taln-2008-long-001"></h4>
               			Auteur : Amanda Rocha-Chaves<br>
               			Auteur : Lucia-Helena Machado-Rino<br><p></p><em>Version anglaise :</em><h4>The Mitkov algorithm for anaphora resolution in Portuguese</h4>
               <p>This paper reports on the use of the Mitkov´s algorithm for pronoun resolution in
                  texts written in Brazilian Portuguese. Third person pronouns are the only ones focused
                  upon here, with noun phrases as antecedents. A system for anaphora resolution in Brazilian
                  Portuguese texts was built that embeds most of the Mitkov’s features. Some of his
                  resolution factors were directly incorporated into the system; others had to be slightly
                  modified for language adequacy. The resulting approach was intrinsically evaluated
                  on hand-annotated corpora. It was also compared to Lappin &amp; Leass’s algorithm for
                  pronoun resolution, also customized to Portuguese. Success rate was the evaluation
                  measure used in both experiments. The results of both evaluations are discussed here.
               </p>
               <hr>
               <h4 id="taln-2008-long-002">Réécriture et Détection d’Implication Textuelle</h4>
               			Auteur : Paul Bédaride -
               			Contact : Paul.Bedaride@loria.fr<br>
               			Auteur : Claire Gardent -
               			Contact : Claire.Gardent@loria.fr<br><p>Nous présentons un système de normalisation de la variation syntaxique qui permet
                  de mieux reconnaître la relation d’implication textuelle entre deux phrases. Le système
                  est évalué sur une suite de tests comportant 2 520 paires test et les résultats montrent
                  un gain en précision par rapport à un système de base variant entre 29.8 et 78.5 points
                  la complexité des cas considérés.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present a system for dealing with syntactic variation which significantly improves
                  the treatment of textual implication. The system is evaluated on a testsuite of 2
                  520 sentence pairs and the results show an improvment in precision over the baseline
                  which varies between 29.8 and 78.5 points depending on the complexity of the cases
                  being considered.
               </p>
               <hr>
               <h4 id="taln-2008-long-003">Représentation algébrique des expressions calendaires et vue calendaire d’un texte</h4>
               			Auteur : Delphine Battistelli -
               			Contact : delphine.battistelli@paris-sorbonne.fr<br>
               			Auteur : Javier Couto -
               			Contact : jcouto@fing.edu.uy<br>
               			Auteur : Jean-Luc Minel -
               			Contact : jminel@u-paris10.fr<br>
               			Auteur : Sylviane Schwer -
               			Contact : schwer@lipn.univ-paris13.fr<br><p>Cet article aborde l’étude des expressions temporelles qui font référence directement
                  à des unités de temps relatives aux divisions courantes des calendriers, que nous
                  qualifions d’expressions calendaires (EC). Nous proposons une modélisation de ces
                  expressions en définissant une algèbre d’opérateurs qui sont liés aux classes de marqueurs
                  linguistiques qui apparaissent dans les EC. A partir de notre modélisation, une vue
                  calendaire est construite dans la plate-forme de visualisation et navigation textuelle
                  NaviTexte, visant le support à la lecture de textes. Enfin, nous concluons sur les
                  perspectives offertes par le développement d’une première application de navigation
                  temporelle.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we address the study of temporal expressions that refer directly to
                  text units concerning common calendar divisions, that we name “calendar expressions”
                  (EC). We propose to model these expressions by defining an operator algebra, the operators
                  being related to different linguistics marker classes that occur in the EC. Based
                  on our model, a calendar view is built up into the text visualization and navigation
                  framework NaviTexte, aiming the support of text reading. Finally, we discuss the perspectives
                  offered by the development of a first temporal navigation application.
               </p>
               <hr>
               <h4 id="taln-2008-long-004">Annotation d’expressions temporelles et d’événements en français</h4>
               			Auteur : Gabriel Parent<br>
               			Auteur : Michel Gagnon<br>
               			Auteur : Philippe Muller<br><p>Dans cet article, nous proposons une méthode pour identifier, dans un texte en français,
                  l’ensemble des expressions adverbiales de localisation temporelle, ainsi que tous
                  les verbes, noms et adjectifs dénotant une éventualité (événement ou état). Cette
                  méthode, en plus d’identifier ces expressions, extrait certaines informations sémantiques
                  : la valeur de la localisation temporelle selon la norme TimeML et le type des éventualités.
                  Pour les expressions adverbiales de localisation temporelle, nous utilisons une cascade
                  d’automates, alors que pour l’identification des événements et états nous avons recours
                  à une analyse complète de la phrase. Nos résultats sont proches de travaux comparables
                  sur l’anglais, en l’absence d’évaluation quantitative similaire sur le français.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a method to extract expressions of temporal location in French texts, and
                  all verbs, nouns and adjectived that denote an event or a state. This method also
                  computes some semantic information : the value of the temporal location according
                  to the TimeML standard and the types of eventualities. For temporal location expression,
                  we use a cascade of transducers, wheras event identification is based on full syntactic
                  parsing. Our result are compared to similar work on English, as no other empirical
                  evaluation has been done on French before.
               </p>
               <hr>
               <h4 id="taln-2008-long-005">Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques</h4>
               			Auteur : Stéphane Huet -
               			Contact : stephane.huet@irisa.fr<br>
               			Auteur : Guillaume Gravier -
               			Contact : guillaume.gravier@irisa.fr<br>
               			Auteur : Pascale Sébillot -
               			Contact : pascale.sebillot@irisa.fr<br><p>Nous présentons une méthode de segmentation de journaux radiophoniques en sujets,
                  basée sur la prise en compte d’indices lexicaux, syntaxiques et acoustiques. Partant
                  d’un modèle statistique existant de segmentation thématique, exploitant la notion
                  de cohésion lexicale, nous étendons le formalisme pour y inclure des informations
                  d’ordre syntaxique et acoustique. Les résultats expérimentaux montrent que le seul
                  modèle de cohésion lexicale ne suffit pas pour le type de documents étudié en raison
                  de la taille variable des segments et de l’absence d’un lien direct entre segment
                  et thème. L’utilisation d’informations syntaxiques et acoustiques permet une amélioration
                  substantielle de la segmentation obtenue.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present a method for story segmentation of radio broadcast news, based on lexical,
                  syntactic and audio cues. Starting from an existing statistical topic segmentation
                  model which exploits the notion of lexical cohesion, we extend the formalism to include
                  syntactic and acoustic knwoledge sources. Experimental results show that the sole
                  use of lexical cohesion is not efficient for the type of documents under study because
                  of the variable size of the segments and the lack of direct relation between topics
                  and stories. The use of syntactics and acoustics enables a consequent improvement
                  of the quality of the segmentation.
               </p>
               <hr>
               <h4 id="taln-2008-long-006">Extraction automatique d'informations à partir de micro-textes non structurés</h4>
               			Auteur : Cédric Vidrequin -
               			Contact : cedric.vidrequin@univ-avignon.fr<br>
               			Auteur : Juan-Manuel Torres-Moreno -
               			Contact : juan-manuel.torres@univ-avignon.fr<br>
               			Auteur : Jean-Jacques Schneider -
               			Contact : jjschneider@semantia.com<br>
               			Auteur : Marc El-Bèze -
               			Contact : marc.elbeze@univ-avignon.fr<br><p>Nous présentons dans cet article une méthode d'extraction automatique d'informations
                  sur des textes de très petite taille, faiblement structurés. Nous travaillons sur
                  des textes dont la rédaction n'est pas normalisée, avec très peu de mots pour caractériser
                  chaque information. Les textes ne contiennent pas ou très peu de phrases. Il s'agit
                  le plus souvent de morceaux de phrases ou d'expressions composées de quelques mots.
                  Nous comparons plusieurs méthodes d'extraction, dont certaines sont entièrement automatiques.
                  D'autres utilisent en partie une connaissance du domaine que nous voulons réduite
                  au minimum, de façon à minimiser le travail manuel en amont. Enfin, nous présentons
                  nos résultats qui dépassent ce dont il est fait état dans la littérature, avec une
                  précision équivalente et un rappel supérieur.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this article, we present a method of automatic extraction of informations on very
                  small-sized and weakly structured texts. We work on texts whose drafting is not normalised,
                  with very few words to characterize each information. Texts does not contain sentences,
                  or only few. There are mostly about fragments of sentences or about expressions of
                  some words. We compare several extracting methods, some completely automatic and others
                  using an small domain knowledge. We want this knowledge to be minimalistic to reduce
                  as much as possible any manual work. Then, we present our results, witch are better
                  than those published in the literature, with an equivalent precision and a greater
                  recall.
               </p>
               <hr>
               <h4 id="taln-2008-long-007">Quelles combinaisons de scores et de critères numériques pour un système de Questions/Réponses
                  ?
               </h4>
               			Auteur : Laurent Gillard -
               			Contact : Laurent.Gillard@univ-avignon.fr<br>
               			Auteur : Patrice Bellot -
               			Contact : Patrice.Bellot@univ-avignon.fr<br>
               			Auteur : Marc El-Bèze -
               			Contact : marc.elbeze@univ-avignon.fr<br><p>Dans cet article, nous présentons une discussion sur la combinaison de différents
                  scores et critères numériques pour la sélection finale d’une réponse dans la partie
                  en charge des questions factuelles du système de Questions/Réponses développé au LIA.
                  Ces scores et critères numériques sont dérivés de ceux obtenus en sortie de deux composants
                  cruciaux pour notre système : celui de sélection des passages susceptibles de contenir
                  une réponse et celui d’extraction et de sélection d’une réponse. Ils sont étudiés
                  au regard de leur expressivité. Des comparaisons sont faites avec des approches de
                  sélection de passages mettant en oeuvre des scores conventionnels en recherche d’information.
                  Parallèlement, l’influence de la taille des contextes (en nombre de phrases) est évaluée.
                  Cela permet de mettre en évidence que le choix de passages constitués de trois phrases
                  autour d’une réponse candidate, avec une sélection des réponses basée sur une combinaison
                  entre un score de passage de type Lucene ou Cosine et d’un score de compacité apparaît
                  comme un compromis intéressant.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article discusses combinations of scores for selecting the best answer in a factual
                  question answering system. Two major components of our QA system: (i) relevant passage
                  selection, and (ii) answer extraction, produce a variety of scores. Here we study
                  the expressivity of these scores, comparing our passage density score (i) to more
                  conventional ranking techniques in information retrieval. In addition, we study varying
                  the length (in number of sentences) of context retained in the relevant passages.
                  We find that a three sentences window, and a mixing of Lucene or Cosine ranking with
                  our compactness score (ii) provides the best results.
               </p>
               <hr>
               <h4 id="taln-2008-long-008">Contrôle rhétorique de la génération des connecteurs concessifs en dialogue homme-machine</h4>
               			Auteur : Vladimir Popescu -
               			Contact : vladimir.popescu@imag.fr<br>
               			Auteur : Jean Caelen -
               			Contact : jean.caelen@imag.fr<br><p>Les connecteurs discursifs ont on rôle important dans l’interprétation des discours
                  (dialogiques ou pas), donc lorsqu’il s’agit de produire des énoncés, le choix des
                  mots qui relient les énoncés (par exemple, en dialogue oral) s’avère essentiel pour
                  assurer la compréhension des visées illocutoires des locuteurs. En linguistique computationnelle,
                  le problème a été abordé surtout au niveau de l’interprétation des discours monologiques,
                  tandis que pour le dialogue, les recherches se sont limitées en général à établir
                  une correspondance quasiment biunivoque entre relations rhétoriques et connecteurs.
                  Dans ce papier nous proposons un mécanisme pour guider la génération des connecteurs
                  concessifs en dialogue, à la fois du point de vue discursif et sémantique ; chaque
                  connecteur considéré sera contraint par un ensemble de conditions qui prennent en
                  compte la cohérence du discours et la pertinence sémantique de chaque mot concerné.
                  Les contraintes discursives, exprimées dans un formalisme dérivé de la SDRT (« Segmented
                  Discourse Representation Theory ») seront plongées dans des contraintes sémantiques
                  sur les connecteurs, proposées par l’école genevoise (Moeschler), pour enfin évaluer
                  la cohérence du discours résultant de l’emploi de ces connecteurs.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Cue words play an important part in discourse interpretation (whether dialogues are
                  concerned or not), hence when utterances have to be produced, the choice of the words
                  that connect these utterances (for example, in spoken dialogue) is essential for ensuring
                  the comprehension of the illocutionary goals of the speakers. In computational linguistics,
                  the issue has been mitigated particularly in the context of monologue discourse interpretation,
                  whereas for dialogues, research is usually limited to establishing an almost one-to-one
                  mapping between rhetorical relations and cue words. In this paper we propose a mechanism
                  for guiding concessive connectors in dialogue, at the same time from a discourse and
                  from a semantic point of view; each considered connector will be constrained via a
                  set of conditions that take into account discourse coherence and the semantic relevance
                  of each word concerned. Discourse constraints, expressed in a formalism derived from
                  SDRT (“Segmented Discourse Representation Theory”) will be mapped to semantic constraints
                  on the connectors ; these semantic constraints are proposed by the Geneve linguistics
                  school (Moeschler). Finally, the coherence of the discourse resulted from the use
                  of these connectors will be assessed.
               </p>
               <hr>
               <h4 id="taln-2008-long-009">Modélisation du principe d’ancrage pour la robustesse des systèmes de dialogue homme-machine
                  finalisés
               </h4>
               			Auteur : Alexandre Denis -
               			Contact : alexandre.denis@loria.fr<br>
               			Auteur : Matthieu Quignard -
               			Contact : matthieu.quignard@loria.fr<br><p>Cet article présente une modélisation du principe d’ancrage (grounding) pour la robustesse
                  des systèmes de dialogue finalisés. Ce principe, décrit dans (Clark &amp; Schaefer, 1989),
                  suggère que les participants à un dialogue fournissent des preuves de compréhension
                  afin d’atteindre la compréhension mutuelle. Nous explicitons une définition computationnelle
                  du principe d’ancrage fondée sur des jugements de compréhension qui, contrairement
                  à d’autres modèles, conserve une motivation pour l’expression de la compréhension.
                  Nous déroulons enfin le processus d’ancrage sur un exemple tiré de l’implémentation
                  du modèle.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a grounding model for robustness in dialogue systems. The grounding
                  process (Clark &amp; Schaefer, 1989) suggests that dialogue participants provide evidence
                  of understanding in order to reach mutual understanding. We propose here a computational
                  definition of the grounding criterion based on understanding judgments that keeps
                  a motivation for providing evidence of understanding, as opposed to some existing
                  models. Eventually, we detail the grounding process on a dialogue generated by the
                  actual implementation.
               </p>
               <hr>
               <h4 id="taln-2008-long-010">Enertex : un système basé sur l’énergie textuelle</h4>
               			Auteur : Silvia Fernández -
               			Contact : silvia.fernandez@univ-avignon.fr<br>
               			Auteur : Eric Sanjuan -
               			Contact : eric.sanjuan@univ-avignon.fr<br>
               			Auteur : Juan-Manuel Torres-Moreno -
               			Contact : juan-manuel.torres@univ-avignon.fr<br><p>Dans cet article, nous présentons des applications du système Enertex au Traitement
                  Automatique de la Langue Naturelle. Enertex est basé sur l’énergie textuelle, une
                  approche par réseaux de neurones inspirée de la physique statistique des systèmes
                  magnétiques. Nous avons appliqué cette approche aux problèmes du résumé automatique
                  multi-documents et de la détection de frontières thématiques. Les résultats, en trois
                  langues : anglais, espagnol et français, sont très encourageants.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we present Enertex applications to study fundamental problems in Natural
                  Language Processing. Enertex is based on textual energy, a neural networks approach,
                  inspired by statistical physics of magnetic systems.We obtained good results on the
                  application of this method to automatic multi-document summarization and thematic
                  border detection in three languages : English, Spanish and French.
               </p>
               <hr>
               <h4 id="taln-2008-long-011">Intégration d’une étape de pré-filtrage et d’une fonction multiobjectif en vue d’améliorer
                  le système ExtraNews de résumé de documents multiples
               </h4>
               			Auteur : Fatma Kallel Jaoua -
               			Contact : fatma_fseg@yahoo.fr<br>
               			Auteur : Lamia Hadrich Belguith -
               			Contact : l.belguith@fsegs.rnu.tn<br>
               			Auteur : Maher Jaoua -
               			Contact : maher.jaoua@fsegs.rnu.tn<br>
               			Auteur : Abdelmajid Ben Hamadou -
               			Contact : abdelmajid.benhamadou@fsegs.rnu.tn<br><p>Dans cet article, nous présentons les améliorations que nous avons apportées au système
                  ExtraNews de résumé automatique de documents multiples. Ce système se base sur l’utilisation
                  d’un algorithme génétique qui permet de combiner les phrases des documents sources
                  pour former les extraits, qui seront croisés et mutés pour générer de nouveaux extraits.
                  La multiplicité des critères de sélection d’extraits nous a inspiré une première amélioration
                  qui consiste à utiliser une technique d’optimisation multi-objectif en vue d’évaluer
                  ces extraits. La deuxième amélioration consiste à intégrer une étape de pré-filtrage
                  de phrases qui a pour objectif la réduction du nombre des phrases des textes sources
                  en entrée. Une évaluation des améliorations apportées à notre système est réalisée
                  sur les corpus de DUC’04 et DUC’07.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present the improvements that we brought to the ExtraNews system
                  dedicated for automatic summarisation of multiple documents. This system is based
                  on the use of a genetic algorithm that combines sentences of the source documents
                  to form the extracts. These extracts are crossed and transferred to generate new ones.
                  The multiplicity of the extract selection criteria inspired us the first improvement
                  that consists in the use of a multi-objectif optimization technique to evaluate these
                  extracts. The second improvement consists of the integration of a sentence pre-filtering
                  step which is based on the notion of dominance between sentences. Our objective is
                  to reduce the sentence number of the source texts. An evaluation of the proposed improvements
                  to our system is realized on DUC' 04 and DUC' 07 corpus.
               </p>
               <hr>
               <h4 id="taln-2008-long-012">Recherche locale pour la traduction statistique à base de segments</h4>
               			Auteur : Philippe Langlais -
               			Contact : felipe@iro.umontreal.ca<br>
               			Auteur : Alexandre Patry -
               			Contact : patryale@iro.umontreal.ca<br>
               			Auteur : Fabrizio Gotti -
               			Contact : gottif@iro.umontreal.ca<br><p>Dans cette étude, nous nous intéressons à des algorithmes de recherche locale pour
                  la traduction statistique à base de segments (phrase-based machine translation). Les
                  algorithmes que nous étudions s’appuient sur une formulation complète d’un état dans
                  l’espace de recherche contrairement aux décodeurs couramment utilisés qui explorent
                  l’espace des préfixes des traductions possibles. Nous montrons que la recherche locale
                  seule, permet de produire des traductions proches en qualité de celles fournies par
                  les décodeurs usuels, en un temps nettement inférieur et à un coût mémoire constant.
                  Nous montrons également sur plusieurs directions de traduction qu’elle permet d’améliorer
                  de manière significative les traductions produites par le système à l’état de l’art
                  Pharaoh (Koehn, 2004).
               </p><em>Version anglaise :</em><h4></h4>
               <p>Most phrase-based statistical machine translation decoders rely on a dynamicprogramming
                  technique for maximizing a combination of models, including one or several language
                  models and translation tables. One implication of this choice is the design of a scoring
                  function that can be computed incrementally on partial translations, a restriction
                  a search engine using a complete-state formulation does not have. In this paper, we
                  present experiments we conducted with a simple, yet effective greedy search engine.We
                  report significant improvements in translation quality over a state-of-the-art beam-search
                  decoder, for some configurations.
               </p>
               <hr>
               <h4 id="taln-2008-long-013">Transcrire les SMS comme on reconnaît la parole</h4>
               			Auteur : Catherine Kobus<br>
               			Auteur : François Yvon<br>
               			Auteur : Géraldine Damnati<br><p>Cet article présente une architecture inspirée des systèmes de reconnaissance vocale
                  pour effectuer une normalisation orthographique de messages en « langage SMS ». Nous
                  décrivons notre système de base, ainsi que diverses évolutions de ce système, qui
                  permettent d’améliorer sensiblement la qualité des normalisations produites.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a system aiming at normalizing the orthography of SMS messages,
                  using techniques that are commonly used in automatic speech recognition devices. We
                  describe a baseline system and various evolutions, which are shown to improve significantly
                  the quality of the output normalizations.
               </p>
               <hr>
               <h4 id="taln-2008-long-014">Convertir des grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres
                  (TT-MCTAG) en grammaires à concaténation d’intervalles (RCG)
               </h4>
               			Auteur : Laura Kallmeyer -
               			Contact : lk@sfs.uni-tuebingen.de<br>
               			Auteur : Yannick Parmentier -
               			Contact : parmenti@sfs.uni-tuebingen.de<br><p>Cet article étudie la relation entre les grammaires d’arbres adjoints à composantes
                  multiples avec tuples d’arbres (TT-MCTAG), un formalisme utilisé en linguistique informatique,
                  et les grammaires à concaténation d’intervalles (RCG). Les RCGs sont connues pour
                  décrire exactement la classe PTIME, il a en outre été démontré que les RCGs « simples
                  » sont même équivalentes aux systèmes de réécriture hors-contextes linéaires (LCFRS),
                  en d’autres termes, elles sont légèrement sensibles au contexte. TT-MCTAG a été proposé
                  pour modéliser les langages à ordre des mots libre. En général ces langages sont NP-complets.
                  Dans cet article, nous définissons une contrainte additionnelle sur les dérivations
                  autorisées par le formalisme TT-MCTAG. Nous montrons ensuite comment cette forme restreinte
                  de TT-MCTAG peut être convertie en une RCG simple équivalente. Le résultat est intéressant
                  pour des raisons théoriques (puisqu’il montre que la forme restreinte de TT-MCTAG
                  est légèrement sensible au contexte), mais également pour des raisons pratiques (la
                  transformation proposée ici a été utilisée pour implanter un analyseur pour TT-MCTAG).
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper investigates the relation between TT-MCTAG, a formalism used in computational
                  linguistics, and RCG. RCGs are known to describe exactly the class PTIME ; « simple
                  » RCG even have been shown to be equivalent to linear context-free rewriting systems,
                  i.e., to be mildly context-sensitive. TT-MCTAG has been proposed to model free word
                  order languages. In general, it is NP-complete. In this paper, we will put an additional
                  limitation on the derivations licensed in TT-MCTAG. We show that TT-MCTAG with this
                  additional limitation can be transformed into equivalent simple RCGs. This result
                  is interesting for theoretical reasons (since it shows that TT-MCTAG in this limited
                  form is mildly context-sensitive) and also for practical reasons (the proposed transformation
                  has been used for implementing a parser for TT-MCTAG).
               </p>
               <hr>
               <h4 id="taln-2008-long-015">Factorisation des contraintes syntaxiques dans un analyseur de dépendance</h4>
               			Auteur : Piet Mertens -
               			Contact : piet.mertens@arts.kuleuven.be<br><p>Cet article décrit un analyseur syntaxique pour grammaires de dépendance lexicalisées.
                  Le formalisme syntaxique se caractérise par une factorisation des contraintes syntaxiques
                  qui se manifeste dans la séparation entre dépendance et ordre linéaire, la spécification
                  fonctionnelle (plutôt que syntagmatique) des dépendants, la distinction entre dépendants
                  valenciels (la sous-catégorisation) et non valenciels (les circonstants) et la saturation
                  progressive des arbres. Ceci résulte en une formulation concise de la grammaire à
                  un niveau très abstrait et l’élimination de la reduplication redondante des informations
                  due aux réalisations alternatives des dépendants ou à leur ordre. Les arbres élémentaires
                  (obtenus à partir des formes dans l’entrée) et dérivés sont combinés entre eux par
                  adjonction d’un arbre dépendant saturé à un arbre régissant, moyennant l’unification
                  des noeuds et des relations. La dérivation est réalisée grâce à un analyseur chart
                  bi-directionnel.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We describe a parser for lexicalized dependency grammar. The formalism is characterized
                  by a factorization of the syntactic constraints, based on the separation between dependency
                  and word order, the functional (rather than phrasal) specification of dependents,
                  the distinction between valency and non valency dependents, and the incremental saturation
                  of the trees. These features enable a concise formulation of the grammar at a very
                  abstract level and eliminate syntactic information redundancy due to alternative forms
                  of dependents and word order. Each word form produces one or more elementary dependency
                  trees. Trees, both elementary and derived, are combined by adjoining a saturated dependent
                  to a governing tree, after unification of shared nodes and relations. This is achieved
                  using a bi-directional chart parser.
               </p>
               <hr>
               <h4 id="taln-2008-long-016">Grammaires factorisées pour des dialectes apparentés</h4>
               			Auteur : Pascal Vaillant -
               			Contact : pascal.vaillant@guyane.univ-ag.fr<br><p>Pour la formalisation du lexique et de la grammaire de dialectes étroitement apparentés,
                  il peut se révéler utile de factoriser une partie du travail de modélisation. Les
                  soussystèmes linguistiques isomorphes dans les différents dialectes peuvent alors
                  faire l’objet d’une description commune, les différences étant spécifiées par ailleurs.
                  Cette démarche aboutit à un modèle de grammaire à couches : le noyau est commun à
                  la famille de dialectes, et une couche superficielle détermine les caractéristiques
                  de chacun. Nous appliquons ce procédé à la famille des langues créoles à base lexicale
                  française de l’aire américano-caraïbe.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The task of writing formal lexicons and grammars for closely related dialects can
                  benefit from factoring part of the modelling. Isomorphic linguistic subsystems from
                  the different dialectsmay have a common description, while the differences are specified
                  aside. This process leads to a layered grammar model: a kernel common to the whole
                  family of dialects, and a superficial skin specifying the particular properties of
                  each one of them. We apply this principle to the family of French-lexifier creole
                  languages of the American-Caribbean area.
               </p>
               <hr>
               <h4 id="taln-2008-long-017">Expériences d’analyse syntaxique statistique du français</h4>
               			Auteur : Benoît Crabbé -
               			Contact : bcrabbe@linguist.jussieu.fr<br>
               			Auteur : Marie Candito -
               			Contact : candito@linguist.jussieu.fr<br><p>Nous montrons qu’il est possible d’obtenir une analyse syntaxique statistique satisfaisante
                  pour le français sur du corpus journalistique, à partir des données issues du French
                  Treebank du laboratoire LLF, à l’aide d’un algorithme d’analyse non lexicalisé.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We show that we can acquire satisfactory parsing results for French from data induced
                  from the French Treebank using an unlexicalised parsing algorithm.
               </p>
               <hr>
               <h4 id="taln-2008-long-018">Construction d’un wordnet libre du français à partir de ressources multilingues</h4>
               			Auteur : Benoît Sagot -
               			Contact : benoit.sagot@inria.fr<br>
               			Auteur : Darja Fišer -
               			Contact : darja.fiser@guest.arnes.si<br><p>Cet article décrit la construction d’un Wordnet Libre du Français (WOLF) à partir
                  du Princeton WordNet et de diverses ressources multilingues. Les lexèmes polysémiques
                  ont été traités au moyen d’une approche reposant sur l’alignement en mots d’un corpus
                  parallèle en cinq langues. Le lexique multilingue extrait a été désambiguïsé sémantiquement
                  à l’aide des wordnets des langues concernées. Par ailleurs, une approche bilingue
                  a été suffisante pour construire de nouvelles entrées à partir des lexèmes monosémiques.
                  Nous avons pour cela extrait des lexiques bilingues à partir deWikipédia et de thésaurus.
                  Le wordnet obtenu a été évalué par rapport au wordnet français issu du projet EuroWordNet.
                  Les résultats sont encourageants, et des applications sont d’ores et déjà envisagées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes the construction of a freely-available wordnet for French (WOLF)
                  based on Princeton WordNet by using various multilingual resources. Polysemous words
                  were dealt with an approach in which a parallel corpus for five languages was wordaligned
                  and the extracted multilingual lexicon was disambiguated with the existing wordnets
                  for these languages. On the other hand, a bilingual approach sufficed to acquire equivalents
                  for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri.
                  The merged wordnet was evaluated against the French WordNet. The results are promising,
                  and applications are already intended.
               </p>
               <hr>
               <h4 id="taln-2008-long-019">Détermination des sens d’usage dans un réseau lexical construit à l’aide d’un jeu
                  en ligne
               </h4>
               			Auteur : Mathieu Lafourcade -
               			Contact : lafourcade@lirmm.fr<br>
               			Auteur : Alain Joubert -
               			Contact : joubert@lirmm.fr<br><p>Les informations lexicales, indispensables pour les tâches réalisées en TALN, sont
                  difficiles à collecter. En effet, effectuée manuellement, cette tâche nécessite la
                  compétence d’experts et la durée nécessaire peut être prohibitive, alors que réalisée
                  automatiquement, les résultats peuvent être biaisés par les corpus de textes retenus.
                  L’approche présentée ici consiste à faire participer un grand nombre de personnes
                  à un projet contributif en leur proposant une application ludique accessible sur le
                  web. A partir d’une base de termes préexistante, ce sont ainsi les joueurs qui vont
                  construire le réseau lexical, en fournissant des associations qui ne sont validées
                  que si elles sont proposées par au moins une paire d’utilisateurs. De plus, ces relations
                  typées sont pondérées en fonction du nombre de paires d’utilisateurs qui les ont proposées.
                  Enfin, nous abordons la question de la détermination des différents sens d’usage d’un
                  terme, en analysant les relations entre ce terme et ses voisins immédiats dans le
                  réseau lexical, avant de présenter brièvement la réalisation et les premiers résultats
                  obtenus.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Lexical information is indispensable for the tasks realized in NLP, but collecting
                  lexical information is a difficult work. Indeed, when done manually, it requires the
                  competence of experts and the duration can be prohibitive. When done automatically,
                  the results can be biased by the corpus of texts. The approach we present here consists
                  in having people take part in a collective project by offering them a playful application
                  accessible on the web. From an already existing base of terms, the players themselves
                  thus build the lexical network, by supplying associations which are validated only
                  if they are suggested by a pair of users. Furthermore, these typed relations are weighted
                  according to the number of pairs of users who provide them. Finally, we approach the
                  question of the word usage determination for a term, by searching relations between
                  this term and its neighbours in the network, before briefly presenting the realization
                  and the first obtained results.
               </p>
               <hr>
               <h4 id="taln-2008-long-020">Modélisation normalisée LMF des dictionnaires électroniques éditoriaux de l’arabe</h4>
               			Auteur : Feten Baccar -
               			Contact : baccarf@yahoo.fr<br>
               			Auteur : Aïda Khemakhem -
               			Contact : aida_khemakhem@yahoo.fr<br>
               			Auteur : Bilel Gargouri -
               			Contact : bilel.gargouri@fsegs.rnu.tn<br>
               			Auteur : Kais Haddar -
               			Contact : kais.haddar@fss.rnu.tn<br>
               			Auteur : Abdelmajid Ben Hamadou -
               			Contact : abdelmajid.benhamadou@isimsf.rnu.tn<br><p>Le présent papier s’intéresse à l’élaboration des dictionnaires électroniques arabes
                  à usage éditorial. Il propose un modèle unifié et normalisé de ces dictionnaires en
                  se référant à la future norme LMF (Lexical Markup Framework) ISO 24613. Ce modèle
                  permet de construire des dictionnaires extensibles, sur lesquels on peut réaliser,
                  grâce à une structuration fine et standard, des fonctions de consultation génériques
                  adaptées aux besoins des utilisateurs. La mise en oeuvre du modèle proposé est testée
                  sur des dictionnaires existants de la langue arabe en utilisant, pour la consultation,
                  le système ADIQTO (Arabic DIctionary Query TOols) que nous avons développé pour l’interrogation
                  générique des dictionnaires normalisés de l’arabe.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper is interested in the development of the Arabic electronic dictionaries
                  of human use. It proposes a unified and standardized model for these dictionaries
                  according to the future standard LMF (Lexical Markup Framework) ISO 24613. Thanks
                  to its subtle and standardized structure, this model allows the development of extendable
                  dictionaries on which generic interrogation functions adapted to the user’s needs
                  can be implemented. This model has already been carried out on some existing Arabic
                  dictionaries using the ADIQTQ (Arabic DIctionary Query Tool) system, which we developed
                  for the generic interrogation of standardized dictionaries of Arabic.
               </p>
               <hr>
               <h4 id="taln-2008-long-021">La polysémie régulière dans WordNet</h4>
               			Auteur : Lucie Barque -
               			Contact : lucie.barque@linguist.jussieu.fr<br>
               			Auteur : François-Régis Chaumartin -
               			Contact : frc@proxem.com<br><p>Cette étude propose une analyse et une modélisation des relations de polysémie dans
                  le lexique électronique anglais WordNet. Elle exploite pour cela la hiérarchie des
                  concepts (représentés par des synsets), et la définition associée à chacun de ces
                  concepts. Le résultat est constitué d'un ensemble de règles qui nous ont permis d'identifier
                  d’une façon largement automatisée, avec une précision voisine de 91%, plus de 2100
                  paires de synsets liés par une relation de polysémie régulière. Notre méthode permet
                  aussi une désambiguïsation lexicale partielle des mots de la définition associée à
                  ces synsets.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents an analysis and modeling of polysemy in the WordNet English lexical
                  database. It exploits the concepts hierarchy (constituted by synsets), and the gloss
                  defining each of these concepts. The result consists of rules set which enabled us
                  to identify in a largely automated way, with a precision close to 91%, more than 2100
                  synsets pairs, connected by a regular polysemy relation. Our method also allows a
                  partial word sense disambiguation of the definition associated with these synsets.
               </p>
               <hr>
               <h4 id="taln-2008-long-022">Une alternative aux modèles de traduction statistique d’IBM: Les triggers inter-langues</h4>
               			Auteur : Caroline Lavecchia<br>
               			Auteur : Kamel Smaïli<br>
               			Auteur : David Langlois<br><p>Dans cet article, nous présentons une nouvelle approche pour la traduction automatique
                  fondée sur les triggers inter-langues. Dans un premier temps, nous expliquons le concept
                  de triggers inter-langues ainsi que la façon dont ils sont déterminés. Nous présentons
                  ensuite les différentes expérimentations qui ont été menées à partir de ces triggers
                  afin de les intégrer au mieux dans un processus complet de traduction automatique.
                  Pour cela, nous construisons à partir des triggers inter-langues des tables de traduction
                  suivant différentes méthodes. Nous comparons par la suite notre système de traduction
                  fondé sur les triggers interlangues à un système état de l’art reposant sur le modèle
                  3 d’IBM (Brown &amp; al., 1993). Les tests menés ont montré que les traductions automatiques
                  générées par notre système améliorent le score BLEU (Papineni &amp; al., 2001) de 2, 4%
                  comparé à celles produites par le système état de l’art.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present an original approach for machine translation based on inter-lingual
                  triggers. First, we describe the idea of inter-lingual triggers and how to determine
                  them. Then, we present the way to make good use of them in order to integrate them
                  in an entire translation process. We used inter-lingual triggers to estimate different
                  translation tables. Then we compared our translation system based on triggers to a
                  state-of-the-art system based on IBM model 3 (Brown &amp; al., 1993). The experiments
                  showed that automatic translations generated by our system outperform model 3 of IBM
                  by 2.4% in terms of BLEU (Papineni &amp; al., 2001).
               </p>
               <hr>
               <h4 id="taln-2008-long-023">Génération de reformulations locales par pivot pour l’aide à la révision</h4>
               			Auteur : Aurélien Max -
               			Contact : aurelien.max@limsi.fr<br><p>Cet article présente une approche pour obtenir des paraphrases pour de courts segments
                  de texte qui peuvent aider un rédacteur à reformuler localement des textes. La ressource
                  principale utilisée est une table d’alignements bilingues de segments d’un système
                  de traduction automatique statistique. Un segment marqué par le rédacteur est tout
                  d’abord traduit dans une langue pivot avant d’être traduit à nouveau dans la langue
                  d’origine, ce qui est permis par la nature même de la ressource bilingue utilisée
                  sans avoir recours à un processus de traduction complet. Le cadre proposé permet l’intégration
                  et la combinaison de différents modèles d’estimation de la qualité des paraphrases.
                  Des modèles linguistiques tentant de prendre en compte des caractéristiques des paraphrases
                  de courts segments de textes sont proposés, et une évaluation est décrite et ses résultats
                  analysés. Les domaines d’application possibles incluent, outre l’aide à la reformulation,
                  le résumé et la réécriture des textes pour répondre à des conventions ou à des préférences
                  stylistiques. L’approche est critiquée et des perspectives d’amélioration sont proposées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this article, we present a method to obtain paraphrases for short text spans that
                  can be useful to help a writer in reformulating text. The main resource used is a
                  bilingual phrase table containing aligned phrases, a common resource in statistical
                  machine translation. The writer can mark a segment for paraphrasing, and this segment
                  is first translated into a pivot language before being back-translated into the original
                  language, which is possible without performing a full translation of the input. Our
                  proposed framework allows integrating and combining various models for estimating
                  paraphrase quality. We propose linguistic models which permits to conduct empirical
                  experiments about the characteristics of paraphrases for short text spans. Application
                  domains include, in addition to paraphrasing aids, summarization and rephrasing of
                  text for conforming to conventional or stylistic guidelines. We finally discuss the
                  limitations of our work and describe possible ways of improvement.
               </p>
               <hr>
               <h4 id="taln-2008-long-024">Les architectures linguistiques et computationnelles en traduction automatique sont
                  indépendantes
               </h4>
               			Auteur : Christian Boitet -
               			Contact : Christian.Boitet@imag.fr<br><p>Contrairement à une idée répandue, les architectures linguistiques et computationnelles
                  des systèmes de traduction automatique sont indépendantes. Les premières concernent
                  le choix des représentations intermédiaires, les secondes le type d'algorithme, de
                  programmation et de ressources utilisés. Il est ainsi possible d'utiliser des méthodes
                  de calcul « expertes » ou « empiriques » pour construire diverses phases ou modules
                  de systèmes d'architectures linguistiques variées. Nous terminons en donnant quelques
                  éléments pour le choix de ces architectures en fonction des situations traductionnelles
                  et des ressources disponibles, en termes de dictionnaires, de corpus, et de compétences
                  humaines.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Contrary to a wide-spread idea, the linguistic and computational architectures of
                  MT systems are independent. The former concern the choice of the intermediate representations,
                  the latter the type of algorithm, programming, and resources used. It is thus possible
                  to use "expert" or "empirical" computational methods to build various phases or modules
                  of systems having various linguistic architectures. We finish by giving some elements
                  for choosing these architectures depending on the translational situations and the
                  available resources, in terms of dictionaries, corpora, and human competences.
               </p>
               <hr>
               <h4 id="taln-2008-long-025">Vérification sémantique pour l’annotation d’entités nommées</h4>
               			Auteur : Caroline Brun -
               			Contact : Caroline.Brun@xrce.xerox.com<br>
               			Auteur : Caroline Hagège -
               			Contact : Caroline.Hagege@xrce.xerox.com<br><p>Dans cet article, nous proposons une méthode visant à corriger et à associer dynamiquement
                  de nouveaux types sémantiques dans le cadre de systèmes de détection automatique d’entités
                  nommées (EN). Après la détection des entités nommées et aussi de manière plus générale
                  des noms propres dans les textes, une vérification de compatibilité de types sémantiques
                  est effectuée non seulement pour confirmer ou corriger les résultats obtenus par le
                  système de détection d’EN, mais aussi pour associer de nouveaux types non couverts
                  par le système de détection d’EN. Cette vérification est effectuée en utilisant l’information
                  syntaxique associée aux EN par un système d’analyse syntaxique robuste et en confrontant
                  ces résultats avec la ressource sémantique WordNet. Les résultats du système de détection
                  d’EN sont alors considérablement enrichis, ainsi que les étiquettes sémantiques associées
                  aux EN, ce qui est particulièrement utile pour l’adaptation de systèmes de détection
                  d’EN à de nouveaux domaines.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we propose a new method that enables to correct and to associate new
                  semantic types in the context of Named Entity (NE) Recognition Systems. After named
                  entities (and more generally proper nouns) have been detected in texts, a semantic
                  compatibility checking is performed. This checking can not only confirm or correct
                  previous results of the NER system but also associate new NE types that have not been
                  previously foreseen. This checking is performed using information associated to the
                  NE by a robust syntactic analyzer and confronting this information to WordNet. After
                  this checking is performed, final results of the NER system are better and new NE
                  semantic tags are created. This second point is particularly useful when adapting
                  existing NER systems to new domains.
               </p>
               <hr>
               <h4 id="taln-2008-long-026">Exploitation de treillis de Galois en désambiguïsation non supervisée d’entités nommées</h4>
               			Auteur : Thomas Girault -
               			Contact : thomas.girault@orange-ftgroup.com<br><p>Nous présentons une méthode non supervisée de désambiguïsation d’entités nommées,
                  basée sur l’exploitation des treillis de Galois. Nous réalisons une analyse de concepts
                  formels à partir de relations entre des entités nommées et leurs contextes syntaxiques
                  extraits d’un corpus d’apprentissage. Le treillis de Galois résultant fournit des
                  concepts qui sont utilisés comme des étiquettes pour annoter les entités nommées et
                  leurs contextes dans un corpus de test. Une évaluation en cascade montre qu’un système
                  d’apprentissage supervisé améliore la classification des entités nommées lorsqu’il
                  s’appuie sur l’annotation réalisée par notre système de désambiguïsation non supervisée.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present an unsupervised method for named entities disambiguation, based on concept
                  lattice mining.We perform a formal concept analysis from relations between named entities
                  and their syntactic contexts observed in a training corpora. The resulting lattice
                  produces concepts which are considered as labels for named entities and context annotation.
                  Our approach is validated through a cascade evaluation which shows that supervised
                  named entity classification is improved by using the annotation produced by our unsupervised
                  disambiguation system.
               </p>
               <hr>
               <h4 id="taln-2008-long-027">Résolution de Métonymie des Entités Nommées : proposition d’une méthode hybride</h4>
               			Auteur : Caroline Brun -
               			Contact : Caroline.Brun@xrce.xerox.com<br>
               			Auteur : Maud Ehrmann -
               			Contact : Maud.Ehrmann@xrce.xerox.com<br>
               			Auteur : Guillaume Jacquet -
               			Contact : Guillaume.Jacquet@xrce.xerox.com<br><p>Dans cet article, nous décrivons la méthode que nous avons développée pour la résolution
                  de métonymie des entités nommées dans le cadre de la compétition SemEval 2007. Afin
                  de résoudre les métonymies sur les noms de lieux et noms d’organisation, tel que requis
                  pour cette tâche, nous avons mis au point un système hybride basé sur l’utilisation
                  d’un analyseur syntaxique robuste combiné avec une méthode d’analyse distributionnelle.
                  Nous décrivons cette méthode ainsi que les résultats obtenus par le système dans le
                  cadre de la compétition SemEval 2007.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we describe the method we develop in order to solve Named entity metonymy
                  in the framework of the SemEval 2007 competition. In order to perform Named Entity
                  metonymy resolution on location names and company names, as required for this task,
                  we developed a hybrid system based on the use of a robust parser that extracts deep
                  syntactic relations combined with a non supervised distributional approach, also relying
                  on the relations extracted by the parser.We describe this methodology as well as the
                  results obtained at SemEval 2007.
               </p>
               <hr>
               <h4 id="taln-2008-long-028">Etude de la corrélation entre morphosyntaxe et sémantique dans une perspective d’étiquetage
                  automatique de textes médicaux arabes
               </h4>
               			Auteur : Tatiana El-Khoury -
               			Contact : tatiana.elkhoury@imag.fr<br><p>Cet article se propose d’étudier les relations sémantiques reliant base et expansion
                  au sein des termes médicaux arabes de type « N+N », particulièrement ceux dont la
                  base est un déverbal. En étudiant les relations sémantiques établies par une base
                  déverbale, ce travail tente d’attirer l’attention sur l’interpénétration du sémantique
                  et du morphosyntaxique ; il montre que, dans une large mesure, la structure morphosyntaxique
                  de la base détermine l’éventail des possibilités relationnelles. La découverte de
                  régularités dans le comportement de la base déverbale permet de prédire le type de
                  relations que peut établir cette base avec son expansion pavant ainsi la voie à un
                  traitement automatique et un travail d’étiquetage sémantique des textes médicaux arabes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper examines the semantic relations existing in Arabic medical texts between
                  the head and its extension in a two-noun compound, particularly when the head is a
                  deverbal noun or a nominalization. By studying semantic relations encoded by nominalizations,
                  this research work aims at underlining the correlation between morphosyntax and semantics
                  notably the influence of the head noun structure on the set of semantic relations
                  that can be established. The discovery of regularities in the functioning of the head
                  noun allows thus to predict the type of relation that will be encoded. Such data are
                  a pre-requisite for natural language processing and automatic part-to-speech tagging
                  of medical Arabic texts.
               </p>
               <hr>
               <h4 id="taln-2008-long-029">Influence de la qualité de l’étiquetage sur le chunking : une corrélation dépendant
                  de la taille des chunks
               </h4>
               			Auteur : Philippe Blache -
               			Contact : philippe.blache@lpl-aix.fr<br>
               			Auteur : Stéphane Rauzy -
               			Contact : stephane.rauzy@lpl-aix.fr<br><p>Nous montrons dans cet article qu’il existe une corrélation étroite existant entre
                  la qualité de l’étiquetage morpho-syntaxique et les performances des chunkers. Cette
                  corrélation devient linéaire lorsque la taille des chunks est limitée. Nous appuyons
                  notre démonstration sur la base d’une expérimentation conduite suite à la campagne
                  d’évaluation Passage 2007 (de la Clergerie et al., 2008). Nous analysons pour cela
                  les comportements de deux analyseurs ayant participé à cette campagne. L’interprétation
                  des résultats montre que la tâche de chunking, lorsqu’elle vise des chunks courts,
                  peut être assimilée à une tâche de “super-étiquetage”.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We show in this paper that a strong correlation exists between the performance of
                  chunk parsers and the quality of the tagging task in input. This dependency becomes
                  linear when the size of the chunks is small. Our demonstration is based on an experiment
                  conducted at the end of the Passage 2007 shared task evaluation initiative (de la
                  Clergerie et al., 2008). The performance of two parsers which took part in this evaluation
                  has been investigated. The results indicate that the chunking task, for sufficiently
                  short chunks, is similar to a super-tagging task.
               </p>
               <hr>
               <h4 id="taln-2008-long-030">Apprentissage artificiel de règles d’indexation pour MEDLINE</h4>
               			Auteur : Aurélie Névéol -
               			Contact : neveola@nlm.nih.gov<br>
               			Auteur : Vincent Claveau -
               			Contact : Vincent.Claveau@irisa.fr<br><p>L’indexation est une composante importante de tout système de recherche d’information.
                  Dans MEDLINE, la base documentaire de référence pour la littérature du domaine biomédical,
                  le contenu des articles référencés est indexé à l’aide de descripteurs issus du thésaurus
                  MeSH. Avec l’augmentation constante de publications à indexer pour maintenir la base
                  à jour, le besoin d’outils automatiques se fait pressant pour les indexeurs. Dans
                  cet article, nous décrivons l’utilisation et l’adaptation de la Programmation Logique
                  Inductive (PLI) pour découvrir des règles d’indexation permettant de générer automatiquement
                  des recommandations d’indexation pour MEDLINE. Les résultats obtenus par cette approche
                  originale sont très satisfaisants comparés à ceux obtenus à l’aide de règles manuelles
                  lorsque celles-ci existent. Ainsi, les jeux de règles obtenus par PLI devraient être
                  prochainement intégrés au système produisant les recommandations d’indexation automatique
                  pour MEDLINE.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Indexing is a crucial step in any information retrieval system. In MEDLINE, a widely
                  used database of the biomedical literature, the indexing process involves the selection
                  of Medical Subject Headings in order to describe the subject matter of articles. The
                  need for automatic tools to assist human indexers in this task is growing with the
                  increasing amount of publications to be referenced in MEDLINE. In this paper, we describe
                  the use and the customization of Inductive Logic Programming (ILP) to infer indexing
                  rules that may be used to produce automatic indexing recommendations for MEDLINE indexers.
                  Our results show that this original ILP-based approach overperforms manual rules when
                  they exist. We expect the sets of ILP rules obtained in this experiment to be integrated
                  in the system producing automatic indexing recommendations for MEDLINE.
               </p>
               <hr>
               <h4 id="taln-2008-court-001">Y a-t-il une véritable équivalence entre les propositions syntaxiques du français
                  et du japonais ?
               </h4>
               			Auteur : Yayoi Nakamura-Delloye -
               			Contact : yayoi@free.fr<br><p>La présente contribution part de nos constats réalisés à partir des résultats d’évaluation
                  de notre système d’alignement des propositions de textes français-japonais. La présence
                  importante de structures fondamentalement difficiles à aligner et les résultats peu
                  satisfaisants de différentes méthodes de mise en correspondance des mots nous ont
                  finalement amenés à remettre en cause l’existence même d’équivalence au niveau des
                  propositions syntaxiques entre le français et le japonais. Afin de compenser les défauts
                  que nous avons découverts, nous proposons des opérations permettant de restaurer l’équivalence
                  des propositions alignées et d’améliorer la qualité des corpus alignés.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper is based on our observations obtained from the results of our French-Japanese
                  clause alignment system. Structures fundamentally difficult to align were so numerous
                  and results obtained by various word-matching methods were so unsatisfactory that
                  we questioned the existence of equivalence at the syntactic clause level between French
                  and Japanese. In order to compensate the defect that we discovered, we propose some
                  operations to restore aligned clause equivalence to improve the quality of aligned
                  corpora.
               </p>
               <hr>
               <h4 id="taln-2008-court-002">Calculs d’unification sur les arbres de dérivation TAG</h4>
               			Auteur : Sylvain Schmitz -
               			Contact : Sylvain.Schmitz@loria.fr<br>
               			Auteur : Joseph Le Roux -
               			Contact : Joseph.LeRoux@loria.fr<br><p>Nous définissons un formalisme, les grammaires rationnelles d’arbres avec traits,
                  et une traduction des grammaires d’arbres adjoints avec traits vers ce nouveau formalisme.
                  Cette traduction préserve les structures de dérivation de la grammaire d’origine en
                  tenant compte de l’unification de traits. La construction peut être appliquée aux
                  réalisateurs de surface qui se fondent sur les arbres de dérivation.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The derivation trees of a tree adjoining grammar provide a first insight into the
                  sentence semantics, and are thus prime targets for generation systems. We define a
                  formalism, feature based regular tree grammars, and a translation from feature based
                  tree adjoining grammars into this new formalism. The translation preserves the derivation
                  structures of the original grammar, and accounts for feature unification.
               </p>
               <hr>
               <h4 id="taln-2008-court-003">Comparaison de méthodes lexicales et syntaxico-sémantiques dans la segmentation thématique
                  de texte non supervisée
               </h4>
               			Auteur : Alexandre Labadié -
               			Contact : labadie@lirmm.fr<br>
               			Auteur : Violaine Prince -
               			Contact : prince@lirmm.fr<br><p>Cet article présente une méthode basée sur des calculs de distance et une analyse
                  sémantique et syntaxique pour la segmentation thématique de texte. Pour évaluer cette
                  méthode nous la comparons à un un algorithme lexical très connu : c99. Nous testons
                  les deux méthodes sur un corpus de discours politique français et comparons les résultats.
                  Les deux conclusions qui ressortent de notre expérience sont que les approches sont
                  complémentaires et que les protocoles d’évaluation actuels sont inadaptés.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper present a semantic and syntactic distance based method in topic text segmentation
                  and compare it to a very well known text segmentation algorithm : c99. To do so we
                  ran the two algorithms on a corpus of twenty two French political discourses and compared
                  their results. Our two conclusions are that the two approaches are complementary and
                  that evaluation methods in this domain should be revised.
               </p>
               <hr>
               <h4 id="taln-2008-court-004">Un modèle de langage pour le DHM : la Grammaire Sémantique Réversible</h4>
               			Auteur : Jérôme Lehuen -
               			Contact : Jerome.Lehuen@lium.univ-lemans.fr<br><p>Cet article propose un modèle de langage dédié au dialogue homme-machine, ainsi que
                  des algorithmes d’analyse et de génération. L’originalité de notre approche est de
                  faire reposer l’analyse et la génération sur les mêmes connaissances, essentiellement
                  sémantiques. Celles-ci sont structurées sous la forme d’une bibliothèque de concepts,
                  et de formes d’usage associées aux concepts. Les algorithmes, quant à eux, sont fondés
                  sur un double principe de correspondance entre des offres et des attentes, et d’un
                  calcul heuristique de score.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we present a language model for man-machine dialogue, as well as algorithms
                  for analysis and text generation. The originality of our approach is to base analysis
                  and generation on the same knowledge. These one is structured like a library of concepts
                  and syntactic patterns. The algorithms are based on a principle of correspondence
                  between offers and expectations, and calculation of a heuristic scoring.
               </p>
               <hr>
               <h4 id="taln-2008-court-005">Discourse Representation Theory et graphes sémantiques : formalisation sémantique
                  en contexte industriel
               </h4>
               			Auteur : Maxime Amblard -
               			Contact : maxime.amblard@orange-ftgroup.com<br>
               			Auteur : Johannes Heinecke -
               			Contact : johannes.heinecke@orange-ftgroup.com<br>
               			Auteur : Estelle Maillebuau -
               			Contact : estelle.maillebuau@orange-ftgroup.com<br><p>Ces travaux présentent une extension des représentations formelles pour la sémantique,
                  de l’outil de traitement automatique des langues de Orange Labs1. Nous abordons ici
                  uniquement des questions relatives à la construction des représentations sémantiques,
                  dans le cadre de l’analyse linguistique. Afin d’obtenir des représentations plus fines
                  de la structure argumentale des énoncés, nous incluons des concepts issus de la DRT
                  dans le système de représentation basé sur les graphes sémantiques afin de rendre
                  compte de la notion de portée.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This works present an extension of the formal representation of semantic for the natural
                  language processing tool developped at Orange Labs. We cover here only issues relating
                  to build the semantic representations, based on linguistic parsing. To obtain more
                  detailed representations of the argumental structure statements, we include insights
                  from the DRT in the system of representation based on the semantic graphs to give
                  an account of scope.
               </p>
               <hr>
               <h4 id="taln-2008-court-006">Sylva : plate-forme de validation multi-niveaux de lexiques</h4>
               			Auteur : Karën Fort -
               			Contact : Karen.Fort@loria.fr<br>
               			Auteur : Bruno Guillaume -
               			Contact : Bruno.Guillaume@loria.fr<br><p>La production de lexiques est une activité indispensable mais complexe, qui nécessite,
                  quelle que soit la méthode de création utilisée (acquisition automatique ou manuelle),
                  une validation humaine. Nous proposons dans ce but une plate-forme Web librement disponible,
                  appelée Sylva (Systematic lexicon validator). Cette plate-forme a pour caractéristiques
                  principales de permettre une validation multi-niveaux (par des validateurs, puis un
                  expert) et une traçabilité de la ressource. La tâche de l’expert(e) linguiste en est
                  allégée puisqu’il ne lui reste à considérer que les données sur lesquelles il n’y
                  a pas d’accord inter-validateurs.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Lexicon production is essential but complex and all creation methods (automatic acquisition
                  or manual creation) require human validation. For this purpose, we propose a freely
                  available Web-based framework, named Sylva (Systematic lexicon validator). The main
                  point of our framework is that it handles multi-level validations and keeps track
                  of the resource’s history. The expert linguist task is made easier : (s)he has only
                  to consider data on which validators disagree.
               </p>
               <hr>
               <h4 id="taln-2008-court-007">E-Gen : Profilage automatique de candidatures</h4>
               			Auteur : Rémy Kessler -
               			Contact : remy.kessler@univ-avignon.fr<br>
               			Auteur : Juan-Manuel Torres-Moreno -
               			Contact : juan-manuel.torres@univ-avignon.fr<br>
               			Auteur : Marc El-Bèze -
               			Contact : marc.elbeze@univ-avignon.fr<br><p>La croissance exponentielle de l’Internet a permis le développement de sites d’offres
                  d’emploi en ligne. Le système E-Gen (Traitement automatique d’offres d’emploi) a pour
                  but de permettre l’analyse et la catégorisation d’offres d’emploi ainsi qu’une analyse
                  et classification des réponses des candidats (Lettre de motivation et CV). Nous présentons
                  les travaux réalisés afin de résoudre la seconde partie : on utilise une représentation
                  vectorielle de texte pour effectuer une classification des pièces jointes contenus
                  dans le mail à l’aide de SVM. Par la suite, une évaluation de la candidature est effectuée
                  à l’aide de différents classifieurs (SVM et n-grammes de mots).
               </p><em>Version anglaise :</em><h4></h4>
               <p>The exponential growth of the Internet has allowed the development of a market of
                  on-line job search sites. This paper presents the E-Gen system (Automatic Job Offer
                  Processing system for Human Resources). E-Gen will perform two complex tasks : an
                  analysis and categorisation of job postings, which are unstructured text documents,
                  an analysis and a relevance ranking of the candidate answers (cover letter and curriculum
                  vitae). Here we present the work related to the second task : we use vectorial representation
                  before generating a classification with SVM to determine the type of the attachment.
                  In the next step, we try to classify the candidate answers with different classifiers
                  (SVM and ngrams of words).
               </p>
               <hr>
               <h4 id="taln-2008-court-008">Typage, produit cartésien et unités d’analyse pour les modèles à états finis</h4>
               			Auteur : François Barthélemy -
               			Contact : barthe@cnam.fr<br><p>Dans cet article, nous présentons un nouveau langage permettant d’écrire des relations
                  rationnelles compilées en automates finis. Les deux caractéristiques innovantes de
                  ce langage sont de pourvoir décrire des relations à plusieurs niveaux, pas nécessairement
                  deux et d’utiliser diverses unités d’analyse pour exprimer les liens entre niveaux.
                  Cela permet d’aligner de façon fine des représentations multiples.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present a new language to write rational relations compiled into
                  finite state automata. There are two main novelties in the language. Firstly, the
                  descriptions may have more than two levels. Secondly, various units may be used to
                  express the relationships between the levels. Using these features, it is possible
                  to align finely multiple representations.
               </p>
               <hr>
               <h4 id="taln-2008-court-009">Vers l’évaluation de systèmes de dialogue homme-machine : de l’oral au multimodal</h4>
               			Auteur : Frédéric Landragin -
               			Contact : frederic.landragin@linguist.jussieu.fr<br><p>L’évaluation pour le dialogue homme-machine ne se caractérise pas par l’efficacité,
                  l’objectivité et le consensus que l’on observe dans d’autres domaines du traitement
                  automatique des langues. Les systèmes de dialogue oraux et multimodaux restent cantonnés
                  à des domaines applicatifs restreints, ce qui rend difficiles les évaluations comparatives
                  ou normées. De plus, les avancées technologiques constantes rendent vite obsolètes
                  les paradigmes d’évaluation et ont pour conséquence une multiplication de ceux-ci.
                  Des solutions restent ainsi à trouver pour améliorer les méthodes existantes et permettre
                  des diagnostics plus automatisés des systèmes. Cet article se veut un ensemble de
                  réflexions autour de l’évaluation de la multimodalité dans les systèmes à forte composante
                  linguistique. Des extensions des paradigmes existants sont proposées, en particulier
                  DQR/DCR, sachant que certains sont mieux adaptés que d’autres au dialogue multimodal.
                  Des conclusions et perspectives sont tirées sur l’avenir de l’évaluation pour le dialogue
                  homme-machine.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Evaluating human-machine dialogue systems is not so efficient, objective, and consensual
                  than evaluating other natural language processing systems. Oral and multimodal dialogue
                  systems are still working within reduced applicative domains. Comparative and normative
                  evaluations are then difficult. Moreover, the continuous technological progress makes
                  obsolete and numerous the evaluating paradigms. Some solutions are still to be identified
                  to improve existing methods and to allow a more automatic diagnosis of systems. The
                  aim of this paper is to provide a set of remarks dealing with the evaluation of multimodal
                  spoken language dialogue systems. Some extensions of existing paradigms are presented,
                  in particular DQR/DCR, considering that some paradigms fit better multimodal issues
                  than others. Some conclusions and perspectives are then drawn on the future of the
                  evaluation of human-machine dialogue systems.
               </p>
               <hr>
               <h4 id="taln-2008-court-010">POLYMOTS : une base de données de constructions dérivationnelles en français à partir
                  de radicaux phonologiques
               </h4>
               			Auteur : Nuria Gala -
               			Contact : nuria.gala@univ-provence.fr<br>
               			Auteur : Véronique Rey -
               			Contact : veronique.rey@univ-provence.fr<br><p>Cet article présente POLYMOTS, une base de données lexicale contenant huit mille mots
                  communs en français. L’originalité de l’approche proposée tient à l'analyse des mots.
                  En effet, à la différence d’autres bases lexicales représentant la morphologie dérivationnelle
                  des mots à partir d’affixes, ici l’idée a été d’isoler un radical commun à un ensemble
                  de mots d’une même famille. Nous avons donc analysé les formes des mots et, par comparaison
                  phonologique (forme phonique comparable) et morphologique (continuité de sens), nous
                  avons regroupé les mots par familles, selon le type de radical phonologique. L’article
                  présente les fonctionnalités de la base et inclut une discussion sur les applications
                  et les perspectives d’une telle ressource.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper we present POLYMOTS, a lexical database containing eight thousand common
                  nouns in French. Whereas most of the existing lexicons for derivational morphology
                  take affixes as starting point for producing paradigms of words, we defend here the
                  idea that it is possible to isolate a morpho-phonological stem and produce a paradigm
                  of words belonging to the same family. This point leads us to describe three types
                  of stems according to their phonological and morphological form. The article presents
                  the different features of such a lexical database and discusses the applications and
                  future work using and enriching this resource.
               </p>
               <hr>
               <h4 id="taln-2008-court-011">Mesure de l’alternance entre préfixes pour la génération en traduction automatique</h4>
               			Auteur : Bruno Cartoni -
               			Contact : bruno.cartoni@eti.unige.ch<br><p>La génération de néologismes construits pose des problèmes dans un système de traduction
                  automatique, notamment au moment de la sélection du préfixe dans les formations préfixées,
                  quand certains préfixes paraissent pouvoir alterner. Nous proposons une étude « extensive
                  », qui vise à rechercher dans de larges ressources textuelles (l’Internet) des formes
                  préfixées générées automatiquement, dans le but d’individualiser les paramètres qui
                  favorisent l’un des préfixes ou qui, au contraire, permettent cette alternance. La
                  volatilité de cette ressource textuelle nécessite certaines précautions dans la méthodologie
                  de décompte des données extraites.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Generating constructed neologisms in a machine translation system is confronted to
                  the issue of selecting the right affixes, especially when some affixes can be used
                  alternately. We propose here an “extensive” study that looks into large textual data
                  collections (web) for prefixed forms that have been automatically generated, in order
                  to find out parameters that allow the use of both prefixes or, on the contrary, that
                  prevent one or the other prefixation. The volatility of web resources requires methodological
                  precautions, especially in data counting.
               </p>
               <hr>
               <h4 id="taln-2008-court-012">Cascades de transducteurs pour le chunking de la parole conversationnelle : l’utilisation
                  de la plateforme CasSys dans le projet EPAC
               </h4>
               			Auteur : Abdenour Mokrane -
               			Contact : Abdenour.Mokrane@univ-tours.fr<br>
               			Auteur : Nathalie Friburger -
               			Contact : Nathalie.Friburger@univ-tours.fr<br>
               			Auteur : Jean-Yves Antoine -
               			Contact : Jean-Yves.Antoine@univ-tours.fr<br><p>Cet article présente l’utilisation de la plate-forme CasSys pour la segmentation de
                  la parole conversationnelle (chunking) à l’aide de cascades de transducteurs Unitex.
                  Le système que nous présentons est utilisé dans le cadre du projet ANR EPAC. Ce projet
                  a pour objectif l’indexation et l’annotation automatique de grands flux de parole
                  issus d’émissions télévisées ou radiophoniques. Cet article présente tout d’abord
                  l’adaptation à ce type de données d’un système antérieur de chunking (Romus) qui avait
                  été développé pour le dialogue oral homme-machine. Il décrit ensuite les principaux
                  problèmes qui se posent à l’analyse : traitement des disfluences de l’oral spontané,
                  mais également gestion des erreurs dues aux étapes antérieures de reconnaissance de
                  la parole et d’étiquetage morphosyntaxique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes the use of the CasSys platform in order to achieve the chunking
                  of conversational speech transcripts by means of cascades of Unitex transducers. Our
                  system is involved in the EPAC project of the French National Agency of Research (ANR).
                  The aim of this project is to develop robust methods for the annotation of audio/multimedia
                  document collections which contains conversational speech sequences such as TV or
                  radio programs. At first, this paper presents the adaptation of a former chunking
                  system (Romus) which was developed in the restricted framework of dedicated spoken
                  manmachine dialogue. Then, it describes the problems that are arising due to 1) spontaneous
                  speech disfluencies and 2) errors for the previous stages of processing (automatic
                  speech recognition and POS tagging).
               </p>
               <hr>
               <h4 id="taln-2008-court-013">Regroupement automatique de documents en classes événementielles</h4>
               			Auteur : Aurélien Bossard -
               			Contact : Aurelien.Bossard@lipn.univ-paris13.fr<br>
               			Auteur : Thierry Poibeau -
               			Contact : Thierry.Poibeau@lipn.univ-paris13.fr<br><p>Cet article porte sur le regroupement automatique de documents sur une base événementielle.
                  Après avoir précisé la notion d’événement, nous nous intéressons à la représentation
                  des documents d’un corpus de dépêches, puis à une approche d’apprentissage pour réaliser
                  les regroupements de manière non supervisée fondée sur k-means. Enfin, nous évaluons
                  le système de regroupement de documents sur un corpus de taille réduite et nous discutons
                  de l’évaluation quantitative de ce type de tâche.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper analyses the problem of automatic document clustering based on events.
                  We first specify the notion of event. Then, we detail the document modelling method
                  and the learning approach for document clustering based on k-means. We finally evaluate
                  our document clustering system on a small corpus and discuss the quantitative evaluation
                  for this kind of task.
               </p>
               <hr>
               <h4 id="taln-2008-court-014"></h4>
               			Auteur : Mary Hearne -
               			Contact : mhearne@computing.dcu.ie<br>
               			Auteur : Sylwia Ozdowska -
               			Contact : sozdowska@computing.dcu.ie<br>
               			Auteur : John Tinsley -
               			Contact : jtinsley@computing.dcu.ie<br><p>Nous évaluons le recours à des techniques de traduction à base de segments syntaxiquement
                  motivés, seules ou en combinaison avec des techniques à base de segments non motivés,
                  et nous comparons les apports respectifs de l’analyse en constituants et de l’analyse
                  en dépendances dans ce cadre. À partir d’un corpus parallèle Anglais–Français, nous
                  construisons automatiquement deux corpus d’entraînement arborés, en constituants et
                  en dépendances, alignés au niveau sous-phrastique et en extrayons des correspondances
                  bilingues entre mots et syntagmes motivées syntaxiquement. Nous mesurons automatiquement
                  la qualité de la traduction obtenue par un système à base de segments. Les résultats
                  montrent que la combinaison des correspondances bilingues non motivées et motivées
                  sur le plan syntaxique améliore la qualité de la traduction quel que soit le type
                  d’analyse considéré. Par ailleurs, le gain en qualité est plus important avec le recours
                  à l’analyse en dépendances au regard des constituants.
               </p><em>Version anglaise :</em><h4>Comparing Constituency and Dependency Representations for SMT Phrase-Extraction</h4>
               <p>We consider the value of replacing and/or combining string-basedmethods with syntax-based
                  methods for phrase-based statistical machine translation (PBSMT), and we also consider
                  the relative merits of using constituency-annotated vs. dependency-annotated training
                  data. We automatically derive two subtree-aligned treebanks, dependency-based and
                  constituency-based, from a parallel English–French corpus and extract syntactically
                  motivated word- and phrase-pairs. We automatically measure PB-SMT quality. The results
                  show that combining string-based and syntax-based word- and phrase-pairs can improve
                  translation quality irrespective of the type of syntactic annotation. Furthermore,
                  using dependency annotation yields greater translation quality than constituency annotation
                  for PB-SMT.
               </p>
               <hr>
               <h4 id="taln-2008-court-015">Repérage de citations, classification des styles de discours rapporté et identification
                  des constituants citationnels en écrits journalistiques
               </h4>
               			Auteur : Fabien Poulard -
               			Contact : fabien.poulard@univ-nantes.fr<br>
               			Auteur : Thierry Waszak -
               			Contact : thierry.waszak@univ-avignon.fr<br>
               			Auteur : Nicolas Hernandez -
               			Contact : nicolas.hernandez@univ-nantes.fr<br>
               			Auteur : Patrice Bellot -
               			Contact : patrice.bellot@univ-avignon.fr<br><p>Dans le contexte de la recherche de plagiat, le repérage de citations et de ses constituants
                  est primordial puisqu’il peut amener à évaluer le caractère licite ou illicite d’une
                  reprise (source citée ou non). Nous proposons ici une comparaison de méthodes automatiques
                  pour le repérage de ces informations et rapportons une évaluation quantitative de
                  celles-ci. Un corpus d’écrits journalistiques français a été manuellement annoté pour
                  nous servir de base d’apprentissage et de test.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In the application context of reported content, that includes plagiarism and impact
                  of textual information searched, citations finding and its fundamentals is essential
                  as it may help estimating legal value of a citation (with or without specifying original
                  source). We propose here a comparison between automatic methods for finding up those
                  elements and we quantitatively evaluate them. A French journalistic corpus has been
                  manually annotated to be used as learning base and for testing.
               </p>
               <hr>
               <h4 id="taln-2008-court-016">Vers l’identification et le traitement des actes de dialogue composites</h4>
               			Auteur : Frédéric Landragin -
               			Contact : frederic.landragin@linguist.jussieu.fr<br><p>Il peut être difficile d’attribuer une seule valeur illocutoire à un énoncé dans un
                  dialogue. En premier lieu, un énoncé peut comporter plusieurs segments de discours
                  ayant chacun leur valeur illocutoire spécifique. De plus, un seul segment peut s’analyser
                  en tant qu’acte de langage composite, regroupant par exemple la formulation d’une
                  question et l’émission simultanée d’une information. Enfin, la structure du dialogue
                  en termes d’échanges et de séquences peut être déterminante dans l’identification
                  de l’acte, et peut également apporter une valeur illocutoire supplémentaire, comme
                  celle de clore la séquence en cours. Dans le but de déterminer la réaction face à
                  un tel acte de dialogue composite, nous présentons une approche théorique pour l’analyse
                  des actes de dialogue en fonction du contexte de tâche et des connaissances des interlocuteurs.
                  Nous illustrons sur un exemple nos choix de segmentation et d’identification des actes
                  composites, et nous présentons les grandes lignes d’une stratégie pour déterminer
                  la réaction qui semble être la plus pertinente.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Attributing one illocutionary value to a utterance in a dialogue can be difficult.
                  First, a utterance can include several discourse segments, each one with a specific
                  illocutionary value. Moreover, one discourse segment can be linked to a complex speech
                  act that groups for instance a question together with the assertion of new information.
                  Finally, the dialogue structure with the various exchanges and sequences can be decisive
                  when identifying the dialogue act, and can also bring an additional illocutionary
                  value, for instance the one consisting of closing the current sequence. With the aim
                  to determine how to react to such a composite dialogue act, we present a theoretical
                  approach to dialogue act analysis considering the task context and the dialogue participants’
                  knowledge. We illustrate our choices in terms of segmentation and identification of
                  composite acts, and we present the main features of a strategy for determining the
                  most relevant reaction.
               </p>
               <hr>
               <h4 id="taln-2008-court-017">Représentation évènementielle des déplacements dans des dépêches épidémiologiques</h4>
               			Auteur : Manal El Zant -
               			Contact : el.zant@medecine.univ-mrs.fr<br>
               			Auteur : Jean Royauté -
               			Contact : jean.royaute@lif.univ-mrs.fr<br>
               			Auteur : Michel Roux -
               			Contact : michel.roux@medecine.univ-mrs.fr<br><p>La représentation évènementielle des déplacements de personnes dans des dépêches épidémiologiques
                  est d’une grande importance pour une compréhension détaillée du sens de ces dépêches.
                  La dissémination des composants d’une telle représentation dans les dépêches rend
                  difficile l’accès à leurs contenus. Ce papier décrit un système d’extraction d’information
                  utilisant des cascades de transducteurs à nombre d’états fini qui ont permis la réalisation
                  de trois tâches : la reconnaissance des entités nommées, l’annotation et la représentation
                  des composants ainsi que la représentation des structures évènementielles. Nous avons
                  obtenu une moyenne de rappel de 80, 93% pour la reconnaissance des entités nommées
                  et de 97, 88% pour la représentation des composants. Ensuite, nous avons effectué
                  un travail de normalisation de cette représentation par la résolution de certaines
                  anaphores pronominales. Nous avons obtenu une valeur moyenne de précision de 81, 72%
                  pour cette résolution.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The representation of motion events is important for an automatic comprehension of
                  disease outbreak reports. The dispersion of components in this type of reports makes
                  it difficult to have such a representation. This paper describes an automatic extraction
                  of event structures representation of these texts.We built an information extraction
                  system by using cascaded finite state transducers which allowed the realization of
                  three tasks : the named entity recognition, the component annotation and representation
                  and the event structure representation. We obtained a recall of 80, 93% for the named
                  entity recognition task and a recall of 97, 88% for argument representation task.
                  Thereafter, we worked in anaphoric pronouns resolution where we obtained a precision
                  of 81.83%.
               </p>
               <hr>
               <h4 id="taln-2008-court-018">Traduction multilingue : le projet MulTra</h4>
               			Auteur : Éric Wehrli -
               			Contact : Eric.Wehrli@lettres.unige.ch<br>
               			Auteur : Luka Nerima -
               			Contact : Luka.Nerima@lettres.unige.ch<br><p>L’augmentation rapide des échanges et des communications pluriculturels, en particulier
                  sur internet, intensifie les besoins d’outils multilingues y compris de traduction.
                  Cet article décrit un projet en cours au LATL pour le développement d’un système de
                  traduction multilingue basé sur un modèle linguistique abstrait et largement générique,
                  ainsi que sur un modèle logiciel basé sur la notion d’objet. Les langues envisagées
                  dans la première phase de ce projet sont l’allemand, le français, l’italien, l’espagnol
                  et l’anglais.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The increase of cross-cultural communication triggered notably by the Internet intensifies
                  the needs for multilingual linguistic tools, in particular translation systems for
                  several languages. The LATL has developed an efficient multilingual parsing technology
                  based on an abstract and generic linguistic model and on object-oriented software
                  design. The proposed project intends to apply a similar approach to the problem of
                  multilingual translation (German, French, Italian and English).
               </p>
               <hr>
               <h4 id="taln-2008-court-019">Appariement d’entités nommées coréférentes : combinaisons de mesures de similarité
                  par apprentissage supervisé
               </h4>
               			Auteur : Erwan Moreau -
               			Contact : emoreau@enst.fr<br>
               			Auteur : François Yvon -
               			Contact : yvon@limsi.fr<br>
               			Auteur : Olivier Cappé -
               			Contact : cappe@enst.fr<br><p>L’appariement d’entités nommées consiste à regrouper les différentes formes sous lesquelles
                  apparaît une entité. Pour cela, des mesures de similarité textuelle sont généralement
                  utilisées. Nous proposons de combiner plusieurs mesures afin d’améliorer les performances
                  de la tâche d’appariement. À l’aide d’expériences menées sur deux corpus, nous montrons
                  la pertinence de l’apprentissage supervisé dans ce but, particulièrement avec l’algorithme
                  C4.5.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Matching named entities consists in grouping the different forms under which an entity
                  may occur. Textual similarity measures are the usual tools for this task. We propose
                  to combine several measures in order to improve the performance. We show the relevance
                  of supervised learning in this objective through experiences with two corpora, especially
                  in the case of the C4.5 algorithm.
               </p>
               <hr>
               <h4 id="taln-2008-court-020">Un sens logique pour les graphes sémantiques</h4>
               			Auteur : Renaud Marlet -
               			Contact : renaud.marlet@inria.fr<br><p>Nous discutons du sens des graphes sémantiques, notamment de ceux utilisés en Théorie
                  Sens-Texte. Nous leur donnons un sens précis, éventuellement sous-spécifié, grâce
                  à une traduction simple vers une formule de Minimal Recursion Semantics qui couvre
                  les cas de prédications multiples sur plusieurs entités, de prédication d’ordre supérieur
                  et de modalités.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We discuss the meaning of semantic graphs, in particular of those used in Meaning-Text
                  Theory. We provide a precise, possibly underspecified, meaning to such graphs through
                  a simple translation into a Minimal Recursion Semantics formula. This translation
                  covers cases of multiple predications over several entities, higher order predication
                  and modalities.
               </p>
               <hr>
               <h4 id="taln-2008-court-021">Annotation en Frames Sémantiques du corpus de dialogue MEDIA</h4>
               			Auteur : Marie-Jean Meurs -
               			Contact : marie-jean.meurs@univ-avignon.fr<br>
               			Auteur : Frédéric Duvert -
               			Contact : frederic.duvert@univ-avignon.fr<br>
               			Auteur : Frédéric Béchet -
               			Contact : frederic.bechet@univ-avignon.fr<br>
               			Auteur : Fabrice Lefèvre -
               			Contact : fabrice.lefevre@univ-avignon.fr<br>
               			Auteur : Renato De Mori -
               			Contact : renato.demori@univ-avignon.fr<br><p>Cet article présente un formalisme de représentation des connaissances qui a été utilisé
                  pour fournir des annotations sémantiques de haut niveau pour le corpus de dialogue
                  oral MEDIA. Ces annotations en structures sémantiques, basées sur le paradigme FrameNet,
                  sont obtenues de manière incrémentale et partiellement automatisée. Nous décrivons
                  le processus d’interprétation automatique qui permet d’obtenir des compositions sémantiques
                  et de générer des hypothèses de frames par inférence. Le corpus MEDIA est un corpus
                  de dialogues en langue française dont les tours de parole de l’utilisateur ont été
                  manuellement transcrits et annotés (niveaux mots et constituants sémantiques de base).
                  Le processus proposé utilise ces niveaux pour produire une annotation de haut niveau
                  en frames sémantiques. La base de connaissances développée (définitions des frames
                  et règles de composition) est présentée, ainsi que les résultats de l’annotation automatique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper introduces a knowledge representation formalism, used for incremental and
                  partially automated annotation of the French MEDIA dialogue corpus in terms of semantic
                  structures. We describe an automatic interpretation process for composing semantic
                  structures from basic semantic constituents using patterns involving constituents
                  and words. The process has procedures for obtaining semantic compositions and generating
                  frame hypotheses by inference. This process is applied to MEDIA, a dialogue corpus
                  manually annotated at the word and semantic constituent levels, and thus produces
                  a higher level semantic frame annotation. The Knowledge Source defined and the results
                  obtained on the automatically-derived annotation are reported.
               </p>
               <hr>
               <h4 id="taln-2008-court-022">Dissymétrie entre l'indexation des documents et le traitement des requêtes pour la
                  recherche d’information en langue arabe
               </h4>
               			Auteur : Ramzi Abbès -
               			Contact : ramzi.abbes@univ-lyon2.fr<br>
               			Auteur : Malek Boualem -
               			Contact : malek.boualem@orange-ftgroup.com<br><p>Les moteurs de recherches sur le web produisent des résultats comparables et assez
                  satisfaisants pour la recherche de documents écrits en caractères latins. Cependant,
                  ils présentent de sérieuses lacunes dès que l'ont s'intéresse à des langues peu dotées
                  ou des langues sémitiques comme l'arabe. Dans cet article nous présentons une étude
                  analytique et qualitative de la recherche d’information en langue arabe en mettant
                  l'accent sur l'insuffisance des outils de recherche actuels, souvent mal adaptés aux
                  spécificités de la langue arabe. Pour argumenter notre analyse, nous présentons des
                  résultats issus d’observations et de tests autour de certains phénomènes linguistiques
                  de l’arabe écrit. Pour la validation des ces observations, nous avons testé essentiellement
                  le moteur de recherche Google.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Web search engines provide quite good results for Latin characters-based languages.
                  However, they still show many weaknesses when searching in other languages such as
                  Arabic. This paper discusses a qualitative analysis of information retrieval in Arabic
                  language, highlighting some of the numerous limitations of available search engines,
                  mainly when they are not properly adapted to the Arabic language specificities. To
                  argue our analysis, we present some results based on quite sufficient observations
                  and tests on various Arabic linguistic phenomena. To validate these observations,
                  we essentially have tested the Google search engine.
               </p>
               <hr>
            </div>
         </div>
         <div class="conference">
            <h2 id="TALN'2009">2009 - 
               					TALN'2009 - Président(s) :  
               					Adeline Nazarenko - 
               	Thierry Poibeau - 
               	 à 
               					Senlis
            </h2>
            
            				Il y a 92 articles référencés. 
            
            				<br>
            			Statistiques :
            			
            <ul>
               <li>108 articles long soumis dont 29 acceptés. </li>
               <li>6 articles position soumis dont 3 acceptés. </li>
               <li>108 articles court soumis dont 46 acceptés. </li>
            </ul><br>
            			Meilleur(s) article(s) : 
            			
            <ul>
               <li><a href="#taln-2009-long-019">taln-2009-long-019</a></li>
            </ul>
            <h3>Les articles</h3>
            <div id="articles">
               <h4 id="taln-2009-long-001">Acquisition morphologique à partir d’un dictionnaire informatisé</h4>
               			Auteur : Nabil Hathout -
               			Contact : Nabil.Hathout@univ-tlse2.fr<br><p>L’article propose un modèle linguistique et informatique permettant de faire émerger
                  la structure morphologique dérivationnelle du lexique à partir des régularités sémantiques
                  et formelles des mots qu’il contient. Ce modèle est radicalement lexématique. La structure
                  morphologique est constituée par les relations que chaque mot entretient avec les
                  autres unités du lexique et notamment avec les mots de sa famille morphologique et
                  de sa série dérivationnelle. Ces relations forment des paradigmes analogiques. La
                  modélisation a été testée sur le lexique du français en utilisant le dictionnaire
                  informatisé TLFi.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The paper presents a linguistic and computational model aiming at making the morphological
                  structure of the lexicon emerge from the formal and semantic regularities of the words
                  it contains. The model is word-based. The proposed morphological structure consists
                  of (1) binary relations that connect each headword with words that are morphologically
                  related, and especially with the members of its morphological family and its derivational
                  series, and of (2) the analogies that hold between the words. The model has been tested
                  on the lexicon of French using the TLFi machine readable dictionary.
               </p>
               <hr>
               <h4 id="taln-2009-long-002">Analyse déductive pour les grammaires d’interaction</h4>
               			Auteur : Joseph Le Roux -
               			Contact : jleroux@computing.dcu.ie<br><p>Nous proposons un algorithme d’analyse pour les grammaires d’interaction qui utilise
                  le cadre formel de l’analyse déductive. Cette approche donne un point de vue nouveau
                  sur ce problème puisque les méthodes précédentes réduisaient ce dernier à la réécriture
                  de graphes et utilisaient des techniques de résolution de contraintes. D’autre part,
                  cette présentation permet de décrire le processus de manière standard et d’exhiber
                  les sources d’indéterminisme qui rendent ce problème difficile.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a parsing algorithm for Interaction Grammars using the deductive parsing
                  framework. This approach brings new perspectives on this problem, departing from previous
                  methods relying on constraint-solving techniques to interpret it as a graph-rewriting
                  problem. Furthermore, this presentation allows a standard description of the algorithm
                  and a fine-grained inspection of the sources of non-determinism.
               </p>
               <hr>
               <h4 id="taln-2009-long-003">Analyse syntaxique en dépendances de l’oral spontané</h4>
               			Auteur : Alexis Nasr<br>
               			Auteur : Frédéric Béchet<br><p>Cet article décrit un modèle d’analyse syntaxique de l’oral spontané axé sur la reconnaissance
                  de cadres valenciels verbaux. Le modèle d’analyse se décompose en deux étapes : une
                  étape générique, basée sur des ressources génériques du français et une étape de réordonnancement
                  des solutions de l’analyseur réalisé par un modèle spécifique à une application. Le
                  modèle est évalué sur le corpus MEDIA.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We describe in this paper a syntactic parser for spontaneous speech geared towards
                  the identification of verbal subcategorization frames. The parser proceeds in two
                  stages. The first stage is based on generic syntactic ressources for French. The second
                  stage is a reranker which is specially trained for a given application. The parser
                  is evaluated on the MEDIA corpus.
               </p>
               <hr>
               <h4 id="taln-2009-long-004">Analyse syntaxique du français : des constituants aux dépendances</h4>
               			Auteur : Marie Candito -
               			Contact : mcandito@linguist.jussieu.fr<br>
               			Auteur : Benoît Crabbé -
               			Contact : bcrabbe@linguist.jussieu.fr<br>
               			Auteur : Pascal Denis -
               			Contact : pascal.denis@inria.fr<br>
               			Auteur : François Guérin -
               			Contact : francois.guerin@inria.fr<br><p>Cet article présente une technique d’analyse syntaxique statistique à la fois en constituants
                  et en dépendances. L’analyse procède en ajoutant des étiquettes fonctionnelles aux
                  sorties d’un analyseur en constituants, entraîné sur le French Treebank, pour permettre
                  l’extraction de dépendances typées. D’une part, nous spécifions d’un point de vue
                  formel et linguistique les structures de dépendances à produire, ainsi que la procédure
                  de conversion du corpus en constituants (le French Treebank) vers un corpus cible
                  annoté en dépendances, et partiellement validé. D’autre part, nous décrivons l’approche
                  algorithmique qui permet de réaliser automatiquement le typage des dépendances. En
                  particulier, nous nous focalisons sur les méthodes d’apprentissage discriminantes
                  d’étiquetage en fonctions grammaticales.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper describes a technique for both constituent and dependency parsing. Parsing
                  proceeds by adding functional labels to the output of a constituent parser trained
                  on the French Treebank in order to further extract typed dependencies. On the one
                  hand we specify on formal and linguistic grounds the nature of the dependencies to
                  output as well as the conversion algorithm from the French Treebank to this dependency
                  representation. On the other hand, we describe a class of algorithms that allows to
                  perform the automatic labeling of the functions from the output of a constituent based
                  parser. We specifically focus on discriminative learning methods for functional labelling.
               </p>
               <hr>
               <h4 id="taln-2009-long-005">Annotation fonctionnelle de corpus arborés avec des Champs Aléatoires Conditionnels</h4>
               			Auteur : Erwan Moreau -
               			Contact : erwan.moreau@lipn.univ-paris13.fr<br>
               			Auteur : Isabelle Tellier -
               			Contact : isabelle.tellier@univ-orleans.fr<br>
               			Auteur : Antonio Balvet<br>
               			Auteur : Grégoire Laurence<br>
               			Auteur : Antoine Rozenknop<br>
               			Auteur : Thierry Poibeau<br><p>L’objectif de cet article est d’évaluer dans quelle mesure les “fonctions syntaxiques”
                  qui figurent dans une partie du corpus arboré de Paris 7 sont apprenables à partir
                  d’exemples. La technique d’apprentissage automatique employée pour cela fait appel
                  aux “Champs Aléatoires Conditionnels” (Conditional Random Fields ou CRF), dans une
                  variante adaptée à l’annotation d’arbres. Les expériences menées sont décrites en
                  détail et analysées. Moyennant un bon paramétrage, elles atteignent une F1-mesure
                  de plus de 80%.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The purpose of this paper is to evaluatewhether the "syntactic functions" present
                  in a part of the Paris 7 Treebank are learnable from examples. The learning technic
                  used is the one of "Conditional Random Fields" (CRF), in an original variant adapted
                  to tree labelling. The conducted experiments are extensively described and analyzed.
                  With good parameters, a F1-mesure value of over 80% is reached.
               </p>
               <hr>
               <h4 id="taln-2009-long-006">Apport d’un corpus comparable déséquilibré à l’extraction de lexiques bilingues</h4>
               			Auteur : Emmanuel Morin -
               			Contact : emmanuel.morin@univ-nantes.fr<br><p>Les principaux travaux en extraction de lexiques bilingues à partir de corpus comparables
                  reposent sur l’hypothèse implicite que ces corpus sont équilibrés. Cependant, les
                  différentes méthodes computationnelles associées sont relativement insensibles à la
                  taille de chaque partie du corpus. Dans ce contexte, nous étudions l’influence que
                  peut avoir un corpus comparable déséquilibré sur la qualité des terminologies bilingues
                  extraites à travers différentes expériences. Nos résultats montrent que sous certaines
                  conditions l’utilisation d’un corpus comparable déséquilibré peut engendrer un gain
                  significatif dans la qualité des lexiques extraits.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The main work in bilingual lexicon extraction from comparable corpora is based on
                  the implicit hypothesis that corpora are balanced. However, the different related
                  approaches are relatively insensitive to sizes of each part of the comparable corpus.
                  Within this context, we study the influence of unbalanced comparable corpora on the
                  quality of bilingual terminology extraction through different experiments. Our results
                  show the conditions under which the use of an unbalanced comparable corpus can induce
                  a significant gain in the quality of extracted lexicons.
               </p>
               <hr>
               <h4 id="taln-2009-long-007">Classification d’un contenu encyclopédique en vue d’un étiquetage par entités nommées</h4>
               			Auteur : Eric Charton -
               			Contact : eric.charton@univ-avignon.fr<br>
               			Auteur : Juan-Manuel Torres-Moreno -
               			Contact : juan-manuel.torres@univ-avignon.fr<br><p>On utilise souvent des ressources lexicales externes pour améliorer les performances
                  des systèmes d’étiquetage d’entités nommées. Les contenus de ces ressources lexicales
                  peuvent être variés : liste de noms propres, de lieux, de marques. On note cependant
                  que la disponibilité de corpus encyclopédiques exhaustifs et ouverts de grande taille
                  tels que Worldnet ou Wikipedia, a fait émerger de nombreuses propositions spécifiques
                  d’exploitation de ces contenus par des systèmes d’étiquetage. Un problème demeure
                  néanmoins ouvert avec ces ressources : celui de l’adaptation de leur taxonomie interne,
                  complexe et composée de dizaines de milliers catégories, aux exigences particulières
                  de l’étiquetage des entités nommées. Pour ces dernières, au plus de quelques centaines
                  de classes sémantiques sont requises. Dans cet article nous explorons cette difficulté
                  et proposons un système complet de transformation d’un arbre taxonomique encyclopédique
                  en une système à classe sémantiques adapté à l’étiquetage d’entités nommées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The advent of Wikipedia and WordNet aroused new interest in labeling by named entity
                  aided by external resources. The availability of these large, multilingual, comprehensive
                  and open digital encyclopaedic corpora suggests the development of labeling solutions
                  that exploit the knowledge contained in these corpora. The mapping of a word sequence
                  to an encyclopedic document is possible, however the classification of encyclopaedic
                  entities and their related labels, is not yet fully resolved. The inconsistency of
                  an open encyclopaedic corpus such as Wikipedia, makes sometimes difficult establishing
                  a relationship between its entities and a restricted taxonomy. In this article we
                  explore this problem and propose a complete system to meet this need.
               </p>
               <hr>
               <h4 id="taln-2009-long-008">Étude quantitative de liens entre l’analogie formelle et la morphologie constructionnelle</h4>
               			Auteur : Philippe Langlais -
               			Contact : felipe@iro.umontreal.ca<br><p>Plusieurs travaux ont récemment étudié l’apport de l’apprentissage analogique dans
                  des applications du traitement automatique des langues comme la traduction automatique,
                  ou la recherche d’information. Il est souvent admis que les relations analogiques
                  de forme entre les mots capturent des informations de nature morphologique. Le but
                  de cette étude est de présenter une analyse des points de rencontre entre l’analyse
                  morphologique et les analogies de forme. C’est à notre connaissance la première étude
                  de ce type portant sur des corpus de grande taille et sur plusieurs langues. Bien
                  que notre étude ne soit pas dédiée à une tâche particulière du traitement des langues,
                  nous montrons cependant que le principe d’analogie permet de segmenter des mots en
                  morphèmes avec une bonne précision.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Several studies recently showed the interest of analogical learning for Natural Language
                  processing tasks such as Machine Translation and Information Retrieval. It is often
                  admitted that formal analogies between words capture morphological information. The
                  purpose of this study os to quantify the correlations between morphological analysis
                  and formal analogies. This is to our knowledge the first attempt to conduct such a
                  quantitative analysis on large datasets and for several languages. Although this paper
                  was not geared toward tackling a specific natural language processing task, we show
                  that segmenting a word token into morphemes can be accomplished with a good precision
                  by a simple strategy relying solely on formal analogy.
               </p>
               <hr>
               <h4 id="taln-2009-long-009">Exploitation d’un corpus bilingue pour la création d’un système de traduction probabiliste
                  Vietnamien - Français
               </h4>
               			Auteur : Thi-Ngoc-Diep Do -
               			Contact : thi-ngoc-diep.do@imag.fr<br>
               			Auteur : Viet-Bac Le<br>
               			Auteur : Brigitte Bigi<br>
               			Auteur : Laurent Besacier<br>
               			Auteur : Eric Castelli<br><p>Cet article présente nos premiers travaux en vue de la construction d’un système de
                  traduction probabiliste pour le couple de langue vietnamien-français. La langue vietnamienne
                  étant considérée comme une langue peu dotée, une des difficultés réside dans la constitution
                  des corpus parallèles, indispensable à l’apprentissage des modèles. Nous nous concentrons
                  sur la constitution d’un grand corpus parallèle vietnamien-français. La méthode d’identification
                  automatique des paires de documents parallèles fondée sur la date de publication,
                  les mots spéciaux et les scores d’alignements des phrases est appliquée. Cet article
                  présente également la construction d’un premier système de traduction automatique
                  probabiliste vietnamienfrançais et français-vietnamien à partir de ce corpus et discute
                  l’opportunité d’utiliser des unités lexicales ou sous-lexicales pour le vietnamien
                  (syllabes, mots, ou leurs combinaisons). Les performances du système sont encourageantes
                  et se comparent avantageusement à celles du système de Google.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents our first attempt at constructing a Vietnamese-French statistical
                  machine translation system. Since Vietnamese is considered as an under-resourced language,
                  one of the difficulties is building a large Vietnamese-French parallel corpus, which
                  is indispensable to train the models. We concentrate on building a large Vietnamese-French
                  parallel corpus. The document alignment method based on publication date, special
                  words and sentence alignment result is applied. The paper also presents an application
                  of the obtained parallel corpus to the construction of a Vietnamese-French statistical
                  machine translation system, where the use of different units for Vietnamese (syllables,
                  words, or their combinations) is discussed. The performance of the system is encouraging
                  and it compares favourably to that of Google Translate.
               </p>
               <hr>
               <h4 id="taln-2009-long-010">Influence des points d’ancrage pour l’extraction lexicale bilingue à partir de corpus
                  comparables spécialisés
               </h4>
               			Auteur : Emmanuel Prochasson -
               			Contact : emmanuel.prochasson@univ-nantes.fr<br>
               			Auteur : Emmanuel Morin -
               			Contact : emmanuel.morin@univ-nantes.fr<br><p>L’extraction de lexiques bilingues à partir de corpus comparables affiche de bonnes
                  performances pour des corpus volumineux mais chute fortement pour des corpus d’une
                  taille plus modeste. Pour pallier cette faiblesse, nous proposons une nouvelle contribution
                  au processus d’alignement lexical à partir de corpus comparables spécialisés qui vise
                  à renforcer la significativité des contextes lexicaux en s’appuyant sur le vocabulaire
                  spécialisé du domaine étudié. Les expériences que nous avons réalisées en ce sens
                  montrent qu’une meilleure prise en compte du vocabulaire spécialisé permet d’améliorer
                  la qualité des lexiques extraits.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Bilingual lexicon extraction from comparable corpora gives good results for large
                  corpora but drops significantly for small size corpora. In order to compensate this
                  weakness, we suggest a new contribution dedicated to the lexical alignment from specialized
                  comparable corpora that strengthens the representativeness of the lexical contexts
                  based on domainspecific vocabulary. The experiments carried out in this way show that
                  taking better account the specialized vocabulary induces a significant improvement
                  in the quality of extracted lexicons.
               </p>
               <hr>
               <h4 id="taln-2009-long-011">Intégration de l’alignement de mots dans le concordancier bilingue TransSearch</h4>
               			Auteur : Stéphane Huet -
               			Contact : huetstep@iro.umontreal.ca<br>
               			Auteur : Julien Bourdaillet -
               			Contact : bourdaij@iro.umontreal.ca<br>
               			Auteur : Philippe Langlais -
               			Contact : felipe@iro.umontreal.ca<br><p>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction
                  assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie
                  de qualité est recherchée. Dans cet article, nous présentons nos travaux menés dans
                  le but d’améliorer le concordancier bilingue TransSearch. Ce service, accessible sur
                  le Web, repose principalement sur un alignement au niveau des phrases. Dans cette
                  étude, nous discutons et évaluons l’intégration d’un alignement statistique au niveau
                  des mots. Nous présentons deux nouvelles problématiques essentielles au succès de
                  notre nouveau prototype : la détection des traductions erronées et le regroupement
                  des variantes de traduction similaires.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Despite the impressive amount of recent studies devoted to improving the state of
                  the art of machine translation, computer assisted translation tools remain the preferred
                  solution of human translators when publication quality is of concern. In this paper,
                  we present our ongoing efforts conducted within a project which aims at improving
                  the commercial bilingual concordancer TransSearch. The core technology of this Web-based
                  service mainly relies on sentence-level alignment. In this study, we discuss and evaluate
                  the embedding of statistical word-level alignment. Two novel issues that are essential
                  to the success of our new prototype are tackled: detecting erroneous translations
                  and grouping together similar translations.
               </p>
               <hr>
               <h4 id="taln-2009-long-012">Jugements d'évaluation et constituants périphériques</h4>
               			Auteur : Agata Jackiewicz -
               			Contact : Agata.Jackiewicz@paris-sorbonne.fr<br>
               			Auteur : Thierry Charnois -
               			Contact : Thierry.Charnois@info.unicaen.fr<br>
               			Auteur : Stéphane Ferrari -
               			Contact : Stephane.Ferrari@info.unicaen.fr<br><p>L’article présente une étude portant sur des constituants détachés à valeur axiologique.
                  Dans un premier temps, une analyse linguistique sur corpus met en évidence un ensemble
                  de patrons caractéristiques du phénomène. Ensuite, une expérimentation informatique
                  est proposée sur un corpus de plus grande taille afin de permettre l’observation des
                  patrons en vue d’un retour sur le modèle linguistique. Ce travail s’inscrit dans un
                  projet mené à l’interface de la linguistique et du TAL, qui se donne pour but d’enrichir,
                  d’adapter au français et de formaliser le modèle général Appraisal de l’évaluation
                  dans la langue.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present a study about peripheral constituent expressing some axiological
                  value. First, a linguistic corpus analysis highlights some characteristic patterns
                  for this phenomenon. Then, a computational experiment is carried out on a larger corpus
                  in order to enable the observation of these patterns and to get a feedback on the
                  linguistic model. This work takes part in a project at the intersection of Linguistics
                  and NLP, which aims at enhancing, adapting to French language and formalizing the
                  Appraisal generic model of the evaluation in language.
               </p>
               <hr>
               <h4 id="taln-2009-long-013">Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise
                  de décision en unité néonatale
               </h4>
               			Auteur : François Portet -
               			Contact : portet@imag.fr<br>
               			Auteur : Albert Gatt -
               			Contact : a.gatt@abdn.ac.uk<br>
               			Auteur : Jim Hunter -
               			Contact : j.hunter@abdn.ac.uk<br>
               			Auteur : Ehud Reiter -
               			Contact : e.reiter@abdn.ac.uk<br>
               			Auteur : Somayajulu Sripada -
               			Contact : yaji.sripada@abdn.ac.uk<br><p>Notre société génère une masse d’information toujours croissante, que ce soit en médecine,
                  en météorologie, etc. La méthode la plus employée pour analyser ces données est de
                  les résumer sous forme graphique. Cependant, il a été démontré qu'un résumé textuel
                  est aussi un mode de présentation efficace. L'objectif du prototype BT-45, développé
                  dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux
                  physiologiques continus et d'événements temporels discrets en unité néonatale de soins
                  intensifs (NICU). L'article présente l'aspect génération de texte de ce prototype.
                  Une expérimentation clinique a montré que les résumés humains améliorent la prise
                  de décision par rapport à l'approche graphique, tandis que les textes de BT-45 donnent
                  des résultats similaires à l’approche graphique. Une analyse a identifié certaines
                  des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu'il est
                  possible de produire automatiquement des résumés textuels efficaces de données complexes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Nowadays large amount of data is produced every day in medicine, meteorology and other
                  areas and the most common approach to analyse such data is to present it graphically.
                  However, it has been shown that textual summarisation is also an effective approach.
                  As part of the BabyTalk project, the prototype BT-45 was developed to generate summaries
                  of 45 minutes of continuous physiological signals and discrete temporal events in
                  a neonatal intensive care unit (NICU). The paper presents its architecture with an
                  emphasis on its natural language generation part. A clinical experiment showed that
                  human textual summaries led to better decision making than graphical presentation,
                  whereas BT-45 texts led to similar results as visualisations. An analysis identified
                  some of the reasons for the BT-45 texts inferiority, but, despite these deficiencies,
                  our work shows that it is possible for computer systems to generate effective textual
                  summaries of complex data.
               </p>
               <hr>
               <h4 id="taln-2009-long-014">Les adjectifs relationnels dans les lexiques informatisés : formalisation et exploitation
                  dans un contexte multilingue
               </h4>
               			Auteur : Bruno Cartoni -
               			Contact : cartonib@gmail.com<br><p>Dans cet article, nous nous intéressons aux adjectifs dits relationnels et à leur
                  statut en traitement automatique des langues naturelles (TALN). Nous montrons qu’ils
                  constituent une « sous-classe » d’adjectifs rarement explicitée et donc rarement représentée
                  dans les lexiques sur lesquels reposent les applications du TALN, alors qu’ils jouent
                  un rôle important dans de nombreuses applications. Leur formation morphologique est
                  source d’importantes divergences entre différentes langues, et c’est pourquoi ces
                  adjectifs sont un véritable défi pour les applications informatiques multilingues.
                  Dans une partie plus pratique, nous proposons une formalisation de ces adjectifs permettant
                  de rendre compte de leurs liens avec leur base nominale. Nous tentons d’extraire ces
                  informations dans les lexiques informatisés existants, puis nous les exploitons pour
                  traduire les adjectifs relationnels préfixés de l’italien en français.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article focuses on a particular type of adjectives, relational adjectives, and
                  especially on the way they are processed in natural language processing systems. We
                  show that this class of adjectives is barely recorded in an explicit manner in computer
                  lexicons. There is an important discrepancy in the way those adjectives are morphologically
                  constructed in different languages, and therefore, they are a real challenge for multilingual
                  computing applications. On a more practical side, we propose a formalisation for the
                  adjectives that shows their semantic link with their nominal base. We make an attempt
                  to extract this kind of information in existing machine lexica, and we exploit their
                  semantic links in the translation of prefixed relational adjectives from Italian into
                  French.
               </p>
               <hr>
               <h4 id="taln-2009-long-015">Motifs séquentiels pour l’extraction d’information : illustration sur le problème
                  de la détection d’interactions entre gènes
               </h4>
               			Auteur : Marc Plantevit -
               			Contact : Marc.Plantevit@info.unicaen.fr<br>
               			Auteur : Thierry Charnois -
               			Contact : Thierry.Charnois@info.unicaen.fr<br><p>Face à la prolifération des publications en biologie et médecine (plus de 18 millions
                  de publications actuellement recensées dans PubMed), l’extraction d’information automatique
                  est devenue un enjeu crucial. Il existe de nombreux travaux dans le domaine du traitement
                  de la langue appliquée à la biomédecine ("BioNLP"). Ces travaux se distribuent en
                  deux grandes tendances. La première est fondée sur les méthodes d’apprentissage automatique
                  de type numérique qui donnent de bons résultats mais ont un fonctionnement de type
                  "boite noire". La deuxième tendance est celle du TALN à base d’analyses (lexicales,
                  syntaxiques, voire sémantiques ou discursives) coûteuses en temps de développement
                  des ressources nécessaires (lexiques, grammaires, etc.). Nous proposons dans cet article
                  une approche basée sur la découverte de motifs séquentiels pour apprendre automatiquement
                  les ressources linguistiques, en l’occurrence les patrons linguistiques qui permettent
                  l’extraction de l’information dans les textes. Plusieurs aspects méritent d’être soulignés
                  : cette approche permet de s’affranchir de l’analyse syntaxique de la phrase, elle
                  ne nécessite pas de ressources en dehors du corpus d’apprentissage et elle ne demande
                  que très peu d’intervention manuelle. Nous illustrons l’approche sur le problème de
                  la détection d’interactions entre gènes et donnons les résultats obtenus sur des corpus
                  biologiques qui montrent l’intérêt de ce type d’approche.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The proliferation of publications in biology andmedicine (more than 18million publications
                  currently listed in PubMed) has lead to the crucial need of automatic information
                  extraction. There are many work in the field of natural language processing applied
                  to biomedicine (BioNLP). Two types of approaches tackle this problem. On the one hand,
                  machine learning based approaches give good results but run as a "black box". On the
                  second hand, NLP based approaches are highly time consuming for developing the resources
                  (lexicons, grammars, etc.). In this paper, we propose an approach based on sequential
                  pattern mining to automatically discover linguistic patterns that allow the information
                  extraction in texts. This approach allows to overcome sentence parsing and it does
                  not require resources outside the training data set. We illustrate the approach on
                  the problem of detecting interactions between genes and give the results obtained
                  on biological corpora that show the relevance of this type of approach.
               </p>
               <hr>
               <h4 id="taln-2009-long-016">Prise en compte de dépendances syntaxiques pour la traduction contextuelle de segments</h4>
               			Auteur : Aurélien Max -
               			Contact : aurelien.max@limsi.fr<br>
               			Auteur : Rafik Maklhoufi -
               			Contact : rafik.makhloufi@utt.fr<br>
               			Auteur : Philippe Langlais -
               			Contact : felipe@iro.umontreal.ca<br><p>Dans un système standard de traduction statistique basé sur les segments, le score
                  attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel
                  il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte
                  le contexte source lors de la traduction, mais ces études portent sur des systèmes
                  traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons
                  nos expériences sur la prise en compte du contexte source dans un système statistique
                  traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa
                  et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information
                  contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques
                  d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs
                  de notre système par rapport à un système à l’état de l’art ne faisant pas usage du
                  contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement
                  est en faveur de notre système. Cette évaluation fait également ressortir que la prise
                  en compte de certaines dépendances syntaxiques est bénéfique à notre système.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In standard phrase-based Statistical Machine Translation (PBSMT) systems, the score
                  associated with each translation of a phrase does not depend on its context. While
                  several works have shown the potential gain of exploiting source context, they all
                  considered English, a morphologically poor language, as the target language. In this
                  article, we describe experiments on exploiting the source context in an English -&gt;
                  French PBSMT system, inspired by the work of Stroppa et al. (2007). We report a study
                  on the impact of various types of features that capture contextual information, including
                  syntactic dependencies. While automatic metrics do not show significative gains relative
                  to a baseline system, a manual evaluation of 100 randomly selected sentences concludes
                  that our context-aware system performs consistently better. This evaluation also shows
                  that some types of syntactic dependencies can participate to the gains observed.
               </p>
               <hr>
               <h4 id="taln-2009-long-017">Proposition de caractérisation et de typage des expressions temporelles en contexte</h4>
               			Auteur : Maud Ehrmann -
               			Contact : Maud.Ehrmann@xrce.xerox.com<br>
               			Auteur : Caroline Hagège -
               			Contact : Caroline.Hagege@xrce.xerox.com<br><p>Nous assistons actuellement en TAL à un regain d’intérêt pour le traitement de la
                  temporalité véhiculée par les textes. Dans cet article, nous présentons une proposition
                  de caractérisation et de typage des expressions temporelles tenant compte des travaux
                  effectués dans ce domaine tout en cherchant à pallier les manques et incomplétudes
                  de certains de ces travaux. Nous explicitons comment nous nous situons par rapport
                  à l’existant et les raisons pour lesquelles parfois nous nous en démarquons. Le typage
                  que nous définissons met en évidence de réelles différences dans l’interprétation
                  et le mode de résolution référentielle d’expressions qui, en surface, paraissent similaires
                  ou identiques. Nous proposons un ensemble des critères objectifs et linguistiquement
                  motivés permettant de reconnaître, de segmenter et de typer ces expressions. Nous
                  verrons que cela ne peut se réaliser sans considérer les procès auxquels ces expressions
                  sont associées et un contexte parfois éloigné.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Temporal processing in texts is a topic of renewed interest in NLP. In this paper
                  we present a new way of typing temporal expressions that takes into account both the
                  state of the art of this domain and that also tries to be more precise and accurate
                  that some of the current proposals. We explain into what extent our proposal is compatible
                  and comparable with the state-of-the art and why sometimes we stray from it. The typing
                  system that we define highlights real differences in the interpretation and reference
                  calculus of these expressions. At the same time, by offering objective criteria, it
                  fulfils the necessity of high inter-agreement between annotators. After having defined
                  what we consider as temporal expressions, we will show that tokenization, characterization
                  and typing of those expressions can only be done having into account processes to
                  which these expressions are linked.
               </p>
               <hr>
               <h4 id="taln-2009-long-018">Quel indice pour mesurer l'efficacité en segmentation de textes?</h4>
               			Auteur : Yves Bestgen -
               			Contact : yves.bestgen@psp.ucl.ac.be<br><p>L'évaluation de l'efficacité d'algorithmes de segmentation thématique est généralement
                  effectuée en quantifiant le degré d'accord entre une segmentation hypothétique et
                  une segmentation de référence. Les indices classiques de précision et de rappel étant
                  peu adaptés à ce domaine, WindowDiff (Pevzner, Hearst, 2002) s'est imposé comme l'indice
                  de référence. Une analyse de cet indice montre toutefois qu'il présente plusieurs
                  limitations. L'objectif de ce rapport est d'évaluer un indice proposé par Bookstein,
                  Kulyukin et Raita (2002), la distance de Hamming généralisée, qui est susceptible
                  de remédier à celles-ci. Les analyses montrent que celui-ci conserve tous les avantages
                  de WindowDiff sans les limitations. De plus, contrairement à WindowDiff, il présente
                  une interprétation simple puisqu'il correspond à une vraie distance entre les deux
                  segmentations à comparer.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The evaluation of thematic segmentation algorithms is generally carried out by quantifying
                  the degree of agreement between a hypothetical segmentation and a gold standard. The
                  traditional indices of precision and recall being little adapted to this field, WindowDiff
                  (Pevzner, Hearst, 2002) has become the standard for this kind of assessment. An analysis
                  of this index shows however that it presents several limitations. The objective of
                  this report is to evaluate an index developed by Bookstein, Kulyukin and Raita (2002),
                  the Generalized Hamming Distance, which is likely to overcome these limitations. The
                  analyzes show that it preserves all the advantages of WindowDiff without its limitations.
                  Moreover, contrary to WindowDiff, it presents a simple interpretation since it corresponds
                  to a true distance between the two segmentations.
               </p>
               <hr>
               <h4 id="taln-2009-long-019">Repérer automatiquement les segments obsolescents à l’aide d’indices sémantiques et
                  discursifs
               </h4>
               			Auteur : Marion Laignelet -
               			Contact : marion.laignelet@univ-tlse2.fr<br>
               			Auteur : François Rioult -
               			Contact : Francois.Rioult@info.unicaen.fr<br><p>Cet article vise la description et le repérage automatique des segments d’obsolescence
                  dans les documents de type encyclopédique. Nous supposons que des indices sémantiques
                  et discursifs peuvent permettre le repérage de tels segments. Pour ce faire, nous
                  travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons
                  des indices repérés automatiquement. Les techniques statistiques de base ne permettent
                  pas d’expliquer ce phénomène complexe. Nous proposons l’utilisation de techniques
                  de fouille de données pour le caractériser et nous évaluons le pouvoir prédictif de
                  nos indices. Nous montrons, à l’aide de techniques de classification supervisée et
                  de calcul de l’aire sous la courbe ROC, que nos hypothèses sont pertinentes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper deals with the description and automatic tracking of obsolescence in encyclopedic
                  type of documents. We suppose that semantic and discursive cues may allow the tracking
                  of these segments. For that purpose, we have worked on an expert manually annotated
                  corpus, on which we have projected automatically tracked cues. Basic statistic techniques
                  can not account for this complex phenomenon. We propose the use of techniques of data
                  mining to characterize it, and we evaluate the predictive power of our cues. We show,
                  using techniques of supervised classification and area under the ROC curve, that our
                  hypotheses are relevant.
               </p>
               <hr>
               <h4 id="taln-2009-long-020">Résumé automatique de textes d’opinions</h4>
               			Auteur : Michel Généreux -
               			Contact : Michel.Genereux@lipn.univ-paris13.fr<br>
               			Auteur : Aurélien Bossard -
               			Contact : Aurelien.Bossard@lipn.univ-paris13.fr<br><p>Le traitement des langues fait face à une demande croissante en matière d’analyse
                  de textes véhiculant des critiques ou des opinions. Nous présentons ici un système
                  de résumé automatique tourné vers l’analyse d’articles postés sur des blogues, où
                  sont exprimées à la fois des informations factuelles et des prises de position sur
                  les faits considérés. Nous montrons qu’une approche classique à base de traits de
                  surface est tout à fait efficace dans ce cadre. Le système est évalué à travers une
                  participation à la campagne d’évaluation internationale TAC (Text Analysis Conference)
                  où notre système a réalisé des performances satisfaisantes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>There is currently a growing need concerning the analysis of texts expressing opinions
                  or judgements. In this paper, we present a summarization system that is specifically
                  designed to process blog posts, where factual information is mixed with opinions.
                  We show that a classical approach based on surface cues is efficient to summarize
                  this kind of texts. The system is evaluated through a participation to TAC (Text Analysis
                  Conference), an international evaluation framework for automatic summarization, in
                  which our system obtained good results.
               </p>
               <hr>
               <h4 id="taln-2009-long-021">Sens, synonymes et définitions</h4>
               			Auteur : Ingrid Falk -
               			Contact : ingrid.falk@loria.fr<br>
               			Auteur : Claire Gardent -
               			Contact : claire.gardent@loria.fr<br>
               			Auteur : Evelyne Jacquey -
               			Contact : evelyne.jacquey@atilf.fr<br>
               			Auteur : Fabienne Venant -
               			Contact : fabienne.venant@loria.fr<br><p>Cet article décrit une méthodologie visant la réalisation d’une ressource sémantique
                  en français centrée sur la synonymie. De manière complémentaire aux travaux existants,
                  la méthode proposée n’a pas seulement pour objectif d’établir des liens de synonymie
                  entre lexèmes, mais également d’apparier les sens possibles d’un lexème avec les ensembles
                  de synonymes appropriés. En pratique, les sens possibles des lexèmes proviennent des
                  définitions du TLFi et les synonymes de cinq dictionnaires accessibles à l’ATILF.
                  Pour évaluer la méthode d’appariement entre sens d’un lexème et ensemble de synonymes,
                  une ressource de référence a été réalisée pour 27 verbes du français par quatre lexicographes
                  qui ont spécifié manuellement l’association entre verbe, sens (définition TLFi) et
                  ensemble de synonymes. Relativement à ce standard étalon, la méthode d’appariement
                  affiche une F-mesure de 0.706 lorsque l’ensemble des paramètres est pris en compte,
                  notamment la distinction pronominal / non-pronominal pour les verbes du français et
                  de 0.602 sans cette distinction.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present a method for grouping the synonyms of a word into sets representing the
                  possible meanings of that word. The possible meanings are given by the definitions
                  of a general dictionary for French, the TLFi (Trésor de la langue française informatisé)
                  and the method is applied to the synonyms of 5 synonym dictionnaries. To evaluate
                  the method, we manually constructed a gold standard where for each (word, definition)
                  pair, 4 lexicographers specified the set of synonyms they judge adequate. The method
                  scores an F-measure of 0.602 when no distinction is made between pronominal and non-pronominal
                  use and 0.706 when it is.
               </p>
               <hr>
               <h4 id="taln-2009-long-022">Vers des contraintes plus linguistiques en résolution de coréférences</h4>
               			Auteur : Étienne Ailloud -
               			Contact : ailloud@cl.uzh.ch<br>
               			Auteur : Manfred Klenner -
               			Contact : klenner@cl.uzh.ch<br><p>Nous proposons un modèle filtrant de résolution de coréférences basé sur les notions
                  de transitivité et d’exclusivité linguistique. À partir de l’hypothèse générale que
                  les chaînes de coréférence demeurent cohérentes tout au long d’un texte, notre modèle
                  assure le respect de certaines contraintes linguistiques (via des filtres) quant à
                  la coréférence, ce qui améliore la résolution globale. Le filtrage a lieu à différentes
                  étapes de l’approche standard (c-à-d. par apprentissage automatique), y compris avant
                  l’apprentissage et avant la classification, accélérant et améliorant ce processus.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a filter model of coreference resolution that is based on the notions of
                  transitivity and linguistic exclusivity. Starting from the general assumption that
                  coreference sets remain coherent throughout a text, our model enforces the checking
                  of some compatibility criteria (filters) between coreference candidates, thereby improving
                  resolution performance. This filtering is achieved at different stages of the workflow
                  of machine-learning-based coreference resolution, including at the standard learning
                  and testing steps, where it may help reduce the computational load and better distribute
                  the actual occurrences to be learned.
               </p>
               <hr>
               <h4 id="taln-2009-long-023">Trouver et confondre les coupables : un processus sophistiqué de correction de lexique</h4>
               			Auteur : Lionel Nicolas -
               			Contact : lnicolas@i3s.unice.fr<br>
               			Auteur : Benoît Sagot -
               			Contact : benoit.sagot@inria.fr<br>
               			Auteur : Miguel A. Molinero -
               			Contact : mmolinero@udc.es<br>
               			Auteur : Jacques Farré -
               			Contact : jf@i3s.unice.fr<br>
               			Auteur : Éric de La Clergerie -
               			Contact : Eric.De_La_Clergerie@inria.fr<br><p>La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et du lexique
                  sur lequel il repose. Le développement d’un lexique complet et précis est une tâche
                  ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de
                  qualité et de couverture. Dans cet article, nous présentons un processus capable de
                  détecter automatiquement les entrées manquantes ou incomplètes d’un lexique, et de
                  suggérer des corrections pour ces entrées. La détection se réalise au moyen de deux
                  techniques reposant soit sur un modèle statistique, soit sur les informations fournies
                  par un étiqueteur syntaxique. Les hypothèses de corrections pour les entrées lexicales
                  détectées sont générées en étudiant les modifications qui permettent d’améliorer le
                  taux d’analyse des phrases dans lesquelles ces entrées apparaissent. Le processus
                  global met en oeuvre plusieurs techniques utilisant divers outils tels que des étiqueteurs
                  et des analyseurs syntaxiques ou des classifieurs d’entropie. Son application au Lefff
                  , un lexique morphologique et syntaxique à large couverture du français, nous a déjà
                  permis de réaliser des améliorations notables.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The coverage of a parser depends mostly on the quality of the underlying grammar and
                  lexicon. The development of a lexicon both complete and accurate is an intricate and
                  demanding task, overall when achieving a certain level of quality and coverage. We
                  introduce an automatic process able to detect missing or incomplete entries in a lexicon,
                  and to suggest corrections hypotheses for these entries. The detection of dubious
                  lexical entries is tackled by two techniques relying either on a specific statistical
                  model, or on the information provided by a part-of-speech tagger. The generation of
                  correction hypotheses for the detected entries is achieved by studying which modifications
                  could improve the parse rate of the sentences in which the entries occur. This process
                  brings together various techniques based on different tools such as taggers, parsers
                  and entropy classifiers. Applying it on the Lefff , a large-coverage morphological
                  and syntactic French lexicon, has already allowed us to perfom noticeable improvements.
               </p>
               <hr>
               <h4 id="taln-2009-long-024">Un analyseur de surface non déterministe pour le français</h4>
               			Auteur : François Trouilleux<br><p>Les analyseurs syntaxiques de surface à base de règles se caractérisent par un processus
                  en deux temps : désambiguïsation lexicale, puis reconnaissance de patrons. Considérant
                  que ces deux étapes introduisent une certaine redondance dans la description linguistique
                  et une dilution des heuristiques dans les différents processus, nous proposons de
                  définir un analyseur de surface qui fonctionne sur une entrée non désambiguïsée et
                  produise l’ensemble des analyses possibles en termes de syntagmes noyau (chunks).
                  L’analyseur, implanté avec NooJ, repose sur la définition de patrons étendus qui annotent
                  des séquences de syntagmes noyau. Les résultats obtenus sur un corpus de développement
                  d’environ 22 500 mots, avec un rappel proche de 100 %, montrent la faisabilité de
                  l’approche et signalent quelques points d’ambiguïté à étudier plus particulièrement
                  pour améliorer la précision.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Rule-based chunkers are characterized by a two-tier process : part-of-speech disambiguation,
                  and pattern matching. Considering that these two stages introduce some redundancy
                  in the linguistic description and a dilution of heuristics over the different processes,
                  we propose to define a chunker which parses a non-disambiguated input, and produces
                  all possible analysis in terms of chunks. The parser, implemented with NooJ, relies
                  on the definition of extended patterns, which annotate sequences of chunks. The results
                  obtained on an approx. 22500 word corpus, with almost 100 % recall, demonstrate the
                  feasability of the approach, and signal which ambiguities should be further studied
                  in order to improve precision.
               </p>
               <hr>
               <h4 id="taln-2009-long-025">Une approche mixte-statistique et structurelle - pour le résumé automatique de dépêches</h4>
               			Auteur : Aurélien Bossard -
               			Contact : aurelien.bossard@lipn.univ-paris13.fr<br><p>Les techniques de résumé automatique multi-documents par extraction ont récemment
                  évolué vers des méthodes statistiques pour la sélection des phrases à extraire. Dans
                  cet article, nous présentons un système conforme à l’« état de l’art » — CBSEAS —
                  que nous avons développé pour les tâches Opinion (résumés d’opinions issues de blogs)
                  et Update (résumés de dépêches et mise à jour du résumé à partir de nouvelles dépêches
                  sur le même événement) de la campagne d’évaluation TAC 2008, et montrons l’intérêt
                  d’analyses structurelles et linguistiques des documents à résumer. Nous présentons
                  également notre étude sur la structure des dépêches et l’impact de son intégration
                  à CBSEAS.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Automatic multi-document summarization techniques have recently evolved into statistical
                  methods for selecting the sentences that will be used to generate the summary. In
                  this paper, we present a system in accordance with « State-of-the-art » — CBSEAS —
                  that we have developped for the « Opinion Task » (automatic summaries of opinions
                  from blogs) and the « Update Task » (automatic summaries of newswire articles and
                  information update) of the TAC 2008 evaluation campaign, and show the interest of
                  structural and linguistic analysis of the documents to summarize .We also present
                  our study on news structure and its integration to CBSEAS impact.
               </p>
               <hr>
               <h4 id="taln-2009-long-026">Une expérience de fusion pour l’annotation d'entités nommées</h4>
               			Auteur : Caroline Brun -
               			Contact : Caroline.Brun@xrce.xerox.com<br>
               			Auteur : Nicolas Dessaigne -
               			Contact : Nicolas.Dessaigne@arisem.com<br>
               			Auteur : Maud Ehrmann -
               			Contact : Maud.Ehrmann@xrce.xerox.com<br>
               			Auteur : Baptiste Gaillard -
               			Contact : Baptiste.gaillard@fr.thalesgroup.com<br>
               			Auteur : Sylvie Guillemin-Lanne -
               			Contact : sylvie.guillemin-lanne@temis.com<br>
               			Auteur : Guillaume Jacquet -
               			Contact : Guillaume.Jacquet@xrce.xerox.com<br>
               			Auteur : Aaron Kaplan -
               			Contact : Aaron.Kaplan@xrce.xerox.com<br>
               			Auteur : Marianna Kucharski -
               			Contact : marianna.kucharski@temis.com<br>
               			Auteur : Claude Martineau -
               			Contact : claude.martineau@univ-mlv.fr<br>
               			Auteur : Aurélie Migeotte -
               			Contact : Aurelie.Migeotte@arisem.com<br>
               			Auteur : Takuya Nakamura -
               			Contact : takuya.nakamura@univ-mlv.fr<br>
               			Auteur : Stavroula Voyatzi -
               			Contact : stavroula.voyatzi@univ-mlv.fr<br><p>Nous présentons une expérience de fusion d’annotations d’entités nommées provenant
                  de différents annotateurs. Ce travail a été réalisé dans le cadre du projet Infom@gic,
                  projet visant à l’intégration et à la validation d’applications opérationnelles autour
                  de l’ingénierie des connaissances et de l’analyse de l’information, et soutenu par
                  le pôle de compétitivité Cap Digital « Image, MultiMédia et Vie Numérique ». Nous
                  décrivons tout d’abord les quatre annotateurs d’entités nommées à l’origine de cette
                  expérience. Chacun d’entre eux fournit des annotations d’entités conformes à une norme
                  développée dans le cadre du projet Infom@gic. L’algorithme de fusion des annotations
                  est ensuite présenté ; il permet de gérer la compatibilité entre annotations et de
                  mettre en évidence les conflits, et ainsi de fournir des informations plus fiables.
                  Nous concluons en présentant et interprétant les résultats de la fusion, obtenus sur
                  un corpus de référence annoté manuellement.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present an experiment aimed at merging named entity annotations
                  provided by different annotators. This work has been performed as part of the Infom@gic
                  project, whose goal is the integration and validation of knowledge engineering and
                  information analysis applications, and which is supported by the pole of competitiveness
                  Cap Digital « Image, MultiMédia et Vie Numérique ». We first describe the four annotators,
                  which provide named entity annotations that conform to guidelines defined in the Infom@gic
                  project. Then we present an algorithm for merging the different annotations. It uses
                  information about the compatibility of various annotations and can point out conflicts,
                  and thus yields annotations that are more reliable than those of any single annotator.
                  We conclude by describing and interpreting the merging results obtained on a manually
                  annotated reference corpus.
               </p>
               <hr>
               <h4 id="taln-2009-long-027">Un système modulaire d’acquisition automatique de traductions à partir du Web</h4>
               			Auteur : Stéphanie Léon -
               			Contact : stephanie.leon@lirmm.fr<br><p>Nous présentons une méthode de Traduction Automatique d’Unités Lexicales Complexes
                  (ULC) pour la construction de ressources bilingues français/anglais, basée sur un
                  système modulaire qui prend en compte les propriétés linguistiques des unités sources
                  (compositionnalité, polysémie, etc.). Notre système exploite les différentes « facettes
                  » du Web multilingue pour valider des traductions candidates ou acquérir de nouvelles
                  traductions. Après avoir collecté une base d’ULC en français à partir d’un corpus
                  de pages Web, nous passons par trois phases de traduction qui s’appliquent à un cas
                  linguistique, avec une méthode adaptée : les traductions compositionnelles non polysémiques,
                  les traductions compositionnelles polysémiques et les traductions non compositionnelles
                  et/ou inconnues. Notre évaluation sur un vaste échantillon d’ULC montre que l’exploitation
                  du Web pour la traduction et la prise en compte des propriétés linguistiques au sein
                  d’un système modulaire permet une acquisition automatique de traductions avec une
                  excellente précision.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present a method of automatic translation (French/English) of Complex Lexical Units
                  (CLU) for aiming at extracting a bilingual lexicon. Our modular system is based on
                  linguistic properties (compositionality, polysemy, etc.). Different aspects of the
                  multilingual Web are used to validate candidate translations and collect new terms.
                  We first build a French corpus of Web pages to collect CLU. Three adapted processing
                  stages are applied for each linguistic property : compositional and non polysemous
                  translations, compositional polysemous translations and non compositional translations.
                  Our evaluation on a sample of CLU shows that our technique based on the Web can reach
                  a very high precision.
               </p>
               <hr>
               <h4 id="taln-2009-long-028">Des relations d’alignement pour décrire l’interaction des domaines linguistiques :
                  vers des Grammaires Multimodales
               </h4>
               			Auteur : Philippe Blache -
               			Contact : blache@lpl-aix.fr<br><p>Un des problèmes majeurs de la linguistique aujourd’hui réside dans la prise en compte
                  de phénomènes relevant de domaines et de modalités différentes. Dans la littérature,
                  la réponse consiste à représenter les relations pouvant exister entre ces domaines
                  de façon externe, en termes de relation de structure à structure, s’appuyant donc
                  sur une description distincte de chaque domaine ou chaque modalité. Nous proposons
                  dans cet article une approche différente permettant représenter ces phénomènes dans
                  un cadre formel unique, permettant de rendre compte au sein d’une même grammaire tous
                  les phénomènes concernés. Cette représentation précise de l’interaction entre domaines
                  et modalités s’appuie sur la définition de relations d’alignement.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Linguistics is now faced with the question of representing information coming from
                  different domains or modalities. The classical answer consists in representing separately
                  each of these domains, building for each of them an independent structure, and then
                  representing domain interaction in terms of relation between structures. We propose
                  n this paper a different approach in which all information is represented within a
                  unique and homogeneous framework, making it possible to represent into a same grammar
                  all interaction phenomena. This precise representation of interaction relies on the
                  definition of a new notion of alignment relations.
               </p>
               <hr>
               <h4 id="taln-2009-long-029">Vers une méthodologie d’annotation des entités nommées en corpus ?</h4>
               			Auteur : Karën Fort -
               			Contact : karen.fort@inist.fr<br>
               			Auteur : Maud Ehrmann -
               			Contact : maud.ehrmann@xrce.xerox.com<br>
               			Auteur : Adeline Nazarenko -
               			Contact : adeline.nazarenko@lipn.univ-paris13.fr<br><p>La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’entités nommées,
                  présente des difficultés spécifiques en matière d’annotation. Nous les précisons ici,
                  en les illustrant par des expériences d’annotation manuelle dans le domaine de la
                  microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce
                  que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions
                  pour cela les applications nécessitant l’extraction d’entités nommées et, en fonction
                  des besoins de ces applications, nous proposons de définir sémantiquement les éléments
                  à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques
                  permettant d’assurer un cadre d’annotation cohérent et évaluable.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Today, the named entity recognition task is considered as fundamental, but it involves
                  some specific difficulties in terms of annotation. We list them here, with illustrations
                  taken from manual annotation experiments in microbiology. Those issues lead us to
                  ask the fundamental question of what the annotators should annotate and, even more
                  important, for which purpose. We thus identify the applications using named entity
                  recognition and, according to the real needs of those applications, we propose to
                  semantically define the elements to annotate. Finally, we put forward a number of
                  methodological recommendations to ensure a coherent and reliable annotation scheme.
               </p>
               <hr>
               <h4 id="taln-2009-position-001">Données bilingues pour la TAS français-anglais : impact de la langue source et direction
                  de traduction originales sur la qualité de la traduction
               </h4>
               			Auteur : Sylwia Ozdowska -
               			Contact : sozdowska@computing.dcu.ie<br><p>Dans cet article, nous prenons position par rapport à la question de la qualité des
                  données bilingues destinées à la traduction automatique statistique en terme de langue
                  source et direction de traduction originales à l’égard d’une tâche de traduction français-anglais.
                  Nous montrons que l’entraînement sur un corpus contenant des textes qui ont été à
                  l’origine traduits du français vers l’anglais améliore la qualité de la traduction.
                  Inversement, l’entraînement sur un corpus contenant exclusivement des textes dont
                  la langue source originale n’est ni le français ni l’anglais dégrade la traduction.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we argue about the quality of bilingual data for statistical machine
                  translation in terms of the original source language and translation direction in
                  the context of a French to English translation task. We show that data containing
                  original French and English translated from French improves translation quality. Conversely,
                  using data comprising exclusively French and English translated from several other
                  languages results in a clear-cut decrease in translation quality.
               </p>
               <hr>
               <h4 id="taln-2009-position-002">La place de la désambiguïsation lexicale dans la Traduction Automatique Statistique</h4>
               			Auteur : Marianna Apidianaki -
               			Contact : mapidianaki@computing.dcu.ie<br><p>L’étape de la désambiguïsation lexicale est souvent esquivée dans les systèmes de
                  Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car considérée
                  comme non nécessaire à la sélection de traductions correctes. Le débat autour de cette
                  nécessité est actuellement assez vif. Dans cet article, nous présentons les principales
                  positions sur le sujet. Nous analysons les avantages et les inconvénients de la conception
                  actuelle de la désambiguïsation dans le cadre de la SMT, d’après laquelle les sens
                  des mots correspondent à leurs traductions dans des corpus parallèles. Ensuite, nous
                  présentons des arguments en faveur d’une analyse plus poussée des informations sémantiques
                  induites à partir de corpus parallèles et nous expliquons comment les résultats d’une
                  telle analyse pourraient être exploités pour une évaluation plus flexible et concluante
                  de l’impact de la désambiguïsation dans la SMT.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Word Sense Disambiguation (WSD) is often omitted in Statistical Machine Translation
                  (SMT) systems, as it is considered unnecessary for lexical selection. The discussion
                  on the need ofWSD is currently very active. In this article we present the main positions
                  on the subject. We analyze the advantages and weaknesses of the current conception
                  of WSD in SMT, according to which the senses of ambiguous words correspond to their
                  translations in a parallel corpus. Then we present some arguments towards a more thorough
                  analysis of the semantic information induced from parallel corpora and we explain
                  how the results of this analysis could be exploited for a more flexible and conclusive
                  evaluation of the impact of WSD on SMT.
               </p>
               <hr>
               <h4 id="taln-2009-position-003">Nouveau paradigme d’évaluation des systèmes de dialogue homme-machine</h4>
               			Auteur : Marianne Laurent<br>
               			Auteur : Ghislain Putois<br>
               			Auteur : Philippe Bretier<br>
               			Auteur : Thierry Moudenc<br><p>L’évaluation des systèmes de dialogue homme-machine est un problème difficile et pour
                  lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité.
                  Les approches ergonomiques traditionnelles soumettent le système de dialogue au regard
                  critique de l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre
                  objectivable des usages de ces utilisateurs empêche une comparaison entre systèmes
                  différents, ou entre évolutions d’un même système. Nous proposons d’inverser cette
                  vision et de mesurer le comportement de l’utilisateur au regard du système de dialogue.
                  Aussi, au lieu d’évaluer l’adéquation du système à ses utilisateurs, nous mesurons
                  l’adéquation des utilisateurs au système. Ce changement de paradigme permet un changement
                  de référentiel qui n’est plus les usages des utilisateurs mais le cadre du système.
                  Puisque le système est complètement défini, ce paradigme permet des approches quantitatives
                  et donc des évaluations comparatives de systèmes.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Evaluation of a human-machine dialogue system is a difficult problem for which neither
                  the objectives nor the proposed solutions gather a unanimous support. Traditional
                  approaches in the ergonomics field evaluate the system by describing how it fits the
                  user in the user referential of practices. However, the user referential is even more
                  complicated to formalise, and one cannot ground a common use context to enable the
                  comparison of two systems, even if they are merely an evolution of the same service.
                  We propose to shift the point of view on the evaluation problem : instead of evaluating
                  the system in interaction with the user in the user’s referential, we will now measure
                  the user’s adequacy to the system in the system referential. This is our Copernician
                  revolution : for the evaluation purpose, our system is no longer user-centric, because
                  the user referential is not properly objectifiable, while the system referential is
                  completely known by design.
               </p>
               <hr>
               <h4 id="taln-2009-demo-001">ACOLAD un environnement pour l’édition de corpus de dépendances</h4>
               			Auteur : Francis Brunet-Manquat -
               			Contact : Francis.Brunet-Manquat@imag.fr<br>
               			Auteur : Jérôme Goulian -
               			Contact : Jerome.Goulian@imag.fr<br><p>Dans cette démonstration, nous présentons le prototype d’un environnement open-source
                  pour l’édition de corpus de dépendances. Cet environnement, nommé ACOLAD (Annotation
                  de COrpus Linguistique pour l’Analyse de dépendances), propose des services manuels
                  de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes
                  minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des
                  chunks et annotation syntaxique des dépendances entre mots ou entre chunks).
               </p>
               <hr>
               <h4 id="taln-2009-demo-002">Amener des utilisateurs à créer et évaluer des paraphrases par le jeu</h4>
               			Auteur : Houda Bouamor -
               			Contact : Houda.Bouamor@limsi.fr<br>
               			Auteur : Aurélien Max -
               			Contact : Aurelien.Max@limsi.fr<br>
               			Auteur : Anne Vilnat -
               			Contact : Anne.Vilnat@limsi.fr<br><p>Dans cet article, nous présentons une application sur le web pour l’acquisition de
                  paraphrases phrastiques et sous-phrastiques sous forme de jeu. L’application permet
                  l’acquisition à la fois de paraphrases et de jugements humains multiples sur ces paraphrases,
                  ce qui constitue des données particulièrement utiles pour les applications du TAL
                  basées sur les phénomènes paraphrastiques.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this article, we present a web application presented as a game for acquiring sentencial
                  and phrasal paraphrases. It can be used both to acquire paraphrases and important
                  quantities of human evaluations of their quality. These are particularly useful for
                  NLP applications relying on paraphrasing.
               </p>
               <hr>
               <h4 id="taln-2009-demo-003">anymalign : un outil d’alignement sous-phrastique libre pour les êtres humains</h4>
               			Auteur : Adrien Lardilleux -
               			Contact : Adrien.Lardilleux@info.unicaen.fr<br>
               			Auteur : Yves Lepage -
               			Contact : Yves.Lepage@info.unicaen.fr<br><p>Nous présentons anymalign, un aligneur sous-phrastique grand public. Ses résultats
                  ont une qualité qui rivalise avec le meilleur outil du domaine, GIZA++. Il est rapide
                  et simple d’utilisation, et permet de produire dictionnaires et autres tables de traduction
                  en une seule commande. À notre connaissance, c’est le seul outil au monde permettant
                  d’aligner un nombre quelconque de langues simultanément. Il s’agit donc du premier
                  aligneur sousphrastique réellement multilingue.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present anymalign, a sub-sentential aligner oriented towards end users. It produces
                  results that are competitive with the best known tool in the domain, GIZA++. It is
                  fast and easy to use, and allows dictionaries or translation tables to be produced
                  in a single command. To our knowledge, it is the only tool in the world capable of
                  aligning any number of languages simultaneously. It is therefore the first truly multilingual
                  sub-sentential aligner.
               </p>
               <hr>
               <h4 id="taln-2009-demo-004">Apport des outils de TAL à la construction d’ontologies : propositions au sein de
                  la plateforme DaFOE
               </h4>
               			Auteur : Jean Charlet -
               			Contact : Jean.Charlet@spim.jussieu.fr<br>
               			Auteur : Sylvie Szulman -
               			Contact : Sylvie.Szulman@lipn.univ-paris13.fr<br>
               			Auteur : Nathalie Aussenac-Gilles<br>
               			Auteur : Adeline Nazarenko<br>
               			Auteur : Nathalie Hernandez<br>
               			Auteur : Nadia Nadah<br>
               			Auteur : Éric Sardet<br>
               			Auteur : Jean Delahousse<br>
               			Auteur : Guy Pierra<br><p>La construction d’ontologie à partir de textes fait l’objet d’études depuis plusieurs
                  années dans le domaine de l’ingénierie des ontologies. Un cadre méthodologique en
                  quatre étapes (constitution d’un corpus de documents, analyse linguistique du corpus,
                  conceptualisation, opérationnalisation de l’ontologie) est commun à la plupart des
                  méthodes de construction d’ontologies à partir de textes. S’il existe plusieurs plateformes
                  de traitement automatique de la langue (TAL) permettant d’analyser automatiquement
                  les corpus et de les annoter tant du point de vue syntaxique que statistique, il n’existe
                  actuellement aucune procédure généralement acceptée, ni a fortiori aucun ensemble
                  cohérent d’outils supports, permettant de concevoir de façon progressive, explicite
                  et traçable une ontologie de domaine à partir d’un ensemble de ressources informationnelles
                  relevant de ce domaine. Le but de ce court article est de présenter les propositions
                  développées, au sein du projet ANR DaFOE 4app, pour favoriser l’émergence d’un tel
                  ensemble d’outils.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The concept of ontologies, appeared in the nineties, constitute a key point to represent
                  and share the meaning carried out by formal symbols. Thus, the building of such an
                  ontology is quite difficult. A way to do so is to use preexistent elements (textual
                  corpus, taxonomies, norms or other ontologies) and operate them as a basis to define
                  the ontology field. However, there is neither accepted process nor set of tools to
                  progressively built ontologies from the available resources in a traceable and explicit
                  way. We report in this paper several propositions developed within the framework of
                  the ANR DaFOE4App project to support emergence of such tools.
               </p>
               <hr>
               <h4 id="taln-2009-demo-005">ASSIST : un moteur de recherche spécialisé pour l’analyse des cadres d’expériences</h4>
               			Auteur : Davy Weissenbacher -
               			Contact : Davy.Weissenbacher@manchester.ac.uk<br>
               			Auteur : Elisa Pieri -
               			Contact : Elisa.Pieri@manchester.ac.uk<br>
               			Auteur : Sophia Ananiadou -
               			Contact : Sophia.Ananiadou@manchester.ac.uk<br>
               			Auteur : Brian Rea -
               			Contact : Brian.Rea@manchester.ac.uk<br>
               			Auteur : Farida Vis -
               			Contact : Farida.Vis@manchester.ac.uk<br>
               			Auteur : Yuwei Lin -
               			Contact : Yuwei.Lin@manchester.ac.uk<br>
               			Auteur : Rob Procter -
               			Contact : Rob.Procter@manchester.ac.uk<br>
               			Auteur : Peter Halfpenny -
               			Contact : Peter.Halfpenny@manchester.ac.uk<br><p>L’analyse qualitative des données demande au sociologue un important travail de sélection
                  et d’interprétation des documents. Afin de faciliter ce travail, cette communauté
                  c’est dotée d’outils informatique mais leur fonctionnalités sont encore limitées.
                  Le projet ASSIST est une étude exploratoire pour préciser les modules de traitement
                  automatique des langues (TAL) permettant d’assister le sociologue dans son travail
                  d’analyse. Nous présentons le moteur de recherche réalisé et nous justifions le choix
                  des composants de TAL intégrés au prototype.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Qualitative data analysis requiers from the sociologist an important work of selection
                  and interpretation of the documents. To facilitate this work, several software have
                  been created but their functionalities are still limitated. The ASSIST project is
                  a preliminary work to define the natural language processing modules for helping the
                  sociologist. We present the search engine realised and justify the NLP modules integrated
                  in the prototype.
               </p>
               <hr>
               <h4 id="taln-2009-demo-006">CETLEF.fr - diagnostic automatique des erreurs de déclinaison tchèque dans un outil
                  ELAO
               </h4>
               			Auteur : Ivan Šmilauer -
               			Contact : smilauer@cetlef.fr<br><p>CETLEF.fr – une application Web dynamique – propose des exercices de déclinaison tchèque
                  avec un diagnostic automatique des erreurs. Le diagnostic a nécessité l'élaboration
                  d'un modèle formel spécifique de la déclinaison contenant un classement des types
                  paradigmatiques et des règles pour la réalisation des alternances morphématiques.
                  Ce modèle est employé pour l'annotation des formes requises, nécessaire pour le diagnostic,
                  mais également pour une présentation didactique sur la plateforme apprenant. Le diagnostic
                  est effectué par comparaison d'une production erronée avec des formes hypothétiques
                  générées à partir du radical de la forme requise et des différentes désinences casuelles.
                  S'il existe une correspondance, l'erreur est interprétée d'après les différences dans
                  les traits morphologiques de la forme requise et de la forme hypothétique. La majorité
                  des erreurs commises peut être interprétée à l'aide de cette technique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>CETLEF.fr – a dynamic Web application – contains fill-in-the-blank exercises on Czech
                  declension with an automatic error diagnosis. The diagnosis rendered necessary the
                  definition of a specific formal model of nominal inflection containing a classification
                  of the paradigms and the rules for the realization of morphemic alternations. This
                  model has been employed for the morphological annotation of required forms, necessary
                  for the error diagnosis as well as for a didactic presentation on the learning platform.
                  Diagnosis is carried out by the comparison of an erroneous production with hypothetical
                  forms generated from the radical of the required form and various haphazard endings.
                  If a correspondence is found, the error is interpreted according to the differences
                  in the morphological features of the required form and the hypothetical form. The
                  majority of errors can be interpreted with the aid of this technique.
               </p>
               <hr>
               <h4 id="taln-2009-demo-007">CIFLI-SurviTra, deux facettes : démonstrateur de composants de TA fondée sur UNL,
                  et phrasebook multilingue
               </h4>
               			Auteur : Georges Fafiotte -
               			Contact : georges.fafiotte@imag.fr<br>
               			Auteur : Achille Falaise -
               			Contact : achille.falaise@imag.fr<br>
               			Auteur : Jérôme Goulian -
               			Contact : jerome.goulian@imag.fr<br><p>CIFLI-SurviTra ("Survival Translation" assistant) est une plate-forme destinée à favoriser
                  l'ingénierie et la mise au point de composants UNL de TA, à partir d'une mémoire de
                  traduction formée de livres de phrases multilingues avec variables lexicales. SurviTra
                  est aussi un phrasebook digital multilingue, assistant linguistique pour voyageurs
                  monolingues (français, hindi, tamoul, anglais) en situation de "survie linguistique".
                  Le corpus d’un domaine-pilote ("Restaurant") a été structuré et construit : sous-domaines
                  de phrases alignées et classes lexicales de locutions quadrilingues, graphes UNL,
                  dictionnaires UW++/français et UW++/hindi par domaines. L’approche, générique, est
                  applicable à d’autres langues. Le prototype d’assistant linguistique (application
                  Web, à interface textuelle) peut évoluer vers une application UNL embarquée sur SmartPhone,
                  avec Traitement de Parole et multimodalité.
               </p>
               <hr>
               <h4 id="taln-2009-demo-008">Composition multilingue de sentiments</h4>
               			Auteur : Stefanos Petrakis -
               			Contact : petrakis@ifi.uzh.ch<br>
               			Auteur : Manfred Klenner -
               			Contact : klenner@ifi.uzh.ch<br>
               			Auteur : Étienne Ailloud -
               			Contact : ailloud@ifi.uzh.ch<br>
               			Auteur : Angela Fahrni -
               			Contact : angela.fahrni@swissonline.ch<br><p>Nous présentons ici PolArt, un outil multilingue pour l’analyse de sentiments qui
                  aborde la composition des sentiments en appliquant des transducteurs en cascade. La
                  compositionnalité est assurée au moyen de polarités préalables extraites d’un lexique
                  et des règles de composition appliquées de manière incrémentielle.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We introduce PolArt, a multilingual tool for sentiment detection that copes with sentiment
                  composition through the application of cascaded transducers. Compositionality is enabled
                  by prior polarities taken from a polarity lexicon and the compositional rules applied
                  incrementally.
               </p>
               <hr>
               <h4 id="taln-2009-demo-009">EXCOM : Plate-forme d'annotation sémantique de textes multilingues</h4>
               			Auteur : Motasem Alrahabi -
               			Contact : motasem.alrahabi@paris4.sorbonne.fr<br>
               			Auteur : Jean-Pierre Desclés -
               			Contact : jean-pierre.descles@paris-sorbonne.fr<br><p>Nous proposons une plateforme d‟annotation sémantique, appelée « EXCOM ». Basée sur
                  la méthode de l‟ « Exploration Contextuelle », elle permet, à travers une diversité
                  de langues, de procéder à des annotations automatiques de segments textuels par l'analyse
                  des formes de surface dans leur contexte. Les textes sont traités selon des « points
                  de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ».
                  L‟annotation se base sur un ensemble de règles linguistiques, écrites par un analyste,
                  qui permettent d‟identifier les représentations textuelles sous-jacentes aux différentes
                  catégories de la carte. Le système offre, à travers deux types d‟interfaces (développeur
                  ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la
                  segmentation, l‟annotation et d‟autres fonctionnalités de post-traitement. Les documents
                  annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d‟information,
                  de veille, de classification ou de résumé automatique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We propose a platform for semantic annotation, called “EXCOM”. Based on the “Contextual
                  Exploration” method, it enables, across a great range of languages, to perform automatic
                  annotations of textual segments by analyzing surface forms in their context. Texts
                  are approached through discursive “points of view”, of which values are organized
                  into a “semantic map”. The annotation is based on a set of linguistic rules, manually
                  constructed by an analyst, and that enables to automatically identify the textual
                  representations underlying the different semantic categories of the map. The system
                  provides through two sorts of user-friendly interfaces (analyst or end-user) a complete
                  pipeline of automatic text processing which consists of segmentation, annotation and
                  other post-processing functionalities. Annotated documents can be used, for instance,
                  for information retrieval systems, classification or automatic summarization.
               </p>
               <hr>
               <h4 id="taln-2009-demo-010">La plate-forme d’annotation Glozz</h4>
               			Auteur : Antoine Widlöcher -
               			Contact : antoine.widlocher@info.unicaen.fr<br>
               			Auteur : Yann Mathet -
               			Contact : yann.mathet@info.unicaen.fr<br><p></p>
               <hr>
               <h4 id="taln-2009-demo-011">SAGACE-v3.3 ; Analyseur de corpus pour langues non flexionnelles</h4>
               			Auteur : Blin Raoul -
               			Contact : blin@ehess.fr<br><p>Nous présentons la dernière version du logiciel SAGACE, analyseur de corpus pour langues
                  faiblement flexionnelles (par exemple japonais ou chinois). Ce logiciel est distribué
                  avec un lexique où les catégories sont exprimées à l'aide de systèmes de traits.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present a software program named SAGACE, designed to search for and extract word
                  strings from a large corpus. It has been conceived for poor flexional languages, such
                  as Japanese or Chinese. It is associated with a lexicon where categories are expressed
                  with feature systems.
               </p>
               <hr>
               <h4 id="taln-2009-demo-012">Apache UIMA pour le Traitement Automatique des Langues</h4>
               			Auteur : Nicolas Hernandez -
               			Contact : Nicolas.Hernandez@univ-nantes.fr<br>
               			Auteur : Fabien Poulard -
               			Contact : Fabien.Poulard@univ-nantes.fr<br>
               			Auteur : Stergos Afantenos -
               			Contact : Stergos.Afantenos@univ-nantes.fr<br>
               			Auteur : Matthieu Vernier -
               			Contact : Matthieu.Vernier@univ-nantes.fr<br>
               			Auteur : Jérôme Rocheteau -
               			Contact : Jerome.Rocheteau@univ-nantes.fr<br><p>L’objectif de la démonstration est d’une part de faire un retour d’expérience sur
                  la solution logicielle Apache UIMA comme infrastructure de développement d’applications
                  distribuées de TAL, et d’autre part de présenter les développements réalisés par l’équipe
                  TALN du LINA pour permettre à la communauté de s’approprier ce « framework ».
               </p><em>Version anglaise :</em><h4></h4>
               <p>Our objectives are twofold : First, based on some common use cases, we will discuss
                  the interest of using UIMA as a middleware solution for developing Natural Language
                  Processing systems. Second, we will present various preprocessing tools we have developed
                  in order to facilitate the access to the framework for the French community.
               </p>
               <hr>
               <h4 id="taln-2009-demo-013">Un Analyseur Sémantique pour le DHM</h4>
               			Auteur : Jérôme Lehuen -
               			Contact : Jerome.Lehuen@lium.univ-lemans.fr<br>
               			Auteur : Thierry Lemeunier -
               			Contact : Thierry.Lemeunier@lium.univ-lemans.fr<br><p></p>
               <hr>
               <h4 id="taln-2009-demo-014">Un chunker multilingue endogène</h4>
               			Auteur : Jacques Vergne -
               			Contact : Jacques.Vergne@info.unicaen.fr<br><p>Le chunking consiste à segmenter un texte en chunks, segments sous-phrastiques qu'Abney
                  a défini approximativement comme des groupes accentuels. Traditionnellement, le chunking
                  utilise des ressources monolingues, le plus souvent exhaustives, quelquefois partielles
                  : des mots grammaticaux et des ponctuations, qui marquent souvent des débuts et fins
                  de chunk. Mais cette méthode, si l'on veut l'étendre à de nombreuses langues, nécessite
                  de multiplier les ressources monolingues. Nous présentons une nouvelle méthode : le
                  chunking endogène, qui n'utilise aucune ressource hormis le texte analysé lui-même.
                  Cette méthode prolonge les travaux de Zipf : la minimisation de l'effort de communication
                  conduit les locuteurs à raccourcir les mots fréquents. On peut alors caractériser
                  un chunk comme étant la période des fonctions périodiques correllées longueur et effectif
                  des mots sur l'axe syntagmatique. Cette méthode originale présente l'avantage de s'appliquer
                  à un grand nombre de langues d'écriture alphabétique, avec le même algorithme, sans
                  aucune ressource.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately
                  defined as stress groups. Chunking usually uses monolingual resources, most often
                  exhaustive, sometimes partial : function words and punctuations, which often mark
                  beginnings and ends of chunks. But, to extend this method to other languages, monolingual
                  resources have to be multiplied. We present a new method : endogenous chunking, which
                  uses no other resource than the text to be parsed itself. The idea of this method
                  comes from Zipf : to make the least communication effort, speakers are driven to shorten
                  frequent words. A chunk then can be characterised as the period of the periodic correlated
                  functions length and frequency of words on the syntagmatic axis. This original method
                  takes its advantage to be applied to a great number of languages of alphabetic script,
                  with the same algorithm, without any resource.
               </p>
               <hr>
               <h4 id="taln-2009-court-001">Adaptation de parsers statistiques lexicalisés pour le français : Une évaluation complète
                  sur corpus arborés
               </h4>
               			Auteur : Djamé Seddah<br>
               			Auteur : Marie Candito<br>
               			Auteur : Benoît Crabbé<br><p>Cet article présente les résultats d’une évaluation exhaustive des principaux analyseurs
                  syntaxiques probabilistes dit “lexicalisés” initialement conçus pour l’anglais, adaptés
                  pour le français et évalués sur le CORPUS ARBORÉ DU FRANÇAIS (Abeillé et al., 2003)
                  et le MODIFIED FRENCH TREEBANK (Schluter &amp; van Genabith, 2007). Confirmant les résultats
                  de (Crabbé &amp; Candito, 2008), nous montrons que les modèles lexicalisés, à travers
                  les modèles de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le modèle
                  des TIG Stochastiques (Chiang, 2000), présentent des performances moindres face à
                  un analyseur PCFG à Annotation Latente (Petrov et al., 2006). De plus, nous montrons
                  que le choix d’un jeu d’annotations issus de tel ou tel treebank oriente fortement
                  les résultats d’évaluations tant en constituance qu’en dépendance non typée. Comparés
                  à (Schluter &amp; van Genabith, 2008; Arun &amp; Keller, 2005), tous nos résultats sont state-of-the-art
                  et infirment l’hypothèse d’une difficulté particulière qu’aurait le français en terme
                  d’analyse syntaxique probabiliste et de sources de données.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents complete investigation results on the statistical parsing of French
                  by bringing a complete evaluation on French data of the main based probabilistic lexicalized
                  (Charniak, Collins, Chiang) and unlexicalized (Berkeley) parsers designed first on
                  the Penn Treebank. We adapted the parsers on the two existing treebanks of French
                  (Abeillé et al., 2003; Schluter &amp; van Genabith, 2007). To our knowledge, all the results
                  reported here are state-of-the-art for the constituent parsing of French on every
                  available treebank and invalidate the hypothesis of French being particularly difficult
                  to parse. Regarding the algorithms, the comparisons show that lexicalized parsing
                  models are outperformed by the unlexicalized Berkeley parser. Regarding the treebanks,
                  we observe that a tag set with specific features has direct influences over evaluation
                  results depending on the parsing model.
               </p>
               <hr>
               <h4 id="taln-2009-court-002">Analyse automatique des noms déverbaux composés : pourquoi et comment faire interagir
                  analogie et système de règles
               </h4>
               			Auteur : Fiammetta Namer -
               			Contact : fiammetta.namer@univ-nancy2.fr<br><p>Cet article aborde deux problèmes d’analyse morpho-sémantique du lexique : (1) attribuer
                  automatiquement une définition à des noms et verbes morphologiquement construits inconnus
                  des dictionnaires mais présents dans les textes ; (2) proposer une analyse combinant
                  règles et analogie, deux techniques généralement contradictoires. Les noms analysés
                  sont apparemment suffixés et composés (HYDROMASSAGE). La plupart d’entre eux, massivement
                  attestés dans les documents (journaux, Internet) sont absents des dictionnaires. Ils
                  sont souvent reliés à des verbes (HYDROMASSER) également néologiques. Le nombre de
                  ces noms et verbes est estimé à 5.400. L’analyse proposée leur attribue une définition
                  par rapport à leur base, et enrichit un lexique de référence pour le TALN au moyen
                  de cette base, si elle est néologique. L’implémentation des contraintes linguistiques
                  qui régissent ces formations est reproductible dans d’autres langues européennes où
                  sont rencontrés les mêmes types de données dont l’analyse reflète le même raisonnement
                  que pour le français.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper addresses two morpho-semantic parsing issues: (1) to automatically provide
                  morphologically complex unknown nouns and verbs with a definition; (2) to propose
                  a methodology combining both rules and analogy, which are techniques usually seen
                  as inconsistent with eachother. The analysed nouns look like both suffixed and compounded
                  (HYDROMASSAGE). Most of them are not stored in dictionaries, although they are very
                  frequent in newspapers or online documents. They are often related to verbs (HYDROMASSER),
                  also lacking from dictionaries. The estimated amount of these nouns and verbs is 5,400.
                  The proposed analysis assigns them a definition calculated according to their base
                  meaning, and it increases the existing reference lexicon content with this base, from
                  the moment that it is a new-coined form. The implementation of linguistic constraints
                  which govern this word formations is reproducible in other West-European languages,
                  where the same data type is found, subject to the same kind of analysis.
               </p>
               <hr>
               <h4 id="taln-2009-court-003">Analyse en dépendances à l’aide des grammaires d’interaction</h4>
               			Auteur : Jonathan Marchand -
               			Contact : Jonathan.Marchand@loria.fr<br>
               			Auteur : Bruno Guillaume -
               			Contact : Bruno.Guillaume@loria.fr<br>
               			Auteur : Guy Perrier -
               			Contact : Guy.Perrier@loria.fr<br><p>Cet article propose une méthode pour extraire une analyse en dépendances d’un énoncé
                  à partir de son analyse en constituants avec les grammaires d’interaction. Les grammaires
                  d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots
                  à l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi
                  par la saturation des polarités. Les interactions s’effectuent entre les constituants,
                  mais les grammaires étant lexicalisées, ces interactions peuvent se traduire sur les
                  mots. La saturation des polarités lors de l’analyse syntaxique d’un énoncé permet
                  d’extraire des relations de dépendances entre les mots, chaque dépendance étant réalisée
                  par une saturation. Les structures de dépendances ainsi obtenues peuvent être vues
                  comme un raffinement de l’analyse habituellement effectuée sous forme d’arbre de dépendance.
                  Plus généralement, ce travail apporte un éclairage nouveau sur les liens entre analyse
                  en constituants et analyse en dépendances.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This article proposes a method to extract dependency structures from phrasestructure
                  level parsing with Interaction Grammars. Interaction Grammars are a formalism which
                  expresses interactions among words using a polarity system. Syntactical composition
                  is led by the saturation of polarities. Interactions take place between constituents,
                  but as grammars are lexicalized, these interactions can be translated at the level
                  of words. Dependency relations are extracted from the parsing process : every dependency
                  is the consequence of a polarity saturation. The dependency relations we obtain can
                  be seen as a refinement of the usual dependency tree. Generally speaking, this work
                  sheds new light on links between phrase structure and dependency parsing.
               </p>
               <hr>
               <h4 id="taln-2009-court-004">Analyse relâchée à base de contraintes</h4>
               			Auteur : Jean-Philippe Prost -
               			Contact : JPProst@gmail.com<br><p>La question de la grammaticalité, et celle duale de l’agrammaticalité, sont des sujets
                  délicats à aborder, dès lors que l’on souhaite intégrer différents degrés, tant de
                  grammaticalité que d’agrammaticalité. En termes d’analyse automatique, les problèmes
                  posés sont de l’ordre de la représentation des connaissances, du traitement, et bien
                  évidement de l’évaluation. Dans cet article, nous nous concentrons sur l’aspect traitement,
                  et nous nous penchons sur la question de l’analyse d’énoncés agrammaticaux. Nous explorons
                  la possibilité de fournir une analyse la plus complète possible pour un énoncé agrammatical,
                  sans l’apport d’information complémentaire telle que par le biais de mal-règles ou
                  autre grammaire d’erreurs. Nous proposons une solution algorithmique qui permet l’analyse
                  automatique d’un énoncé agrammatical, sur la seule base d’une grammaire modèle-théorique
                  de bonne formation. Cet analyseur est prouvé générer une solution optimale, selon
                  un critère numérique maximisé.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The question of grammaticality, and the dual one of ungrammaticality, are topics delicate
                  to address when interested in modeling different degrees, whether of grammaticality
                  or ungrammaticality. As far as parsing is concerned, the problems are with regard
                  to knowledge representation, processing, and obviously evaluation. In this paper,
                  we concentrate on the processing aspect and we address the question of parsing ungrammatical
                  utterances. We explore the possibility to provide a full parse for an ungrammatical
                  utterance without relying on any kind of additional information, which would be provided
                  by mal-rules or other error grammar. We propose an algorithmic solution in order to
                  parse an ungrammatical utterance using only a model-theoretic grammar of well-formedness.
                  The parser is proven to generate an optimal solution, according to a maximised criterion.
               </p>
               <hr>
               <h4 id="taln-2009-court-005">ANNODIS: une approche outillée de l'annotation de structures discursives</h4>
               			Auteur : Marie-Paule Péry-Woodley -
               			Contact : pery@univ-tlse2.fr<br>
               			Auteur : Nicholas Asher -
               			Contact : asher@irit.fr<br>
               			Auteur : Patrice Enjalbert -
               			Contact : patrice.enjalbert@info.unicaen.fr<br>
               			Auteur : Farah Benamara -
               			Contact : benamara@irit.fr<br>
               			Auteur : Myriam Bras -
               			Contact : bras@univ-tlse2.fr<br>
               			Auteur : Cécile Fabre -
               			Contact : cecile.fabre@univ-tlse2.fr<br>
               			Auteur : Stéphane Ferrari -
               			Contact : stephane.ferrari@info.unicaen.fr<br>
               			Auteur : Lydia-Mai Ho-Dac -
               			Contact : hodac@univ-tlse2.fr<br>
               			Auteur : Anne Le Draoulec -
               			Contact : draoulec@univ-tlse2.fr<br>
               			Auteur : Yann Mathet -
               			Contact : mathet@info.unicaen.fr<br>
               			Auteur : Philippe Muller -
               			Contact : philippe.muller@irit.fr<br>
               			Auteur : Laurent Prévot -
               			Contact : laurent.prevot@lpl-aix.fr<br>
               			Auteur : Josette Rebeyrolle -
               			Contact : rebeyrol@univ-tlse2.fr<br>
               			Auteur : Ludovic Tanguy<br>
               			Auteur : Marianne Vergez-Couret -
               			Contact : vergez@univ-tlse2.fr<br>
               			Auteur : Laure Vieu -
               			Contact : vieu@irit.fr<br>
               			Auteur : Antoine Widlöcher -
               			Contact : awidloch@info.unicaen.fr<br><p>Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif
                  ainsi que le développement d'outils pour l’annotation et l’exploitation de corpus.
                  Les annotations adoptent deux points de vue complémentaires : une perspective ascendante
                  part d'unités de discours minimales pour construire des structures complexes via un
                  jeu de relations de discours ; une perspective descendante aborde le texte dans son
                  entier et se base sur des indices pré-identifiés pour détecter des structures discursives
                  de haut niveau. La construction du corpus est associée à la création de deux interfaces
                  : la première assiste l'annotation manuelle des relations et structures discursives
                  en permettant une visualisation du marquage issu des prétraitements ; une seconde
                  sera destinée à l'exploitation des annotations. Nous présentons les modèles et protocoles
                  d'annotation élaborés pour mettre en oeuvre, au travers de l'interface dédiée, la
                  campagne d'annotation.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The ANNODIS project has two interconnected objectives: to produce a corpus of texts
                  annotated at discourse-level, and to develop tools for corpus annotation and exploitation.
                  Two sets of annotations are proposed, representing two complementary perspectives
                  on discourse organisation: a bottom-up approach starting from minimal discourse units
                  and building complex structures via a set of discourse relations; a top-down approach
                  envisaging the text as a whole and using pre-identified cues to detect discourse macro-structures.
                  The construction of the corpus goes hand in hand with the development of two interfaces:
                  the first one supports manual annotation of discourse structures, and allows different
                  views of the texts using NLPM-based pre-processing; another interface will support
                  the exploitation of the annotations. We present the discourse models and annotation
                  protocols, and the interface which embodies them.
               </p>
               <hr>
               <h4 id="taln-2009-court-006">Apport de la syntaxe dans un système de question-réponse : étude du système FIDJI.</h4>
               			Auteur : Véronique Moriceau -
               			Contact : moriceau@limsi.fr<br>
               			Auteur : Xavier Tannier -
               			Contact : xtannier@limsi.fr<br><p>Cet article présente une série d’évaluations visant à étudier l’apport d’une analyse
                  syntaxique robuste des questions et des documents dans un système de questions-réponses.
                  Ces évaluations ont été effectuées sur le système FIDJI, qui utilise à la fois des
                  informations syntaxiques et des techniques plus “traditionnelles”. La sélection des
                  documents, l’extraction de la réponse ainsi que le comportement selon les différents
                  types de questions ont été étudiés.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents some experiments aiming at estimating the contribution of a syntactic
                  parser on both questions and documents in a question-answering system. This evaluation
                  has been performed with the system FIDJI, which makes use of both syntactic information
                  and more “traditional” techniques. Document selection, answer extraction as well as
                  system behaviour on different types of questions have been experimented.
               </p>
               <hr>
               <h4 id="taln-2009-court-007">Apport des cooccurrences à la correction et à l'analyse syntaxique</h4>
               			Auteur : Dominique Laurent -
               			Contact : dlaurent@synapse-fr.com<br>
               			Auteur : Sophie Nègre -
               			Contact : sophie.negre@synapse-fr.com<br>
               			Auteur : Patrick Séguéla -
               			Contact : patrick.seguela@synapse-fr.com<br><p>Le correcteur grammatical Cordial utilise depuis de nombreuses années les cooccurrences
                  pour la désambiguïsation sémantique. Un dictionnaire de cooccurrences ayant été constitué
                  pour les utilisateurs du logiciel de correction et d'aides à la rédaction, la grande
                  richesse de ce dictionnaire a incité à l'utiliser intensivement pour la correction,
                  spécialement des homonymes et paronymes. Les résultats obtenus sont spectaculaires
                  sur ces types d'erreurs mais la prise en compte des cooccurrences a également été
                  utilisée avec profit pour la pure correction orthographique et pour le rattachement
                  des groupes en analyse syntaxique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>For many years, the spellchecker named Cordial has been using cooccurrences for semantic
                  disambiguation. Since a dictionary of co-occurrences had been established for users
                  of the spellchecker and of the writing aid, the richness of this dictionary led us
                  to use it intensively for the correction, especially for homonyms and paronyms. The
                  results are impressive on this kind of errors but taking into account the cooccurrences
                  proved to be very profitable for pure spellchecking and for the attachment of groups
                  in syntactic parsing.
               </p>
               <hr>
               <h4 id="taln-2009-court-008"></h4>
               			Auteur : Daoud Daoud -
               			Contact : daoud@batelco.jo<br>
               			Auteur : Mohammad Daoud -
               			Contact : Mohammad.Daoud@imag.fr<br><p></p><em>Version anglaise :</em><h4>Arabic Disambiguation Using Dependency Grammar</h4>
               <p>In this paper, we present a new approach to disambiguation Arabic using a joint rule-based
                  model which is conceptualized using Dependency Grammar. This approach helps in highly
                  accurate analysis of sentences. The analysis produces a semantic net like structure
                  expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua.
                  Extremely varied and complex phenomena of Arabic language have been addressed.
               </p>
               <hr>
               <h4 id="taln-2009-court-009">Association automatique de lemmes et de paradigmes de flexion à un mot inconnu</h4>
               			Auteur : Claude de Loupy -
               			Contact : loupy@syllabs.com<br>
               			Auteur : Michaël Bagur -
               			Contact : bagur@syllabs.com<br>
               			Auteur : Helena Blancafort -
               			Contact : blancafort@syllabs.com<br><p>La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des
                  tâches fastidieuses. Dans cet article nous présentons la mise en place d’une procédure
                  de guessing de flexion afin d’aider les linguistes dans leur travail de lexicographes.
                  Le guesser développé ne fait pas qu’évaluer l’étiquette morphosyntaxique comme c’est
                  généralement le cas. Il propose pour un mot français inconnu, un ou plusieurs candidats-lemmes,
                  ainsi que les paradigmes de flexion associés (formes fléchies et étiquettes morphosyntaxiques).
                  Dans cet article, nous décrivons le modèle probabiliste utilisé ainsi que les résultats
                  obtenus. La méthode utilisée permet de réduire considérablement le nombre de règles
                  à valider, permettant ainsi un gain de temps important.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper,
                  we present preliminary work on an inflectional guessing procedure for helping the
                  linguist in lexicographic tasks. The guesser presented here does not only output morphosyntactic
                  tags, but also suggests for an unknown French word one or more lemma candidates as
                  well as their corresponding inflectional rules and morphosyntactic tags that the linguist
                  has to validate. In this article, we present the probabilistic model we used as well
                  as obtained results. The method allows a drastic reduction of the number of rules
                  to validate.
               </p>
               <hr>
               <h4 id="taln-2009-court-010">Catégorisation sémantico-discursive des évaluations exprimées dans la blogosphère</h4>
               			Auteur : Matthieu Vernier -
               			Contact : Matthieu.Vernier@univ-nantes.fr<br>
               			Auteur : Laura Monceaux -
               			Contact : Laura.Monceaux@univ-nantes.fr<br>
               			Auteur : Béatrice Daille -
               			Contact : Beatrice.Daille@univ-nantes.fr<br>
               			Auteur : Estelle Dubreil -
               			Contact : Estelle.Dubreil@univ-nantes.fr<br><p>Les blogs constituent un support d’observations idéal pour des applications liées
                  à la fouille d’opinion. Toutefois, ils imposent de nouvelles problématiques et de
                  nouveaux défis au regard des méthodes traditionnelles du domaine. De ce fait, nous
                  proposons une méthode automatique pour la détection et la catégorisation des évaluations
                  localement exprimées dans un corpus de blogs multi-domaine. Celle-ci rend compte des
                  spécificités du langage évaluatif décrites dans deux théories linguistiques. L’outil
                  développé au sein de la plateforme UIMA vise d’une part à construire automatiquement
                  une grammaire du langage évaluatif, et d’autre part à utiliser cette grammaire pour
                  la détection et la catégorisation des passages évaluatifs d’un texte. La catégorisation
                  traite en particulier l’aspect axiologique de l’évaluation, sa configuration d’énonciation
                  et sa modalité dans le discours.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Blogs are an ideal observation for applications related to the opinion mining task.
                  However, they impose new problems and new challenges in this field. Therefore, we
                  propose a method for automatic detection and classification of appraisal locally expressed
                  in a multi-domain blogs corpus. It reflects the specific aspects of appraisal language
                  described in two linguistic theories. The tool developed within the UIMA platform
                  aims both to automatically build a grammar of the appraisal language, and the other
                  part to use this grammar for the detection and categorization of evaluative segments
                  in a text. Categorization especially deals with axiological aspect of an evaluative
                  segments, enunciative configuration and its attitude in discourse.
               </p>
               <hr>
               <h4 id="taln-2009-court-011">Chaîne de traitement linguistique : du repérage d'expressions temporelles au peuplement
                  d'une ontologie de tourisme
               </h4>
               			Auteur : Stéphanie Weiser -
               			Contact : sweiser@u-paris10.fr<br>
               			Auteur : Martin Coste -
               			Contact : martin.coste@mondeca.com<br>
               			Auteur : Florence Amardeilh -
               			Contact : florence.amardeilh@mondeca.com<br><p>Cet article présente la chaîne de traitement linguistique réalisée pour la mise en
                  place d'une plateforme touristique sur Internet. Les premières étapes de cette chaîne
                  sont le repérage et l'annotation des expressions temporelles présentes dans des pages
                  Web. Ces deux tâches sont effectuées à l'aide de patrons linguistiques. Elles soulèvent
                  de nombreux questionnements auxquels nous tentons de répondre, notamment au sujet
                  de la définition des informations à extraire, du format d'annotation et des contraintes.
                  L'étape suivante consiste en l'exploitation des données annotées pour le peuplement
                  d'une ontologie du tourisme. Nous présentons les règles d'acquisition nécessaires
                  pour alimenter la base de connaissance du projet. Enfin, nous exposons une évaluation
                  du système d'annotation. Cette évaluation permet de juger aussi bien le repérage des
                  expressions temporelles que leur annotation.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents the linguistic data processing sequence built for a tourism web
                  portal. The first steps of this sequence are the detection and the annotation of the
                  temporal expressions found in the web pages. These tasks are performed using linguistic
                  patterns. They lead to many questions which we try to answer, such as the definition
                  of information to detect, annotation format and constraints. In the next step this
                  annotated data is used to populate a tourism ontology. We present the acquisition
                  rules which are necessary to enrich the portal knowledge base. Then we present an
                  evaluation of our annotation system. This evaluation is able to judge the detection
                  of the temporal expressions and their annotation.
               </p>
               <hr>
               <h4 id="taln-2009-court-012">Détection des contradictions dans les annotations sémantiques</h4>
               			Auteur : Yue Ma -
               			Contact : Yue.Ma@lipn.univ-paris13.fr<br>
               			Auteur : Laurent Audibert -
               			Contact : Laurent.Audibert@lipn.univ-paris13.fr<br><p>L’annotation sémantique a pour objectif d’apporter au texte une représentation explicite
                  de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre
                  les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour
                  l’annotation sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme
                  d’annotation linguistique automatique. Nous présentons dans cet article une mesure,
                  basée sur la valeur de Shapley, permettant d’identifier les règles qui sont sources
                  de contradiction dans l’annotation sémantique. Par rapport aux classiques mesures
                  de précision et de rappel, l’intérêt de cette mesure est de ne pas nécessiter de corpus
                  manuellement annoté, d’être entièrement automatisable et de permettre l’identification
                  des règles qui posent problème.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The semantic annotation has the objective to bring to a text an explicit representation
                  of its semantic interpretation. In a preceding article, we suggested extending ontologies
                  by semantic annotation rules. These rules are used for the semantic annotation of
                  a text with respect to an ontology within the framework of an automated linguistic
                  annotation platform. We present in this article a measure, based on the Shapley value,
                  allowing to identify the rules which are sources of contradictions in the semantic
                  annotation. With regard to the classic measures, precision and recall, the interest
                  of this measure is without the requirement of manually annotated corpus, completely
                  automated and its ability to identify rules which raise problems.
               </p>
               <hr>
               <h4 id="taln-2009-court-013">Détection des émotions à partir du contenu linguistique d’énoncés oraux : application
                  à un robot compagnon pour enfants fragilisés
               </h4>
               			Auteur : Marc Le Tallec -
               			Contact : Marc.letallec@univ-tours.fr<br>
               			Auteur : Jeanne Villaneau<br>
               			Auteur : Jean-Yves Antoine<br>
               			Auteur : Agata Savary<br>
               			Auteur : Arielle Syssau-Vaccarella<br><p>Le projet ANR Emotirob aborde la question de la détection des émotions sous un cadre
                  original : concevoir un robot compagnon émotionnel pour enfants fragilisés. Notre
                  approche consiste à combiner détection linguistique et prosodie. Nos expériences montrent
                  qu'un sujet humain peut estimer de manière fiable la valence émotionnelle d'un énoncé
                  à partir de son contenu propositionnel. Nous avons donc développé un premier modèle
                  de détection linguistique qui repose sur le principe de compositionnalité des émotions
                  : les mots simples ont une valence émotionnelle donnée et les prédicats modifient
                  la valence de leurs arguments. Après une description succincte du système logique
                  de compréhension dont les sorties sont utilisées pour le calcul global de l'émotion,
                  cet article présente la construction d'une norme émotionnelle lexicale de référence,
                  ainsi que d'une ontologie de classes émotionnelles de prédicats, pour des enfants
                  de 5 et 7 ans.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Project ANR Emotirob aims at detecting emotions from an original point of view: realizing
                  an emotional companion robot for weakened children. In our approach, linguistic detection
                  and prosodie are combined. Our experiments show that human beings can estimate the
                  emotional value of an utterance from its propositional content in a reliable way.
                  So we have implemented a first model of linguistic detection, based on the principle
                  that emotions can be compound: lexical words have an emotional value while predicates
                  can modify emotional values of their arguments. This paper presents a short description
                  of the logical understanding system, the outputs of which are used for the final emotional
                  value calculus. Then, the creation of a lexical emotional reference standard is presented
                  with an ontology of emotional predicate classes for children, aged between 5 and 7.
               </p>
               <hr>
               <h4 id="taln-2009-court-014">Dispersion sémantique dans des familles morpho-phonologiques : éléments théoriques
                  et empiriques
               </h4>
               			Auteur : Nuria Gala -
               			Contact : nuria.gala@lif.univ-mrs.fr<br>
               			Auteur : Véronique Rey -
               			Contact : veronique.rey@univ-provence.fr<br>
               			Auteur : Laurent Tichit -
               			Contact : tichit@iml.univ-mrs.fr<br><p>Traditionnellement, la morphologie lexicale a été diachronique et a permis de proposer
                  le concept de famille de mots. Ce dernier est repris dans les études en synchronie
                  et repose sur une forte cohérence sémantique entre les mots d’une même famille. Dans
                  cet article, nous proposons une approche en synchronie fondée sur la notion de continuité
                  à la fois phonologique et sémantique. Nous nous intéressons, d’une part, à la morpho-phonologie
                  et, d’autre part, à la dispersion sémantique des mots dans les familles. Une première
                  étude (Gala &amp; Rey, 2008) montrait que les familles de mots obtenues présentaient des
                  espaces sémantiques soit de grande cohésion soit de grande dispersion. Afin de valider
                  ces observations, nous présentons ici une méthode empirique qui permet de pondérer
                  automatiquement les unités de sens d’un mot et d’une famille. Une expérience menée
                  auprès de 30 locuteurs natifs valide notre approche et ouvre la voie pour une étude
                  approfondie du lexique sur ces bases phonologiques et sémantiques.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Traditionally, lexical morphology has been diachronic and has established the notion
                  of word families. This notion is reused in synchronic studies and implies strong semantic
                  coherence within the words of a same family. In this paper, we propose an approach
                  in synchrony which highlights phonological and semantic continuity. Our interests
                  go on morphophonology and on the semantic dispersion of words in a family. A first
                  study (Gala &amp; Rey, 2008) showed that the semantic spaces of the families displayed
                  either a strong semantic cohesion or a strong dispersion. In order to validate this
                  observation, we present here a corpus-based method that automatically weights the
                  semantic units of a word and a family. An experience carried out with 30 native speakers
                  validates our approach and allows us to foresee a thorough study of the lexicon based
                  on phonological and semantic basis.
               </p>
               <hr>
               <h4 id="taln-2009-court-015">Exploitation d’une structure pour les questions enchaînées</h4>
               			Auteur : Kévin Séjourné -
               			Contact : kevin.sejourne@limsi.fr<br><p>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses
                  (SQR) utilisant des questions enchaînées. La recherche des documents dans un SQR est
                  perturbée par l’absence des éléments utiles à la recherche dans les questions liées,
                  éléments figurant dans les échanges précédents. Les récentes campagnes d’évaluation
                  montrent que ce problème est sous-estimé, et n’a pas fait l’objet de technique dédiée.
                  Afin d’améliorer la recherche des documents dans un SQR nous utilisons une méthode
                  récente d’organisation des informations liées aux interactions entre questions. Celle-ci
                  se base sur l’exploitation d’une structure de données adaptée à la transmission des
                  informations des questions liées jusqu’au moteur d’interrogation. Le moteur d’interrogation
                  doit alors être adapté afin de tirer partie de cette structure de données.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present works realized in the field of the questions answering (QA) using chained
                  questions. The documents search in QA system is disrupted because useful elements
                  are missing for search using bound questions. Recents evaluation campaigns show this
                  problem as underestimated, and this problem wasn’t solve by specific techniques. To
                  improve documents search in a QA we use a recent information organization method for
                  bound questions to the interactions between questions. This methode is bases on the
                  operation of a special data structure. This data structure transmit informations from
                  bound questions to the interrogation engine. Then the interrogation engine must be
                  improve to take advantage of this data structure.
               </p>
               <hr>
               <h4 id="taln-2009-court-016">Exploitation du terrain commun pour la production d’expressions référentielles dans
                  les systèmes de dialogue
               </h4>
               			Auteur : Alexandre Denis -
               			Contact : alexandre.denis@loria.fr<br>
               			Auteur : Matthieu Quignard -
               			Contact : matthieu.quignard@loria.fr<br><p>Cet article présente un moyen de contraindre la production d’expressions référentielles
                  par un système de dialogue en fonction du terrain commun. Cette capacité, fondamentale
                  pour atteindre la compréhension mutuelle, est trop souvent oubliée dans les systèmes
                  de dialogue. Le modèle que nous proposons s’appuie sur une modélisation du processus
                  d’ancrage (grounding process) en proposant un raffinement du statut d’ancrage appliqué
                  à la description des référents. Il décrit quand et comment ce statut doit être révisé
                  en fonction des jugements de compréhension des deux participants ainsi que son influence
                  dans le choix d’une description partagée destinée à la génération d’une expression
                  référentielle.
               </p><em>Version anglaise :</em><h4></h4>
               <p>This paper presents a way to constraint the production of referring expressions by
                  a dialogue system according to the common ground. This ability – fundamental for reaching
                  mutual understanding – is often neglected in dialogue system design. The proposed
                  model is based on a view of the grounding process and offering a refinement of the
                  grounding status concerning the referent description. It explains how and when this
                  status should be revised with respect to how participants evaluate their understanding
                  and how this status may help to choose a shared description with which a referring
                  expression can be generated.
               </p>
               <hr>
               <h4 id="taln-2009-court-017">Gestion de dialogue oral Homme-machine en arabe</h4>
               			Auteur : Younès Bahou -
               			Contact : bahou_younes@yahoo.fr<br>
               			Auteur : Amine Bayoudhi -
               			Contact : bayoudhi.amine@gmail.com<br>
               			Auteur : Lamia Hadrich Belguith -
               			Contact : l.belguith@fsegs.rnu.tn<br><p>Dans le présent papier, nous présentons nos travaux sur la gestion du dialogue oral
                  arabe Homme-machine. Ces travaux entrent dans le cadre de la réalisation du serveur
                  vocal interactif SARF (Bahou et al., 2008) offrant des renseignements sur le transport
                  ferroviaire tunisien en langue arabe standard moderne. Le gestionnaire de dialogue
                  que nous proposons est basé sur une approche structurelle et est composé de deux modèles
                  à savoir, le modèle de tâche et le modèle de dialogue. Le premier modèle permet de
                  i) compléter et vérifier l’incohérence des structures sémantiques représentant les
                  sens utiles des énoncés, ii) générer une requête vers l’application et iii) récupérer
                  le résultat et de formuler une réponse à l’utilisateur en langage naturel. Quant au
                  modèle de dialogue, il assure l’avancement du dialogue avec l’utilisateur et l’identification
                  de ses intentions. L’interaction entre ces deux modèles est assurée grâce à un contexte
                  du dialogue permettant le suivi et la mise à jour de l’historique du dialogue.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we present our research work on Human-machine Arabic oral dialogue
                  management. This work enters in the context of SARF system (Bahou et al., 2008) an
                  interactive vocal server that provides information on Tunisian railway using modern
                  standard Arabic. The dialogue manager that we propose is based on a structural approach
                  and consists of two models namely, the task model and the dialogue model. The first
                  model is used to i) complete and verify the incoherence of semantic structures representing
                  the useful meaning of utterances, ii) generate a query to the application and iii)
                  get back the results and formulate an answer to the user in natural language. As for
                  the dialogue model, it assures the dialogue progress with the user and the identification
                  of her or his intentions. The interaction between these two models is assured by a
                  dialogue context that allows monitoring and updating the dialogue history.
               </p>
               <hr>
               <h4 id="taln-2009-court-018">Grammaires d’erreur – correction grammaticale avec analyse profonde et proposition
                  de corrections minimales
               </h4>
               			Auteur : Lionel Clément<br>
               			Auteur : Kim Gerdes<br>
               			Auteur : Renaud Marlet<br><p>Nous présentons un système de correction grammatical ouvert, basé sur des analyses
                  syntaxiques profondes. La spécification grammaticale est une grammaire hors-contexte
                  équipée de structures de traits plates. Après une analyse en forêt partagée où les
                  contraintes d’accord de traits sont relâchées, la détection d’erreur minimise globalement
                  les corrections à effectuer et des phrases alternatives correctes sont automatiquement
                  proposées.
               </p><em>Version anglaise :</em><h4></h4>
               <p>We present an open system for grammar checking, based on deep parsing. The grammatical
                  specification is a contex-free grammar with flat feature structures. After a sharedforest
                  analysis where feature agreement constraints are relaxed, error detection globally
                  minimizes the number of fixes and alternate correct sentences are automatically proposed.
               </p>
               <hr>
               <h4 id="taln-2009-court-019">Intégration des constructions à verbe support dans TimeML</h4>
               			Auteur : André Bittar -
               			Contact : andre.bittar@linguist.jussieu.fr<br>
               			Auteur : Laurence Danlos -
               			Contact : laurence.danlos@linguist.jussieu.fr<br><p>Le langage TimeML a été conçu pour l’annotation des informations temporelles dans
                  les textes, notamment les événements, les expressions de temps et les relations entre
                  les deux. Des consignes d’annotation générales ont été élaborées afin de guider l’annotateur
                  dans cette tâche, mais certains phénomènes linguistiques restent à traiter en détail.
                  Un problème commun dans les tâches de TAL, que ce soit en traduction, en génération
                  ou en compréhension, est celui de l’encodage des constructions à verbe support. Relativement
                  peu d’attention a été portée, jusqu’à maintenant, sur ce problème dans le cadre du
                  langage TimeML. Dans cet article, nous proposons des consignes d’annotation pour les
                  constructions à verbe support.
               </p><em>Version anglaise :</em><h4></h4>
               <p>TimeML is a markup language developed for the annotation of temporal information in
                  texts, in particular events, temporal expressions and the relations which hold between
                  the two. General annotation guidelines have been developed to guide the annotator
                  in this task, but certain linguistic phenomena have yet to be dealt with in detail.
                  A common problem in NLP tasks, whether in translation, generation or understanding,
                  is that of the encoding of light verb constructions. Relatively little attention has
                  been paid to this problem, until now, in the TimeML framework. In this article, we
                  propose annotation guidelines for light verb constructions.
               </p>
               <hr>
               <h4 id="taln-2009-court-020">Intégrer les tables du Lexique-Grammaire à un analyseur syntaxique robuste à grande
                  échelle
               </h4>
               			Auteur : Benoît Sagot -
               			Contact : benoit.sagot@inria.fr<br>
               			Auteur : Elsa Tolone -
               			Contact : elsa.tolone@univ-paris-est.fr<br><p>Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire
                  en un format TAL, celui du lexique Lefff, permettant ainsi son intégration dans l’analyseur
                  syntaxique FRMG. Nous présentons les fondements linguistiques de ce processus de conversion
                  et le lexique obtenu. Nous validons le lexique obtenu en évaluant l’analyseur syntaxique
                  FRMG sur le corpus de référence de la campagne EASy selon qu’il utilise les entrées
                  verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this paper, we describe how we converted the lexicon-grammar tables into an NLP
                  format, that of the Lefff lexicon, which allowed us to integrate it into the FRMG
                  parser. We decribe the linguistic basis of this conversion process, and the resulting
                  lexicon.We validate the resulting lexicon by evaluating the FRMG parser on the EASy
                  reference corpus depending on the set of verbal entries it relies on, namely those
                  of the Lefff or those of the converted lexicon-grammar verb tables.
               </p>
               <hr>
               <h4 id="taln-2009-court-021">La complémentarité des approches manuelle et automatique en acquisition lexicale</h4>
               			Auteur : Cédric Messiant -
               			Contact : cedric.messiant@lipn.univ-paris13.fr<br>
               			Auteur : Takuya Nakamura -
               			Contact : takuya.nakamura@univ-mlv.fr<br>
               			Auteur : Stavroula Voyatzi -
               			Contact : voyatzi@univ-mlv.fr<br><p>Les ressources lexicales sont essentielles pour obtenir des systèmes de traitement
                  des langues performants. Ces ressources peuvent être soit construites à la main, soit
                  acquises automatiquement à partir de gros corpus. Dans cet article, nous montrons
                  la complémentarité de ces deux approches. Pour ce faire, nous utilisons l’exemple
                  de la sous-catégorisation verbale en comparant un lexique acquis par des méthodes
                  automatiques (LexSchem) avec un lexique construit manuellement (Le Lexique-Grammaire).
                  Nous montrons que les informations acquises par ces deux méthodes sont bien distinctes
                  et qu’elles peuvent s’enrichir mutuellement.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Lexical resources are essentially created to obtain efficient text-processing systems.
                  These resources can be constructed either manually or automatically from large corpora.
                  In this paper, we show the complementarity of these two types of approaches, comparing
                  an automatically constructed lexicon (LexSchem) to a manually constructed one (Lexique-Grammaire),
                  on examples of verbal subcategorization. The results show that the information retained
                  by these two resources is in fact different and that they can be mutually enhanced.
               </p>
               <hr>
               <h4 id="taln-2009-court-022">L'analyseur syntaxique Cordial dans Passage</h4>
               			Auteur : Dominique Laurent -
               			Contact : dlaurent@synapse-fr.com<br>
               			Auteur : Sophie Nègre -
               			Contact : sophie.negre@synapse-fr.com<br>
               			Auteur : Patrick Séguéla -
               			Contact : patrick.seguela@synapse-fr.com<br><p>Cordial est un analyseur syntaxique et sémantique développé par la société Synapse
                  Développement. Largement utilisé par les laboratoires de TALN depuis plus de dix ans,
                  cet analyseur participe à la campagne Passage ("Produire des Annotations Syntaxiques
                  à Grande Échelle"). Comment fonctionne cet analyseur ? Quels résultats a-t-il obtenu
                  lors de la première phase d'évaluation de cette campagne ? Au-delà de ces questions,
                  cet article montre en quoi les contraintes industrielles façonnent les outils d'analyse
                  automatique du langage naturel.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Cordial is a syntactic and semantic parser developed by Synapse Développement. Widely
                  used by the laboratories of NLP for over ten years, this analyzer is involved in the
                  Passage campaign ("Producing Syntactic Annotations on a Large Scale"). How does this
                  parser work? What were the results obtained during the first phase of the evaluation?
                  Beyond these issues, this article shows how the industrial constraints condition the
                  tools for Natural Language Procesing.
               </p>
               <hr>
               <h4 id="taln-2009-court-023">La plate-forme Glozz : environnement d’annotation et d’exploration de corpus</h4>
               			Auteur : Antoine Widlöcher -
               			Contact : antoine.widlocher@info.unicaen.fr<br>
               			Auteur : Yann Mathet -
               			Contact : yann.mathet@info.unicaen.fr<br><p>La nécessité d’une interaction systématique entre modèles, traitements et corpus impose
                  la disponibilité d’annotations de référence auxquelles modèles et traitements pourront
                  être confrontés. Or l’établissement de telles annotations requiert un cadre formel
                  permettant la représentation d’objets linguistiques variés, et des applications permettant
                  à l’annotateur de localiser sur corpus et de caractériser les occurrences des phénomènes
                  observés. Si différents outils d’annotation ont vu le jour, ils demeurent souvent
                  fortement liés à un modèle théorique et à des objets linguistiques particuliers, et
                  ne permettent que marginalement d’explorer certaines structures plus récemment appréhendées
                  expérimentalement, notamment à granularité élevée et en matière d’analyse du discours.
                  La plate-forme Glozz répond à ces différentes contraintes et propose un environnement
                  d’exploration de corpus et d’annotation fortement configurable et non limité a priori
                  au contexte discursif dans lequel elle a initialement vu le jour.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The need for a systematic confrontation between models and corpora make it necessary
                  to have - and consequently, to produce - reference annotations to which linguistic
                  models could be compared. Creating such annotations requires both a formal framework
                  which copes with various linguistic objects, and specific manual annotation tools,
                  in order to make it possible to locate, identify and feature linguistic phenomena
                  in texts. Though several annotation tools do already exist, they are mostly dedicated
                  to a given theory and to a given set of structures. The Glozz platform, described
                  in this paper, tries to address all of these needs, and provides a highly versatile
                  corpus exploration and annotation framework.
               </p>
               <hr>
               <h4 id="taln-2009-court-024">Morfetik, ressource lexicale pour le TAL</h4>
               			Auteur : Pierre-André Buvet -
               			Contact : pabuvet@ldi.univparis13.fr<br>
               			Auteur : Emmanuel Cartier -
               			Contact : ecartier@ldi.univparis13.fr<br>
               			Auteur : Fabrice Issac -
               			Contact : fissac@ldi.univparis13.fr<br>
               			Auteur : Yassine Madiouni -
               			Contact : ymadiouni@ldi.univparis13.fr<br>
               			Auteur : Michel Mathieu-Colas -
               			Contact : mmathieu-colas@ldi.univparis13.fr<br>
               			Auteur : Salah Mejri -
               			Contact : smejri@ldi.univparis13.fr<br><p>Le traitement automatique des langues exige un recensement lexical aussi rigoureux
                  que possible. Dans ce but, nous avons développé un dictionnaire morphologique du français,
                  conçu comme le point de départ d’un système modulaire (Morfetik) incluant un moteur
                  de flexion, des interfaces de consultation et d’interrogation et des outils d’exploitation.
                  Nous présentons dans cet article, après une brève description du dictionnaire de base
                  (lexique des mots simples), quelques-uns des outils informatiques liés à cette ressource
                  : un moteur de recherche des lemmes et des formes fléchies ; un moteur de flexion
                  XML et MySQL ; des outils NLP permettant d’exploiter le dictionnaire ainsi généré
                  ; nous présentons notamment un analyseur linguistique développé dans notre laboratoire.
                  Nous comparons dans une dernière partie Morfetik avec d’autres ressources analogues
                  du français : Morphalou, Lexique3 et le DELAF.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Automatic language processing requires as rigorous a lexical inventory as possible.
                  For this purpose, we have developed a morphological dictionary for French, conceived
                  as the starting point of a modular system (Morfetik) which includes an inflection
                  generator, user interfaces and operating tools. In this paper, we briefly describe
                  the basic dictionary (lexicon of simple words) and detail some of the computing tools
                  based on the dictionary. The computing tools built on this resource include: a lemma
                  / inflected forms search engine; an XML and MySQL engine to build the inflected forms;
                  the generated dictionary can then be used by various NLP Tools; in this article, we
                  present the use of the dictionary in a linguistic analyser developed at the laboratory.
                  Finally, we compare Morfetik to similar resources : Morphalou, Lexique3 and DELAF.
               </p>
               <hr>
               <h4 id="taln-2009-court-025">Nouvelles considérations pour la détection de réutilisation de texte</h4>
               			Auteur : Fabien Poulard -
               			Contact : Fabien.Poulard@univ-nantes.fr<br>
               			Auteur : Stergos Afantenos -
               			Contact : Stergos.Afantenos@univ-nantes.fr<br>
               			Auteur : Nicolas Hernandez -
               			Contact : Nicolas.Hernandez@univ-nantes.fr<br><p>Dans cet article nous nous intéressons au problème de la détection de réutilisation
                  de texte. Plus particulièrement, étant donné un document original et un ensemble de
                  documents candidats — thématiquement similaires au premier — nous cherchons à classer
                  ceux qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons
                  le problème selon deux approches : dans la première, nous nous intéressons aux similarités
                  discursives entre les documents, dans la seconde au recouvrement de n-grams hapax.
                  Nous présentons le résultat d’expérimentations menées sur un corpus de presse francophone
                  construit dans le cadre du projet ANR PIITHIE.
               </p><em>Version anglaise :</em><h4></h4>
               <p>In this article we are interested in the problem of text reuse. More specifically,
                  given an original document and a set of candidate documents—which are thematically
                  similar to the first one — we are interested in classifying them into those that have
                  been derived from the original document and those that are not. We are approaching
                  the problem in two ways : firstly we are interested in the discourse similarities
                  between the documents, and secondly we are interested in the overlap of n-grams that
                  are hapax. We are presenting the results of the experiments that we have performed
                  on a corpus constituted from articles of the French press which has been created in
                  the context of the PIITHIE project funded by the French National Agency for Research
                  (Agence National de la Recherche, ANR).
               </p>
               <hr>
               <h4 id="taln-2009-court-026">Collecte et analyses de réponses naturelles pour les systèmes de questions-réponses</h4>
               			Auteur : Anne Garcia-Fernandez -
               			Contact : annegf@limsi.fr<br>
               			Auteur : Sophie Rosset -
               			Contact : vilnat@limsi.fr<br>
               			Auteur : Anne Vilnat -
               			Contact : rosset@limsi.fr<br><p>Notre travail se situe dans le cadre des systèmes de réponse a une question et à pour
                  but de fournir une réponse en langue naturelle aux questions posées en langue naturelle.
                  Cet article présente une expérience permettant d’analyser les réponses de locuteurs
                  du français à des questions que nous leur posons. L’expérience se déroule à l’écrit
                  comme à l’oral et propose à des locuteurs français des questions relevant de différents
                  types sémantiques et syntaxiques. Nous mettons en valeur une large variabilité dans
                  les formes de réponses possibles en langue française. D’autre part nous établissons
                  un certain nombre de liens entre formulation de question et formulation de réponse.
                  Nous proposons d’autre part une comparaison des réponses selon la modalité oral /
                  écrit. Ces résultats peuvent être intégrés à des systèmes existants pour produire
                  une réponse en langue naturelle de façon dynamique.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Situated within the domain of interactive question-answering, our work is to increase
                  the naturalness of natural language answers. This paper presents an experiment aiming
                  at observing the formulation of answers by French speakers. The system asked simple
                  questions to which the humans had to answer. Two modalities were used : text (web)
                  and speech (phone). We present and analyze the collected corpus. Within the large
                  variability of answer forms in French, we point some links between the answer form
                  and the question form. Moreover we present a preliminary study on the observed variation
                  between modalities. We expect these results to be integrable in existing systems to
                  dynamically produce adpated natural language answers.
               </p>
               <hr>
               <h4 id="taln-2009-court-027">La /fOnetizasjc/ comme un problème de translittération</h4>
               			Auteur : Vincent Claveau -
               			Contact : IRISA-CNRS, Campus de Beaulieu, 35042 Rennes cedex<br><p>La phonétisation est une étape essentielle pour le traitement de l’oral. Dans cet
                  article, nous décrivons un système automatique de phonétisation de mots isolés qui
                  est simple, portable et performant. Il repose sur une approche par apprentissage ;
                  le système est donc construit à partir d’exemples de mots et de leur représentation
                  phonétique. Nous utilisons pour cela une technique d’inférence de règles de réécriture
                  initialement développée pour la translittération et la traduction. Pour évaluer les
                  performances de notre approche, nous avons utilisé plusieurs jeux de données couvrant
                  différentes langues et divers alphabets phonétiques, tirés du challenge Pascal Pronalsyl.
                  Les très bons résultats obtenus égalent ou dépassent ceux des meilleurs systèmes de
                  l’état de l’art.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Phonetizing is a crucial step to process oral documents. In this paper, a new word-based
                  phonetization approach is proposed ; it is automatic, simple, portable and efficient.
                  It relies on machine learning ; thus, the system is built from examples of words with
                  their phonetic representations. More precisely, it makes the most of a technique inferring
                  rewriting rules initially developed for transliteration and translation. In order
                  to evaluate the performances of this approach, we used several datasets from the Pronalsyl
                  Pascal challenge, including different languages. The obtained results equal or outperform
                  those of the best known systems.
               </p>
               <hr>
               <h4 id="taln-2009-court-028">Plusieurs langues (bien choisies) valent mieux qu’une : traduction statistique multi-source
                  par renforcement lexical
               </h4>
               			Auteur : Josep Maria Crego -
               			Contact : jmcrego@limsi.fr<br>
               			Auteur : Aurélien Max -
               			Contact : amax@limsi.fr<br>
               			Auteur : François Yvon -
               			Contact : yvon@limsi.fr<br><p>Les systèmes de traduction statistiques intègrent différents types de modèles dont
                  les prédictions sont combinées, lors du décodage, afin de produire les meilleures
                  traductions possibles. Traduire correctement des mots polysémiques, comme, par exemple,
                  le mot avocat du français vers l’anglais (lawyer ou avocado), requiert l’utilisation
                  de modèles supplémentaires, dont l’estimation et l’intégration s’avèrent complexes.
                  Une alternative consiste à tirer parti de l’observation selon laquelle les ambiguïtés
                  liées à la polysémie ne sont pas les mêmes selon les langues source considérées. Si
                  l’on dispose, par exemple, d’une traduction vers l’espagnol dans laquelle avocat a
                  été traduit par aguacate, alors la traduction de ce mot vers l’anglais n’est plus
                  ambiguë. Ainsi, la connaissance d’une traduction français!espagnol permet de renforcer
                  la sélection de la traduction avocado pour le système français!anglais. Dans cet article,
                  nous proposons d’utiliser des documents en plusieurs langues pour renforcer les choix
                  lexicaux effectués par un système de traduction automatique. En particulier, nous
                  montrons une amélioration des performances sur plusieurs métriques lorsque les traductions
                  auxiliaires utilisées sont obtenues manuellement.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Statistical Machine Translation (SMT) systems integrate various models that exploit
                  all available features during decoding to produce the best possible translation hypotheses.
                  Correctly translating polysemous words, such as the French word avocat into English
                  (lawyer or avocado) requires integrating complex models. Such translation lexical
                  ambiguities, however, depend on the language pair considered. If one knows, for instance,
                  that avocat was translated into Spanish as aguacate, then translating it into English
                  is no longer ambiguous (avocado). Thus, in this example, the knowledge of the Spanish
                  translation allows to reinforce the choice of the appropriate English word for the
                  French!English system. In this article, we present an approach in which documents
                  available in several languages are used to reinforce the lexical choices made by a
                  SMT system. In particular, we show that gains can be obtained on several metrics when
                  using auxiliary translations produced by human translators.
               </p>
               <hr>
               <h4 id="taln-2009-court-029">Problématique d'analyse et de modélisation des erreurs en production écrite. Approche
                  interdisciplinaire
               </h4>
               			Auteur : Jean-Leon Bouraoui -
               			Contact : bouraoui@irit.fr<br>
               			Auteur : Philippe Boissière -
               			Contact : boissier@irit.fr<br>
               			Auteur : Mustapha Mojahid -
               			Contact : mojahid@irit.fr<br>
               			Auteur : Nadine Vigouroux -
               			Contact : vigourou@irit.fr<br>
               			Auteur : Aurélie Lagarrigue -
               			Contact : alagarri@irit.fr<br>
               			Auteur : Frédéric Vella -
               			Contact : vella@irit.fr<br>
               			Auteur : Jean-Luc Nespoulous -
               			Contact : nespoulo@univ-tlse2.fr<br><p>L'objectif du travail présenté ici est la modélisation de la détection et la correction
                  des erreurs orthographiques et dactylographiques, plus particulièrement dans le contexte
                  des handicaps langagiers. Le travail est fondé sur une analyse fine des erreurs d’écriture
                  commises. La première partie de cet article est consacrée à une description précise
                  de la faute. Dans la seconde partie, nous analysons l’erreur (1) en déterminant la
                  nature de la faute (typographique, orthographique, ou grammaticale) et (2) en explicitant
                  sa conséquence sur le niveau de perturbation linguistique (phonologique, orthographique,
                  morphologique ou syntaxique). Il résulte de ce travail un modèle général des erreurs
                  (une grille) que nous présenterons, ainsi que les résultats statistiques correspondants.
                  Enfin, nous montrerons sur des exemples, l’utilité de l’apport de cette grille, en
                  soumettant ces types de fautes à quelques correcteurs. Nous envisageons également
                  les implications informatiques de ce travail.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The aim of our work is modeling the detection and the correction of spelling and typing
                  errors, especially in the linguistic disabilities context. The work is based on a
                  fine analysis of clerical errors committed. The first part of this article is devoted
                  to a detailed description of error. In the second part, we analyze error in (1) determining
                  the nature of the fault (typographical, spelling, or grammar) and (2) by explaining
                  its consequences on the level of linguistic disturbance (phonological, orthographic,
                  morphological and syntactic). The outcome of this work is a general model of errors
                  (a grid) that we present, as well as the corresponding statistical results. Finally,
                  we show on examples, the usefulness of this grid, by submitting these types of errors
                  to a few spellcheckers. We also envisage the computer implications of this work.
               </p>
               <hr>
               <h4 id="taln-2009-court-030">Profilage de candidatures assisté par Relevance Feedback</h4>
               			Auteur : Rémy Kessler -
               			Contact : remy.kessler@univ-avignon.fr<br>
               			Auteur : Nicolas Béchet -
               			Contact : nicolas.bechet@lirmm.fr<br>
               			Auteur : Juan-Manuel Torres-Moreno -
               			Contact : juan-manuel.torres@univ-avignon.fr<br>
               			Auteur : Mathieu Roche -
               			Contact : mathieu.roche@lirmm.fr<br>
               			Auteur : Marc El-Bèze -
               			Contact : marc.elbeze@univ-avignon.fr<br><p>Le marché d’offres d’emploi et des candidatures sur Internet connaît une croissance
                  exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme
                  de texte libre) qu’il n’est plus possible de traiter manuellement. Une analyse et
                  catégorisation assistées nous semble pertinente en réponse à cette problématique.
                  Nous proposons E-Gen, système qui a pour but l’analyse et catégorisation assistés
                  d’offres d’emploi et des réponses des candidats. Dans cet article nous présentons
                  plusieurs stratégies, reposant sur les modèles vectoriel et probabiliste, afin de
                  résoudre la problématique du profilage des candidatures en fonction d’une offre précise.
                  Nous avons évalué une palette de mesures de similarité afin d’effectuer un classement
                  pertinent des candidatures au moyen des courbes ROC. L’utilisation d’une forme de
                  relevance feedback a permis de surpasser nos résultats sur ce problème difficile et
                  sujet à une grande subjectivité.
               </p><em>Version anglaise :</em><h4></h4>
               <p>The market of online job search sites has grown exponentially. This implies volumes
                  of information (mostly in the form of free text) manually impossible to process. An
                  analysis and assisted categorization seems relevant to address this issue. We present
                  E-Gen, a system which aims to perform assisted analysis and categorization of job
                  offers and the responses of candidates. This paper presents several strategies based
                  on vectorial and probabilistic models to solve the problem of profiling applications
                  according to a specific job offer. We have evaluated a range of measures of similarity
                  to rank candidatures by using ROC curves. Relevance feedback approach allows surpass
                  our previous results on this task, difficult and higly subjective.
               </p>
               <hr>
               <h4 id="taln-2009-court-031">Profilage sémantique endogène des relations de synonymie au sein de Gene Ontology</h4>
               			Auteur : Thierry Hamon<br>
               			Auteur : Natalia Grabar<br><p>Le calcul de la similarité sémantique entre les termes repose sur l’existence et l’utilisation
                  de ressources sémantiques. Cependant de telles ressources, qui proposent des équivalences
                  entre entités, souvent des relations de synonymie, doivent elles-mêmes être d’abord
                  analysées afin de définir des zones de fiabilité où la similarité sémantique est plus
                  forte. Nous proposons une méthode d’acquisition de synonymes élémentaires grâce à
                  l’exploitation des terminologies structurées au travers l’analyse de la structure
                  syntaxique des termes complexes et de leur compositionnalité. Les synonymes acquis
                  sont ensuite profilés grâce aux indicateurs endogènes inférés automatiquement à partir
                  de ces mêmes terminologies (d’autres types de relations, inclusions lexicales, productivité,
                  forme des composantes connexes). Dans le domaine biomédical, il existe de nombreuses
                  terminologies structurées qui peuvent être exploitées pour la constitution de ressources
                  sémantiques. Le travail présenté ici exploite une de ces terminologies, Gene Ontology.
               </p><em>Version anglaise :</em><h4></h4>
               <p>Computing the semantic similarity between terms relies on existence and usage of semantic
                  resources. However, these resources, often composed of equivalent units, or synonyms,
                  must be first analyzed and weighted in order to define within them the reliability
                  zones where the semantic similarity shows to be stronger. We propose a method for
                  acquisition of elementary synonyms which is based on exploitation of structured terminologies,
                  analysis of syntactic structure of complex (multi-unit) terms and their compositionality.
                  The acquired synonyms are then profiled thanks to endogenous indicators (other types
                  of relations, lexical inclusions, productivity, form of connected components), which
                  are automatically inferred within the same terminologies. In the biomedical area,
                  several structured terminologies have been built and can be exploited for the construction
                  of semantic resources. The work we present in this paper, is applied to terms of one
                  of these terminologies, i.e. the Gene Ontology.
               </p>
               <hr>
               <h4 id="taln-2009-court-032">Quels attributs discriminants pour une analyse syntaxique par classification de textes
                  en langue arabe ?
               </h4>
               			Auteur : Fériel Ben Fraj -
               			Contact : Feriel.BenFraj@riadi.rnu.tn<br>
               			Auteur : Chiraz Ben Othmane Zribi -
               			Contact : 